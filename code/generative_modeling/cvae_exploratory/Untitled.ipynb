{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8f68da-b7dd-4190-a68f-acc36d3d6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import random\n",
    "import neptune\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from tensorflow import keras, einsum\n",
    "from tensorflow_probability import layers as tfpl\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Layer, Input, MaxPooling2D, Conv2D, Conv2DTranspose, Flatten, Dropout, Dense, BatchNormalization, Activation, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow_probability.python.distributions import kl_divergence\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "from functools import partial\n",
    "from inspect import isfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72efd4ac-a76b-42a9-81c2-544f0155dca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 12:02:41.785125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 12:02:41.815896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 12:02:41.816185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 12:02:41.818739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 12:02:41.818979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 12:02:41.819187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 12:02:42.285610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 12:02:42.285798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 12:02:42.285924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 12:02:42.286031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20000 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "gpus = tf.config.list_physical_devices('GPU');\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 20000)]);\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU');\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ba718b-f14c-4138-b8b5-aaf23c30a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = h5py.File('/data/HSC/HSC_v6/step2A/64x64/5x64x64_training_z_less_than_2.hdf5', 'r')\n",
    "hf_test = h5py.File('/data/HSC/HSC_v6/step2A/64x64/5x64x64_testing_z_less_than_2.hdf5', 'r')\n",
    "hf_validation = h5py.File('/data/HSC/HSC_v6/step2A/64x64/5x64x64_validation_z_less_than_2.hdf5', 'r')\n",
    "x_train = np.asarray(np.transpose(hf_train['image'][0 : ],(0, 2, 3, 1)))\n",
    "x_test = np.asarray(np.transpose(hf_test['image'][0 : ],(0, 2, 3, 1)))\n",
    "x_validation = np.asarray(np.transpose(hf_validation['image'][0 : ], (0, 2, 3, 1)))\n",
    "y_train = np.asarray(hf_train['specz_redshift'][0 : ])[..., None]\n",
    "y_test = np.asarray(hf_test['specz_redshift'][0 : ])[..., None]\n",
    "y_validation = np.asarray(hf_validation['specz_redshift'][0 : ])[..., None]\n",
    "hf_train.close()\n",
    "hf_test.close()\n",
    "hf_validation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "208a7a4c-15de-4814-ad5f-13d4159fe5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194510, 64, 64, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d2d78c-606e-49e3-991a-9db51c159604",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"/data3/Billy/data/HSC/HSC_v6/step2A/64x64/\"\n",
    "os.makedirs(dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa61ee36-ab73-47fb-89c7-cfad79b3bf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1835"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_files = x_train.shape[0] // 106\n",
    "n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a82ea4-3de0-45a2-92f8-5dcab79559db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved!\n"
     ]
    }
   ],
   "source": [
    "n_files = x_train.shape[0] // 106  # the number of smaller arrays\n",
    "split_arrays = np.array_split(x_train, n_files)\n",
    "\n",
    "# Save each smaller array into a file\n",
    "for i, small_array in enumerate(split_arrays):\n",
    "    filename = f\"/data3/Billy/data/HSC/HSC_v6/step2A/64x64/5x64x64_training_z_less_than_2_batch_{i}.npy\"\n",
    "    np.save(filename, small_array)\n",
    "\n",
    "print(\"All files saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a75aeb1b-537c-40a4-b564-946c5f082ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1835it [00:00, 24961.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_files = y_train.shape[0] // 106  # the number of smaller arrays\n",
    "split_arrays = np.array_split(y_train, n_files)\n",
    "\n",
    "# Save each smaller array into a file\n",
    "for i, small_array in tqdm(enumerate(split_arrays)):\n",
    "    filename = f\"/data3/Billy/data/HSC/HSC_v6/step2A/64x64/5x64x64_training_z_less_than_2_batch_{i}_redshifts.npy\"\n",
    "    np.save(filename, small_array)\n",
    "\n",
    "print(\"All files saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589c061d-836d-4dba-a7df-02ef5190ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "366it [00:12, 30.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_files = x_test.shape[0] // 106  # the number of smaller arrays\n",
    "split_arrays = np.array_split(x_test, n_files)\n",
    "\n",
    "# Save each smaller array into a file\n",
    "for i, small_array in tqdm(enumerate(split_arrays)):\n",
    "    filename = f\"/data3/Billy/data/HSC/HSC_v6/step2A/64x64/5x64x64_testing_z_less_than_2_batch_{i}.npy\"\n",
    "    np.save(filename, small_array)\n",
    "\n",
    "print(\"All files saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da101ec1-2739-49fe-8b47-78bbc011dac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "366it [00:00, 13109.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_files = y_test.shape[0] // 106  # the number of smaller arrays\n",
    "split_arrays = np.array_split(y_test, n_files)\n",
    "\n",
    "# Save each smaller array into a file\n",
    "for i, small_array in tqdm(enumerate(split_arrays)):\n",
    "    filename = f\"/data3/Billy/data/HSC/HSC_v6/step2A/64x64/5x64x64_testing_z_less_than_2_batch_{i}_redshifts.npy\"\n",
    "    np.save(filename, small_array)\n",
    "\n",
    "print(\"All files saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cdc9ccf-84e4-4e06-9222-9bc3d83f17c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "367it [00:11, 30.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_files = x_validation.shape[0] // 106  # the number of smaller arrays\n",
    "split_arrays = np.array_split(x_validation, n_files)\n",
    "\n",
    "# Save each smaller array into a file\n",
    "for i, small_array in tqdm(enumerate(split_arrays)):\n",
    "    filename = f\"/data3/Billy/data/HSC/HSC_v6/step2A/64x64/5x64x64_validation_z_less_than_2_batch_{i}.npy\"\n",
    "    np.save(filename, small_array)\n",
    "\n",
    "print(\"All files saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9a2f753-9977-4def-91ee-99eaee67ba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "367it [00:00, 12540.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_files = y_validation.shape[0] // 106  # the number of smaller arrays\n",
    "split_arrays = np.array_split(y_validation, n_files)\n",
    "\n",
    "# Save each smaller array into a file\n",
    "for i, small_array in tqdm(enumerate(split_arrays)):\n",
    "    filename = f\"/data3/Billy/data/HSC/HSC_v6/step2A/64x64/5x64x64_validation_z_less_than_2_batch_{i}_redshifts.npy\"\n",
    "    np.save(filename, small_array)\n",
    "\n",
    "print(\"All files saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbc744-0abd-47cd-b7d2-ca06d756dece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffusion",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
