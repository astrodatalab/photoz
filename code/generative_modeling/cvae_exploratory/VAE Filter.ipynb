{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23f862b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 17:04:46.418550: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 17:04:46.909517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import stdout\n",
    "from datetime import datetime\n",
    "# import neptune.new as neptune\n",
    "# from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2032fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Cropping2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import layers as tfpl\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.nn import leaky_relu\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e0800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128\n",
    "BASE_DEPTH = 8\n",
    "IMAGE_SHAPE = (5, 127, 127)\n",
    "EPOCHS = 200\n",
    "CHECKPOINTS_TO_SAVE = 4\n",
    "KL_WEIGHT = 1e-5\n",
    "# Good value: 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a44e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 17:04:49.398792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 17:04:49.430267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 17:04:49.430580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 17:04:49.433614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 17:04:49.433857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 17:04:49.434064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 17:04:49.930300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 17:04:49.930493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 17:04:49.930621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 17:04:49.930730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20000 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU');\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 20000)]);\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU');\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41928b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "DATASET_NAME = \"HSC_v6_small\"\n",
    "MODEL_TYPE = \"VAE\"\n",
    "MODEL_VERSION = \"v1.0\"\n",
    "MODEL_SUBVERSION = \"v1.0.20221118_14_03_25\"\n",
    "\n",
    "model_id = '_'.join([DATASET_NAME, MODEL_TYPE, MODEL_VERSION])\n",
    "\n",
    "model_path = os.path.join('/models', model_id, MODEL_SUBVERSION, 'model')\n",
    "\n",
    "weights_path = model_path + '/weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c90a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = h5py.File('/data/HSC/HSC_v6/step2A/127x127/5x127x127_training_small.hdf5', 'r')\n",
    "hf_test = h5py.File('/data/HSC/HSC_v6/step2A/127x127/5x127x127_testing_small.hdf5', 'r')\n",
    "hf_validation = h5py.File('/data/HSC/HSC_v6/step2A/127x127/5x127x127_validation_small.hdf5', 'r')\n",
    "x_train = np.asarray(hf_train['image'][0:])\n",
    "x_test = np.asarray(hf_test['image'][0:])\n",
    "x_validation = np.asarray(hf_validation['image'][0:])\n",
    "max_value = 4.16\n",
    "x_train = np.true_divide(x_train, max_value)\n",
    "x_test = np.true_divide(x_test, max_value)\n",
    "x_validation = np.true_divide(x_validation, max_value)\n",
    "y_train = np.asarray(hf_train['specz_redshift'][0:])[..., None]\n",
    "y_test = np.asarray(hf_test['specz_redshift'][0:])[..., None]\n",
    "y_validation = np.asarray(hf_validation['specz_redshift'][0:])[..., None]\n",
    "# object_id_train = np.asarray(hf_train['object_id'][0:])\n",
    "# object_id = np.asarray(hf_test['object_id'][0:])\n",
    "# object_id_validation = np.asarray(hf_validation['object_id'][0:])\n",
    "hf_train.close()\n",
    "hf_test.close()\n",
    "hf_validation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e33b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(LATENT_DIM), scale = 0.1), reinterpreted_batch_ndims = 1)\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        images = Input(shape = IMAGE_SHAPE)\n",
    "        x = Conv2D(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                   padding = 'same', data_format = 'channels_first')(images)\n",
    "        x = Conv2D(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                   padding = 'same', data_format = 'channels_first')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(tfpl.MultivariateNormalTriL.params_size(LATENT_DIM), activation = None)(x)\n",
    "        z = tfpl.MultivariateNormalTriL(LATENT_DIM,\n",
    "                  activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight = KL_WEIGHT))(x)\n",
    "        self.encoder = Model(images, z, name = 'encoder')\n",
    "\n",
    "        latents = Input(shape = LATENT_DIM)\n",
    "        x = Dense(8 * LATENT_DIM * 32 * 32, activation = None)(latents)\n",
    "        x = Reshape((8 * LATENT_DIM, 32, 32))(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                            padding = 'same', data_format = 'channels_first')(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                            padding = 'same', data_format = 'channels_first')(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides = 1, activation = leaky_relu,\n",
    "                            padding = 'same', data_format = 'channels_first')(x)\n",
    "        x = Conv2D(IMAGE_SHAPE[0], 3, strides = 1, activation = None, \n",
    "                   padding = 'same', data_format = 'channels_first')(x)\n",
    "        outputs = Cropping2D(cropping=((0, 1), (0, 1)), data_format = 'channels_first')(x)\n",
    "        self.decoder = Model(latents, outputs, name = 'decoder')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def summary(self):\n",
    "        print(self.encoder.summary())\n",
    "        print(self.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ce4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 5, 127, 127)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 8, 64, 64)         368       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 32, 32)         584       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8384)              68690112  \n",
      "                                                                 \n",
      " multivariate_normal_tri_l (  ((None, 128),            0         \n",
      " MultivariateNormalTriL)      (None, 128))                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,691,064\n",
      "Trainable params: 68,691,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1048576)           135266304 \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1024, 32, 32)      0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 8, 64, 64)        73736     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 128, 128)      584       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 8, 128, 128)      584       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 5, 128, 128)       365       \n",
      "                                                                 \n",
      " cropping2d (Cropping2D)     (None, 5, 127, 127)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,341,573\n",
      "Trainable params: 135,341,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "model.compile(optimizer = 'adam', loss = 'mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c729aa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neptune' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mneptune\u001b[49m\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      2\u001b[0m     name \u001b[38;5;241m=\u001b[39m MODEL_SUBVERSION,\n\u001b[1;32m      3\u001b[0m     project \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastro-data-lab/VAE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     api_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxOGFlZGMxOC04MWU5LTQ2NDctYjlhZS05NGE2NGQ0NmIzMmEifQ==\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m )  \u001b[38;5;66;03m# your credentials\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# logs_callback = TensorBoard(log_dir = logs_path)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m LR_callback \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'neptune' is not defined"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    name = MODEL_SUBVERSION,\n",
    "    project = \"astro-data-lab/VAE\",\n",
    "    api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxOGFlZGMxOC04MWU5LTQ2NDctYjlhZS05NGE2NGQ0NmIzMmEifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "# logs_callback = TensorBoard(log_dir = logs_path)\n",
    "\n",
    "LR_callback = ReduceLROnPlateau()\n",
    "\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs_path, histogram_freq = 1)\n",
    "\n",
    "neptune_callback = NeptuneCallback(run = run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b02153ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 17:05:44.022132: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3225800000 exceeds 10% of free system memory.\n",
      "2023-05-03 17:05:45.695331: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3225800000 exceeds 10% of free system memory.\n",
      "2023-05-03 17:05:47.312302: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3225800000 exceeds 10% of free system memory.\n",
      "2023-05-03 17:05:48.305810: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3225800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 17:05:51.366996: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-05-03 17:05:51.367051: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 520.61.5\n",
      "2023-05-03 17:05:51.367203: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops.cc:1068 : UNIMPLEMENTED: DNN library is not found.\n",
      "2023-05-03 17:05:51.367221: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNIMPLEMENTED: DNN library is not found.\n",
      "\t [[{{node vae/encoder/conv2d/Conv2D}}]]\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'vae/encoder/conv2d/Conv2D' defined at (most recent call last):\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_4142807/1110390625.py\", line 1, in <module>\n      history = model.fit(x = x_train, y = x_train, epochs = EPOCHS, validation_data = (x_validation, x_validation), verbose = 1)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_4142807/4289805732.py\", line 33, in call\n      x = self.encoder(x)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 290, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 262, in convolution_op\n      return tf.nn.convolution(\nNode: 'vae/encoder/conv2d/Conv2D'\nDNN library is not found.\n\t [[{{node vae/encoder/conv2d/Conv2D}}]] [Op:__inference_train_function_4388]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_validation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'vae/encoder/conv2d/Conv2D' defined at (most recent call last):\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_4142807/1110390625.py\", line 1, in <module>\n      history = model.fit(x = x_train, y = x_train, epochs = EPOCHS, validation_data = (x_validation, x_validation), verbose = 1)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_4142807/4289805732.py\", line 33, in call\n      x = self.encoder(x)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 290, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/billyli/.local/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 262, in convolution_op\n      return tf.nn.convolution(\nNode: 'vae/encoder/conv2d/Conv2D'\nDNN library is not found.\n\t [[{{node vae/encoder/conv2d/Conv2D}}]] [Op:__inference_train_function_4388]"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = x_train, y = x_train, epochs = EPOCHS, validation_data = (x_validation, x_validation), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca798a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_galaxies(num_to_generate = 10):\n",
    "    z = prior.sample(num_to_generate)\n",
    "    xhat = model.decoder(z)\n",
    "    fig, axes = plt.subplots(nrows = num_to_generate, ncols = 5, figsize = (4 * 5, 4 * num_to_generate))\n",
    "    for i in range(num_to_generate):\n",
    "        for j in range(0,5):\n",
    "            axes[i][j].imshow(xhat[i][j], cmap = 'afmhot')\n",
    "            axes[i][j].set_title(f'Generated image {i} band {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_example_galaxies(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af68910",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list_g = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/g_band\"))\n",
    "image_list_r = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/r_band\"))\n",
    "image_list_i = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/i_band\"))\n",
    "image_list_z = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/z_band\"))\n",
    "image_list_y = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/y_band\"))\n",
    "\n",
    "photozdata = pd.read_csv('/mnt/data/HSC/HSC_v6/HSC_v6.csv')\n",
    "photozdata.describe()\n",
    "\n",
    "b = np.argsort(photozdata['object_id'])\n",
    "sorted_photozdata = photozdata.iloc[b][:]\n",
    "photozdata = sorted_photozdata\n",
    "\n",
    "hf_in = h5py.File('/mnt/data/HSC/HSC_v6/step3/127x127_mae_in.hdf5', 'a')\n",
    "hf_out = h5py.File('/mnt/data/HSC/HSC_v6/step3/127x127_mae_out.hdf5', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01132a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_in = 0\n",
    "count_out = 0\n",
    "size = len(image_list_g)\n",
    "for i in range(size):\n",
    "    \n",
    "    stdout.write(\"\\rChecking %d samples of \" % (i + 1) + str(size))\n",
    "    \n",
    "    object_id = image_list_g[i][0:17]\n",
    "\n",
    "    five_band_image = []\n",
    "\n",
    "    image_g = fits.open(\"/mnt/data/HSC/HSC_v6/step1/g_band/\" + image_list_g[i])\n",
    "    image_r = fits.open(\"/mnt/data/HSC/HSC_v6/step1/r_band/\" + image_list_r[i])\n",
    "    image_i = fits.open(\"/mnt/data/HSC/HSC_v6/step1/i_band/\" + image_list_i[i])\n",
    "    image_z = fits.open(\"/mnt/data/HSC/HSC_v6/step1/z_band/\" + image_list_z[i])\n",
    "    image_y = fits.open(\"/mnt/data/HSC/HSC_v6/step1/y_band/\" + image_list_y[i])\n",
    "\n",
    "    image_g_data = image_g[1].data\n",
    "    image_r_data = image_r[1].data\n",
    "    image_i_data = image_i[1].data\n",
    "    image_z_data = image_z[1].data\n",
    "    image_y_data = image_y[1].data\n",
    "\n",
    "    pad1 = int((127 - len(image_g_data)) / 2)\n",
    "    pad2 = 127 - len(image_g_data) - pad1\n",
    "    pad3 = int((127 - len(image_g_data[0])) / 2)\n",
    "    pad4 = 127 - len(image_g_data[0]) - pad3\n",
    "\n",
    "\n",
    "    im_g = np.pad(image_g_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "    im_r = np.pad(image_r_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "    im_i = np.pad(image_i_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "    im_z = np.pad(image_z_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "    im_y = np.pad(image_y_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "\n",
    "    im = np.true_divide(np.array([im_g, im_r, im_i, im_z, im_y]), max_value)\n",
    "    loss = model.evaluate(np.array([im]), np.array([im]), verbose = 0)\n",
    "    \n",
    "    if loss <= 0.05:\n",
    "    \n",
    "        five_band_image.append(im_g)\n",
    "        five_band_image.append(im_r)\n",
    "        five_band_image.append(im_i)\n",
    "        five_band_image.append(im_z)\n",
    "        five_band_image.append(im_y)\n",
    "\n",
    "        five_band_image_reshape = np.reshape(np.array(five_band_image), [1, 5, 127, 127])\n",
    "\n",
    "        photozdata_subset = photozdata.iloc[i]\n",
    "\n",
    "        specz = photozdata_subset['specz_redshift']\n",
    "        specz_reshape = np.reshape(specz, [1, 1])\n",
    "\n",
    "        if count_in == 0:\n",
    "        \n",
    "            for (columnName, columnData) in photozdata.iteritems():\n",
    "                \n",
    "                if columnName == 'specz_name' or columnName == 'coord':\n",
    "                    \n",
    "                    hf_in.create_dataset(columnName, data = np.reshape(np.array(photozdata[columnName]).astype(str), [286401, 1]).astype('S')[i], maxshape = (None, ))\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    hf_in.create_dataset(columnName, data = photozdata[columnName][i : i + 1], maxshape = (None, ))\n",
    "            \n",
    "            hf_in.create_dataset('image', data = five_band_image_reshape, chunks = True, maxshape = (None, 5, 127, 127))\n",
    "\n",
    "        else:\n",
    "        \n",
    "            for (columnName, columnData) in photozdata.iteritems():\n",
    "        \n",
    "                hf_in[columnName].resize((hf_in[columnName].shape[0] + 1), axis = 0)\n",
    "            \n",
    "                if columnName == 'specz_name' or columnName == 'coord':\n",
    "                    \n",
    "                    hf_in[columnName][hf_in[columnName].shape[0] - 1] = np.reshape(np.array(photozdata[columnName]).astype(str), [286401, 1]).astype('S')[i]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    hf_in[columnName][hf_in[columnName].shape[0] - 1] = photozdata[columnName][i : i + 1]\n",
    "            \n",
    "            hf_in['image'].resize((hf_in['image'].shape[0] + 1), axis = 0)\n",
    "            hf_in['image'][hf_in['image'].shape[0] - 1, :, :, :] = five_band_image\n",
    "        \n",
    "        count_in = count_in + 1\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        five_band_image.append(im_g)\n",
    "        five_band_image.append(im_r)\n",
    "        five_band_image.append(im_i)\n",
    "        five_band_image.append(im_z)\n",
    "        five_band_image.append(im_y)\n",
    "\n",
    "        five_band_image_reshape = np.reshape(np.array(five_band_image), [1, 5, 127, 127])\n",
    "\n",
    "        photozdata_subset = photozdata.iloc[i]\n",
    "\n",
    "        specz = photozdata_subset[\"specz_redshift\"]\n",
    "        specz_reshape = np.reshape(specz, [1, 1])\n",
    "\n",
    "        if count_out == 0:\n",
    "            \n",
    "            for (columnName, columnData) in photozdata.iteritems():\n",
    "                \n",
    "                if columnName == 'specz_name' or columnName == 'coord':\n",
    "                    \n",
    "                    hf_out.create_dataset(columnName, data = np.reshape(np.array(photozdata[columnName]).astype(str), [286401, 1]).astype('S')[i], maxshape = (None, ))\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    hf_out.create_dataset(columnName, data = photozdata[columnName][i : i + 1], maxshape = (None, ))\n",
    "                \n",
    "            hf_out.create_dataset('image', data = five_band_image_reshape, chunks = True, maxshape = (None, 5, 127, 127))\n",
    "\n",
    "        else:\n",
    "            \n",
    "            for (columnName, columnData) in photozdata.iteritems():\n",
    "        \n",
    "                hf_out[columnName].resize((hf_out[columnName].shape[0] + 1), axis = 0)\n",
    "            \n",
    "                if columnName == 'specz_name' or columnName == 'coord':\n",
    "                    \n",
    "                    hf_out[columnName][hf_out[columnName].shape[0] - 1] = np.reshape(np.array(photozdata[columnName]).astype(str), [286401, 1]).astype('S')[i]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    hf_out[columnName][hf_out[columnName].shape[0] - 1] = photozdata[columnName][i : i + 1]\n",
    "                \n",
    "            hf_out['image'].resize((hf_out['image'].shape[0] + 1), axis = 0)\n",
    "            hf_out['image'][hf_out[\"image\"].shape[0] - 1, :, :, :] = five_band_image\n",
    "        \n",
    "        count_out = count_out + 1\n",
    "\n",
    "    image_g.close()\n",
    "    image_r.close()\n",
    "    image_i.close()\n",
    "    image_z.close()\n",
    "    image_y.close()\n",
    "\n",
    "hf_in.close()\n",
    "hf_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900333ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca76399",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('/data/HSC/HSC_v6/step3/127x127_mae_in.hdf5', 'r')\n",
    "y_array = np.asarray(hf['specz_redshift'][0:])[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14fce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_array, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
