{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f923d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "write = True # log this version as new\n",
    "MODEL_DESCRIPTION = \"Diffusion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8fc210-73b8-4a4e-a0ef-a05711c926fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:56:43.748584: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 20:56:44.272046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/billyli/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/billyli/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf;\n",
    "from tensorflow import keras, einsum\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.layers as nn\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "\n",
    "import pandas as pd\n",
    "from sys import stdout\n",
    "from datetime import datetime\n",
    "import neptune as neptune\n",
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "from astropy.io import fits\n",
    "import random\n",
    "from DataMaker import HDF5ImageGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a44e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:56:45.634955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-16 20:56:45.652907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-16 20:56:45.653108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-16 20:56:45.655167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-16 20:56:45.655317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-16 20:56:45.655449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-16 20:56:46.139723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-16 20:56:46.139919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-16 20:56:46.140057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-16 20:56:46.140164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10000 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "gpus = tf.config.list_physical_devices('GPU');\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 10000)]);\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU');\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb7cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "DATASET_NAME = \"HSC_v6\"\n",
    "MODEL_TYPE = \"Diffusion\"\n",
    "MODEL_VERSION = \"v1.0\"\n",
    "MODEL_SUBVERSION = \"v1.0.\" + now.strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "\n",
    "model_id = '_'.join([DATASET_NAME, MODEL_TYPE, MODEL_VERSION])\n",
    "model_path = os.path.join('/data3/Billy/models', model_id, MODEL_SUBVERSION, 'model')\n",
    "checkpoints_path = os.path.join('/data3/Billy/models', model_id, MODEL_SUBVERSION, 'checkpoints')\n",
    "logs_path = os.path.join('/data3/Billy/logs', model_id, MODEL_SUBVERSION)\n",
    "predictions_path = os.path.join('/data3/Billy/predictions', model_id, MODEL_SUBVERSION)\n",
    "weights_path = model_path + '/data3/Billy/Diffusion/weights.h5'\n",
    "\n",
    "os.makedirs(model_path, exist_ok = True)\n",
    "os.makedirs(checkpoints_path, exist_ok = True)\n",
    "os.makedirs(logs_path, exist_ok = True)\n",
    "os.makedirs(predictions_path, exist_ok = True)\n",
    "\n",
    "if write == True:\n",
    "    with open(\"/data3/Billy/models/README.md\", \"a\") as myfile:\n",
    "        myfile.write(MODEL_TYPE + \" \" + MODEL_SUBVERSION + \" - \" + MODEL_DESCRIPTION + \" - B. Li\" + \"\\n\")\n",
    "else:\n",
    "    with open(\"/data3/Billy/models/README.md\", \"a\") as myfile:\n",
    "        myfile.write(MODEL_TYPE + \" \" + MODEL_SUBVERSION + \" - ... \"+ \" - B. Li\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84bcf60-c403-4b14-be17-6908a9bd15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc0eecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:56:46.198980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:736] failed to allocate 9.77GiB (10485760000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-05-16 20:56:46.200785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:736] failed to allocate 8.79GiB (9437184000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-05-16 20:56:46.202476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:736] failed to allocate 7.91GiB (8493465600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-05-16 20:56:46.204307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:736] failed to allocate 7.12GiB (7644119040 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-05-16 20:56:46.206067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:736] failed to allocate 6.41GiB (6879707136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-05-16 20:56:46.207836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:736] failed to allocate 5.77GiB (6191736320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    return tf.image.resize(tf.cast(x, tf.float32) / 127.5 - 1, (32, 32))\n",
    "\n",
    "def get_datasets():\n",
    "    # Load the MNIST dataset\n",
    "    train_ds = tfds.load('mnist', as_supervised=True, split=\"train\")\n",
    "\n",
    "    # Normalize to [-1, 1], shuffle and batch\n",
    "    train_ds = train_ds.map(preprocess, tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.shuffle(5000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Return numpy arrays instead of TF tensors while iterating\n",
    "    return tfds.as_numpy(train_ds)\n",
    "\n",
    "\n",
    "dataset = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd19fa51-7721-4185-97de-9911ad8b9aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:56:46.322112: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-05-16 20:56:46.322469: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-05-16 20:56:46.465242: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 32, 32, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da216099-362e-4bb3-95e4-e72f57806b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 200\n",
    "\n",
    "# create a fixed beta schedule\n",
    "beta = np.linspace(0.0001, 0.02, timesteps)\n",
    "\n",
    "# this will be used as discussed in the reparameterization trick\n",
    "alpha = 1 - beta\n",
    "alpha_bar = np.cumprod(alpha, 0)\n",
    "alpha_bar = np.concatenate((np.array([1.]), alpha_bar[ : -1]), axis = 0)\n",
    "sqrt_alpha_bar = np.sqrt(alpha_bar)\n",
    "one_minus_sqrt_alpha_bar = np.sqrt(1 - alpha_bar)\n",
    "\n",
    "# this function will help us set the RNG key for Numpy\n",
    "def set_key(key):\n",
    "    np.random.seed(key)\n",
    "\n",
    "# this function will add noise to the input as per the given timestamp\n",
    "def forward_noise(key, x_0, t):\n",
    "    set_key(key)\n",
    "    noise = np.random.normal(size = x_0.shape)\n",
    "    reshaped_sqrt_alpha_bar_t = np.reshape(np.take(sqrt_alpha_bar, t), (-1, 1, 1, 1))\n",
    "    reshaped_one_minus_sqrt_alpha_bar_t = np.reshape(np.take(one_minus_sqrt_alpha_bar, t), (-1, 1, 1, 1))\n",
    "    noisy_image = reshaped_sqrt_alpha_bar_t  * x_0 + reshaped_one_minus_sqrt_alpha_bar_t  * noise\n",
    "    return noisy_image, noise\n",
    "\n",
    "# this function will be used to create sample timestamps between 0 & T\n",
    "def generate_timestamp(key, num):\n",
    "    set_key(key)\n",
    "    return tf.random.uniform(shape = [num], minval = 0, maxval = timesteps, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438565c5-d1b2-40e0-976f-f68c67d3e611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:52:00.964685: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x3000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_mnist = next(iter(dataset))[0]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 30))\n",
    "\n",
    "for index, i in enumerate([10, 100, 150, 199]):\n",
    "    noisy_im, noise = forward_noise(0, np.expand_dims(sample_mnist, 0), np.array([i,]))\n",
    "    plt.subplot(1, 4, index+1)\n",
    "    plt.imshow(np.squeeze(np.squeeze(noisy_im, -1), 0), cmap='gray')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796c4bd9-4fb6-456d-b5f5-750c3e020ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers functions\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "# We will use this to convert timestamps to time encodings\n",
    "class SinusoidalPosEmb(Layer):\n",
    "    def __init__(self, dim, max_positions = 10000):\n",
    "        super(SinusoidalPosEmb, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.max_positions = max_positions\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.max_positions) / (half_dim - 1)\n",
    "        emb = tf.exp(tf.range(half_dim, dtype = tf.float32) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "\n",
    "        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis = -1)\n",
    "\n",
    "        return emb\n",
    "        \n",
    "# small helper modules\n",
    "class Identity(Layer):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        return tf.identity(x)\n",
    "\n",
    "\n",
    "class Residual(Layer):\n",
    "    def __init__(self, fn):\n",
    "        super(Residual, self).__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        return self.fn(x, training = training) + x\n",
    "\n",
    "def Upsample(dim):\n",
    "    return nn.Conv2DTranspose(filters = dim, kernel_size = 4, strides = 2, padding = 'SAME')\n",
    "\n",
    "def Downsample(dim):\n",
    "    return nn.Conv2D(filters = dim, kernel_size = 4, strides = 2, padding = 'SAME')\n",
    "\n",
    "class LayerNorm(Layer):\n",
    "    def __init__(self, dim, eps = 1e-5, **kwargs):\n",
    "        super(LayerNorm, self).__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "        self.g = tf.Variable(tf.ones([1, 1, 1, dim]))\n",
    "        self.b = tf.Variable(tf.zeros([1, 1, 1, dim]))\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        var = tf.math.reduce_variance(x, axis = -1, keepdims = True)\n",
    "        mean = tf.reduce_mean(x, axis = -1, keepdims = True)\n",
    "\n",
    "        x = (x - mean) / tf.sqrt((var + self.eps)) * self.g + self.b\n",
    "        return x\n",
    "\n",
    "class PreNorm(Layer):\n",
    "    def __init__(self, dim, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "class SiLU(Layer):\n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__()\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        return x * tf.nn.sigmoid(x)\n",
    "\n",
    "def gelu(x, approximate = False):\n",
    "    if approximate:\n",
    "        coeff = tf.cast(0.044715, x.dtype)\n",
    "        return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + coeff * tf.pow(x, 3))))\n",
    "    else:\n",
    "        return 0.5 * x * (1.0 + tf.math.erf(x / tf.cast(1.4142135623730951, x.dtype)))\n",
    "\n",
    "class GELU(Layer):\n",
    "    def __init__(self, approximate = False):\n",
    "        super(GELU, self).__init__()\n",
    "        self.approximate = approximate\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        return gelu(x, self.approximate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf57659-c611-4f91-b309-05cc09870a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building block modules\n",
    "class Block(Layer):\n",
    "    def __init__(self, dim, groups = 8):\n",
    "        super(Block, self).__init__()\n",
    "        self.proj = nn.Conv2D(dim, kernel_size = 3, strides = 1, padding = 'SAME')\n",
    "        self.norm = tfa.layers.GroupNormalization(groups, epsilon = 1e-05)\n",
    "        self.act = SiLU()\n",
    "\n",
    "\n",
    "    def call(self, x, gamma_beta = None, training = True):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x, training=training)\n",
    "\n",
    "        if exists(gamma_beta):\n",
    "            gamma, beta = gamma_beta\n",
    "            x = x * (gamma + 1) + beta\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(Layer):\n",
    "    def __init__(self, dim, dim_out, time_emb_dim = None, groups = 8):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "\n",
    "        self.mlp = Sequential([\n",
    "            SiLU(),\n",
    "            nn.Dense(units = dim_out * 2)\n",
    "        ]) if exists(time_emb_dim) else None\n",
    "\n",
    "        self.block1 = Block(dim_out, groups = groups)\n",
    "        self.block2 = Block(dim_out, groups = groups)\n",
    "        self.res_conv = nn.Conv2D(filters = dim_out, kernel_size = 1, strides = 1) if dim != dim_out else Identity()\n",
    "\n",
    "    def call(self, x, time_emb = None, training = True):\n",
    "        gamma_beta = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b 1 1 c')\n",
    "            gamma_beta = tf.split(time_emb, num_or_size_splits = 2, axis = -1)\n",
    "\n",
    "        h = self.block1(x, gamma_beta = gamma_beta, training = training)\n",
    "        h = self.block2(h, training = training)\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class LinearAttention(Layer):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.hidden_dim = dim_head * heads\n",
    "\n",
    "        self.attend = nn.Softmax()\n",
    "        self.to_qkv = nn.Conv2D(filters = self.hidden_dim * 3, kernel_size = 1, strides = 1, use_bias = False)\n",
    "\n",
    "        self.to_out = Sequential([\n",
    "            nn.Conv2D(filters = dim, kernel_size = 1, strides = 1),\n",
    "            LayerNorm(dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        b, h, w, c = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = tf.split(qkv, num_or_size_splits = 3, axis = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b x y (h c) -> b h c (x y)', h = self.heads), qkv)\n",
    "\n",
    "        q = tf.nn.softmax(q, axis = -2)\n",
    "        k = tf.nn.softmax(k, axis = -1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b x y (h c)', h = self.heads, x = h, y = w)\n",
    "        out = self.to_out(out, training = training)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 127):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.hidden_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Conv2D(filters = self.hidden_dim * 3, kernel_size = 1, strides = 1, use_bias = False)\n",
    "        self.to_out = nn.Conv2D(filters = dim, kernel_size = 1, strides = 1)\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        b, h, w, c = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = tf.split(qkv, num_or_size_splits = 3, axis = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b x y (h c) -> b h c (x y)', h=self.heads), qkv)\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
    "        sim_max = tf.stop_gradient(tf.expand_dims(tf.argmax(sim, axis = -1), axis = -1))\n",
    "        sim_max = tf.cast(sim_max, tf.float32)\n",
    "        sim = sim - sim_max\n",
    "        attn = tf.nn.softmax(sim, axis = -1)\n",
    "\n",
    "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h (x y) d -> b x y (h d)', x = h, y = w)\n",
    "        out = self.to_out(out, training = training)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afee365c-cd1f-4ec3-9b14-71dfd6914664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(Model):\n",
    "    def __init__(self,\n",
    "                 dim=64,\n",
    "                 init_dim=None,\n",
    "                 out_dim=None,\n",
    "                 dim_mults=(1, 2, 4, 8),\n",
    "                 channels=3,\n",
    "                 resnet_block_groups=8,\n",
    "                 learned_variance=False,\n",
    "                 sinusoidal_cond_mlp=True\n",
    "                 ):\n",
    "        super(Unet, self).__init__()\n",
    "        \n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        \n",
    "        init_dim = default(init_dim, dim // 3 * 2)\n",
    "        self.init_conv = nn.Conv2D(filters=init_dim, kernel_size=7, strides=1, padding='SAME')\n",
    "        \n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        \n",
    "        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n",
    "        \n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "        self.sinusoidal_cond_mlp = sinusoidal_cond_mlp\n",
    "        \n",
    "        self.time_mlp = Sequential([\n",
    "            SinusoidalPosEmb(dim),\n",
    "            nn.Dense(units=time_dim),\n",
    "            GELU(),\n",
    "            nn.Dense(units=time_dim)\n",
    "        ], name=\"time embeddings\")\n",
    "        \n",
    "        # layers\n",
    "        self.downs = []\n",
    "        self.ups = []\n",
    "        num_resolutions = len(in_out)\n",
    "        \n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append([\n",
    "                block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                Downsample(dim_out) if not is_last else Identity()\n",
    "            ])\n",
    "  \n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        \n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.ups.append([\n",
    "                block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
    "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Upsample(dim_in) if not is_last else Identity()\n",
    "            ])\n",
    "        \n",
    "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
    "        self.out_dim = default(out_dim, default_out_dim)\n",
    "        \n",
    "        self.final_conv = Sequential([\n",
    "            block_klass(dim * 2, dim),\n",
    "            nn.Conv2D(filters=self.out_dim, kernel_size=1, strides=1)\n",
    "        ], name=\"output\")\n",
    "        \n",
    "    def call(self, x, time=None, training=True, **kwargs):\n",
    "        x = self.init_conv(x)\n",
    "        t = self.time_mlp(time)\n",
    "        \n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = tf.concat([x, h.pop()], axis=-1)\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = tf.concat([x, h.pop()], axis=-1)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca7f7ea7-3a99-4fe6-82a0-05df8abc9081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a901ee460> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a901eeb20>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a901ee460> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a901eeb20>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a900fcc10> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a90080310>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a900fcc10> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a90080310>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a900a83d0> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a900a8a90>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a900a83d0> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a900a8a90>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a90052b50> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a90058250>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a90052b50> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a90058250>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a7ffe4df0> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a7ffe9520>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a7ffe4df0> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a7ffe9520>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a7ff959d0> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a7ff99100>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a7ff959d0> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a7ff99100>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a7ff435b0> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a7ff43ca0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<__main__.LayerNorm object at 0x7f2a7ff435b0> and <keras.layers.convolutional.conv2d.Conv2D object at 0x7f2a7ff43ca0>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ./checkpoints/ckpt-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:52:02.319289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-16 20:52:02.797410: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-16 20:52:02.958109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "# create our unet model\n",
    "unet = Unet(channels=1)\n",
    "\n",
    "# create our checkopint manager\n",
    "ckpt = tf.train.Checkpoint(unet=unet)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=2)\n",
    "\n",
    "# load from a previous checkpoint if it exists, else initialize the model from scratch\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    start_interation = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "# initialize the model in the memory of our GPU\n",
    "test_images = np.ones([1, 32, 32, 1])\n",
    "test_timestamps = generate_timestamp(0, 1)\n",
    "k = unet(test_images, test_timestamps)\n",
    "\n",
    "# create our optimizer, we will use adam with a Learning rate of 1e-4\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "917b6f9d-5f56-4bd0-9a52-11ba43af03ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"unet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  2100      \n",
      "                                                                 \n",
      " time embeddings (Sequential  (1, 256)                 82432     \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block (ResnetBlock)  multiple                  97088     \n",
      "                                                                 \n",
      " resnet_block_1 (ResnetBlock  multiple                 107008    \n",
      " )                                                               \n",
      "                                                                 \n",
      " residual (Residual)         multiple                  33088     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           multiple                  65600     \n",
      "                                                                 \n",
      " resnet_block_2 (ResnetBlock  multiple                 296064    \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_3 (ResnetBlock  multiple                 361472    \n",
      " )                                                               \n",
      "                                                                 \n",
      " residual_1 (Residual)       multiple                  66176     \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          multiple                  262272    \n",
      "                                                                 \n",
      " resnet_block_4 (ResnetBlock  multiple                 1050880   \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_5 (ResnetBlock  multiple                 1312768   \n",
      " )                                                               \n",
      "                                                                 \n",
      " residual_2 (Residual)       multiple                  132352    \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          multiple                  1048832   \n",
      "                                                                 \n",
      " resnet_block_6 (ResnetBlock  multiple                 3936768   \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_7 (ResnetBlock  multiple                 4984832   \n",
      " )                                                               \n",
      "                                                                 \n",
      " residual_3 (Residual)       multiple                  264704    \n",
      "                                                                 \n",
      " identity_4 (Identity)       multiple                  0         \n",
      "                                                                 \n",
      " resnet_block_10 (ResnetBloc  multiple                 3344640   \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_11 (ResnetBloc  multiple                 1312768   \n",
      " k)                                                              \n",
      "                                                                 \n",
      " residual_5 (Residual)       multiple                  132352    \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  multiple                 1048832   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " resnet_block_12 (ResnetBloc  multiple                 869504    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_13 (ResnetBloc  multiple                 361472    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " residual_6 (Residual)       multiple                  66176     \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  multiple                 262272    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " resnet_block_14 (ResnetBloc  multiple                 234048    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_15 (ResnetBloc  multiple                 107008    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " residual_7 (Residual)       multiple                  33088     \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  multiple                 65600     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " resnet_block_8 (ResnetBlock  multiple                 4984832   \n",
      " )                                                               \n",
      "                                                                 \n",
      " residual_4 (Residual)       multiple                  1041920   \n",
      "                                                                 \n",
      " resnet_block_9 (ResnetBlock  multiple                 4984832   \n",
      " )                                                               \n",
      "                                                                 \n",
      " output (Sequential)         (1, 32, 32, 1)            119297    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,073,077\n",
      "Trainable params: 33,073,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ea24ac9-a7cf-445d-aac4-491f303b1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(real, generated):\n",
    "    loss = tf.math.reduce_mean((real - generated) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b971a0c7-442e-4fd9-9436-d78b34611fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = 0\n",
    "\n",
    "def train_step(batch):\n",
    "    rng, tsrng = np.random.randint(0, 100000, size=(2,))\n",
    "    timestep_values = generate_timestamp(tsrng, batch.shape[0])\n",
    "\n",
    "    noised_image, noise = forward_noise(rng, batch, timestep_values)\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = unet(noised_image, timestep_values)\n",
    "        \n",
    "        loss_value = loss_fn(noise, prediction)\n",
    "    \n",
    "    gradients = tape.gradient(loss_value, unet.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, unet.trainable_variables))\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fcdde90-1ef9-4819-87c1-6d38f8d82bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _eager_dataset_iterator at 0x7f3264484eb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f612e54-129b-490c-bd95-e5307abe9b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:52:06.838512: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x892f5910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-16 20:52:06.838536: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-05-16 20:52:06.841899: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-16 20:52:06.899476: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-16 20:52:06.950118: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f2a7c363040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f2a7c363040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f2a7c363040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f2a7c363040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  44/1874 [..............................] - ETA: 12:38 - loss: 0.1105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:52:47.490759: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset)):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# run the training loop\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     10\u001b[0m     bar\u001b[38;5;241m.\u001b[39mupdate(i, values\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)])\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m unet(noised_image, timestep_values)\n\u001b[1;32m     11\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss_fn(noise, prediction)\n\u001b[0;32m---> 13\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m opt\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, unet\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_value\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1057\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1058\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1059\u001b[0m           output_gradients))\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1063\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1072\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:146\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:1298\u001b[0m, in \u001b[0;36m_AddGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(grad, ops\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     _ShapesFullySpecifiedAndEqual(x, y, grad)):\n\u001b[1;32m   1296\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad, grad\n\u001b[1;32m   1297\u001b[0m (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1298\u001b[0m     \u001b[43mSmartBroadcastGradientArgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_input_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m skip_input_indices:\n\u001b[1;32m   1300\u001b[0m   gx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:90\u001b[0m, in \u001b[0;36mSmartBroadcastGradientArgs\u001b[0;34m(x, y, grad)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# NOTE: It may be productive to apply these optimizations in the eager case\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# as well.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(x, ops\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, ops\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grad, ops\u001b[38;5;241m.\u001b[39mTensor)):\n\u001b[0;32m---> 90\u001b[0m   sx \u001b[38;5;241m=\u001b[39m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m   sy \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(y)\n\u001b[1;32m     92\u001b[0m   rx, ry \u001b[38;5;241m=\u001b[39m gen_array_ops\u001b[38;5;241m.\u001b[39mbroadcast_gradient_args(sx, sy)\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:664\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    641\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28minput\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out_type\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint32):\n\u001b[1;32m    643\u001b[0m   \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m    644\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns the shape of a tensor.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m  This operation returns a 1-D integer tensor representing the shape of `input`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m    A `Tensor` of type `out_type`.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:705\u001b[0m, in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_type:\n\u001b[1;32m    704\u001b[0m   out_type \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mint32\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Diffusion/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:9380\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   9378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   9379\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 9380\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9381\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   9382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   9383\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for e in range(1, epochs+1):\n",
    "    # this is cool utility in Tensorflow that will create a nice looking progress bar\n",
    "    bar = tf.keras.utils.Progbar(len(dataset)-1)\n",
    "    losses = []\n",
    "    for i, batch in enumerate(iter(dataset)):\n",
    "        # run the training loop\n",
    "        loss = train_step(batch)\n",
    "        losses.append(loss)\n",
    "        bar.update(i, values=[(\"loss\", loss)])\n",
    "\n",
    "    avg = np.mean(losses)\n",
    "    print(f\"Average loss for epoch {e}/{epochs}: {avg}\")\n",
    "    ckpt_manager.save(checkpoint_number=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a4c08-119d-4fdc-9483-e3b5b1de7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a GIF using logged images\n",
    "def save_gif(img_list, path=\"\", interval=200):\n",
    "    # Transform images from [-1,1] to [0, 255]\n",
    "    imgs = []\n",
    "    for im in img_list:\n",
    "        im = np.array(im)\n",
    "        im = (im + 1) * 127.5\n",
    "        im = np.clip(im, 0, 255).astype(np.int32)\n",
    "        im = Image.fromarray(im)\n",
    "        imgs.append(im)\n",
    "    \n",
    "    imgs = iter(imgs)\n",
    "\n",
    "    # Extract first image from iterator\n",
    "    img = next(imgs)\n",
    "\n",
    "    # Append the other images and save as GIF\n",
    "    img.save(fp=path, format='GIF', append_images=imgs,\n",
    "             save_all=True, duration=interval, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78d592-262f-4b70-9277-3fc4e10b889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ddpm(x_t, pred_noise, t):\n",
    "    alpha_t = np.take(alpha, t)\n",
    "    alpha_t_bar = np.take(alpha_bar, t)\n",
    "\n",
    "    eps_coef = (1 - alpha_t) / (1 - alpha_t_bar) ** .5\n",
    "    mean = (1 / (alpha_t ** .5)) * (x_t - eps_coef * pred_noise)\n",
    "\n",
    "    var = np.take(beta, t)\n",
    "    z = np.random.normal(size=x_t.shape)\n",
    "\n",
    "    return mean + (var ** .5) * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350caa1-f067-46b5-a9ec-471be96ba7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal((1,32,32,1))\n",
    "img_list = []\n",
    "img_list.append(np.squeeze(np.squeeze(x, 0),-1))\n",
    "\n",
    "for i in tqdm(range(timesteps-1)):\n",
    "    t = np.expand_dims(np.array(timesteps-i-1, np.int32), 0)\n",
    "    pred_noise = unet(x, t)\n",
    "    x = ddpm(x, pred_noise, t)\n",
    "    img_list.append(np.squeeze(np.squeeze(x, 0),-1))\n",
    "\n",
    "save_gif(img_list + ([img_list[-1]] * 100), \"ddpm.gif\", interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f58dd6-c4fa-4155-b81a-c63dcc544a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddim(x_t, pred_noise, t, sigma_t):\n",
    "    alpha_t_bar = np.take(alpha_bar, t)\n",
    "    alpha_t_minus_one = np.take(alpha, t-1)\n",
    "\n",
    "    pred = (x_t - ((1 - alpha_t_bar) ** 0.5) * pred_noise)/ (alpha_t_bar ** 0.5)\n",
    "    pred = (alpha_t_minus_one ** 0.5) * pred\n",
    "\n",
    "    pred = pred + ((1 - alpha_t_minus_one - (sigma_t ** 2)) ** 0.5) * pred_noise\n",
    "    eps_t = np.random.normal(size=x_t.shape)\n",
    "    pred = pred+(sigma_t * eps_t)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745aa87-00dc-4b7a-a7cc-b63f37634614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of inference loops to run\n",
    "inference_timesteps = 10\n",
    "\n",
    "# Create a range of inference steps that the output should be sampled at\n",
    "inference_range = range(0, timesteps, timesteps // inference_timesteps)\n",
    "\n",
    "x = tf.random.normal((1,32,32,1))\n",
    "img_list = []\n",
    "img_list.append(np.squeeze(np.squeeze(x, 0),-1))\n",
    "\n",
    "# Iterate over inference_timesteps\n",
    "for index, i in tqdm(enumerate(reversed(range(inference_timesteps))), total=inference_timesteps):\n",
    "    t = np.expand_dims(inference_range[i], 0)\n",
    "\n",
    "    pred_noise = unet(x, t)\n",
    "\n",
    "    x = ddim(x, pred_noise, t, 0)\n",
    "    img_list.append(np.squeeze(np.squeeze(x, 0),-1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ccccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffusion",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
