{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9110656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import photoz_utils\n",
    "import random\n",
    "import h5py\n",
    "import make_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data_rockfish.make_hsc_v6_small_z(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa03495f-3968-41f6-8a37-4b9534d8cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data_rockfish.make_hsc_v6_large_z(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d84a43-5424-4ace-9388-808c986df20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output /data/HSC/HSC_v6/temp/5x127x127_testing_z_less_than_2.hdf5\n"
     ]
    }
   ],
   "source": [
    "make_data.make_hsc_v6_small_z(cap = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb22eac-a831-47da-b7ee-3120f6644f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output /data/HSC/HSC_v6/temp/5x127x127_testing_z_more_than_2.hdf5\n"
     ]
    }
   ],
   "source": [
    "make_data.make_hsc_v6_large_z(cap = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4efbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data.make_hsc_v6_large_z(cap = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data.make_hsc_v6_large()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05838316",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data.make_hsc_v6_small_hdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f46d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data.make_hsc_v6_small_hdf_single()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70fcf1-05e5-43bb-a29d-363df625782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hsc_v6_large_double(ntrain=1,ntest=1,nvalidation=2000):\n",
    "    inputfile_1 = '5x64x64_validation.hdf5'\n",
    "    inputfile_2 = '5x127x127_validation.hdf5'\n",
    "    directory_1 = '/data/HSC/HSC_v6/temp/'\n",
    "    directory_2 = '/data/HSC/HSC_v6/temp/'\n",
    "    current_file_1 = os.path.join(directory_1, inputfile_1)\n",
    "    current_file_2 = os.path.join(directory_2, inputfile_2)\n",
    "    hf_1 = h5py.File(current_file_1,'r')\n",
    "    hf_2 = h5py.File(current_file_2,'r')\n",
    "    \n",
    "    length_1 = len(hf_1['image'])\n",
    "    length_2 = len(hf_2['image'])\n",
    "    print(length_1)\n",
    "    print(length_2)\n",
    "    if length_1 != length_2:\n",
    "        stop\n",
    "    inds = random.sample(list(np.arange(length_1)),ntrain+ntest+nvalidation)\n",
    "    inds_train = np.sort(inds[:ntrain])\n",
    "    inds_test = np.sort(inds[ntrain:ntrain+ntest])\n",
    "    inds_validation = np.sort(inds[ntrain+ntest:])\n",
    "\n",
    "    part_1 = os.path.splitext(current_file_1)\n",
    "    part_2 = os.path.splitext(current_file_2)\n",
    "    subsizes = [ntrain, ntest, nvalidation]\n",
    "    file_ends = ['_training_small', '_testing_small', '_small']\n",
    "    ind_list = [inds_train, inds_test, inds_validation]\n",
    "    \n",
    "    for subsize, file_end, ind in zip(subsizes, file_ends, ind_list):\n",
    "        f_1 = h5py.File(part_1[0] + file_end + part_1[1], 'w')\n",
    "        f_2 = h5py.File(part_2[0] + file_end + part_2[1], 'w')\n",
    "        for k in hf_1.keys():\n",
    "            tmp = hf_1[k]\n",
    "            subshape = list(np.shape(tmp))\n",
    "            subshape[0] = subsize\n",
    "            dataset = f_1.create_dataset(k,shape=subshape,dtype=tmp.dtype)\n",
    "            for i, index in enumerate(ind):\n",
    "                dataset[i] = tmp[index]\n",
    "            tmp = None\n",
    "        for k in hf_2.keys():\n",
    "            tmp = hf_2[k]\n",
    "            subshape = list(np.shape(tmp))\n",
    "            subshape[0] = subsize\n",
    "            dataset = f_2.create_dataset(k,shape=subshape,dtype=tmp.dtype)\n",
    "            for i, index in enumerate(ind):\n",
    "                dataset[i] = tmp[index]\n",
    "            tmp = None\n",
    "        f_1.close()\n",
    "        f_2.close()\n",
    "    hf_1.close()\n",
    "    hf_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b8bea-d3fd-4ec5-b328-8ad4db2110a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hsc_v6_large_double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b282e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
