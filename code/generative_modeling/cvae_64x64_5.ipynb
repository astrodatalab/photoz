{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23f862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import stdout\n",
    "from datetime import datetime\n",
    "import neptune as neptune\n",
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "from astropy.io import fits\n",
    "import random\n",
    "from DataMakerCVAE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2032fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database is locked')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Cropping2D, ZeroPadding2D, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import layers as tfpl\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.nn import leaky_relu\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e0800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128\n",
    "BASE_DEPTH = 8\n",
    "IMAGE_SHAPE = (5, 64, 64)\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 250\n",
    "CHECKPOINTS_TO_SAVE = 4\n",
    "KL_WEIGHT = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a44e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 10:57:10.755550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 10:57:10.836665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 10:57:10.836869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 10:57:10.840109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 10:57:10.840377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 10:57:10.840587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 10:57:11.522320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 10:57:11.522514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 10:57:11.522653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-01 10:57:11.522761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20000 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU');\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 20000)]);\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU');\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb7cabe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'write' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(logs_path, exist_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(predictions_path, exist_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mwrite\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data3/Billy/models/README.md\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m myfile:\n\u001b[1;32m     22\u001b[0m         myfile\u001b[38;5;241m.\u001b[39mwrite(MODEL_TYPE \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m MODEL_SUBVERSION \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m MODEL_DESCRIPTION \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - B. Li\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'write' is not defined"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "DATASET_NAME = \"HSC_v6\"\n",
    "MODEL_TYPE = \"CVAE\"\n",
    "MODEL_VERSION = \"v1.0\"\n",
    "MODEL_SUBVERSION = \"v1.0.\" + now.strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "\n",
    "model_id = '_'.join([DATASET_NAME, MODEL_TYPE, MODEL_VERSION])\n",
    "model_path = os.path.join('/data3/Billy/models', model_id, MODEL_SUBVERSION, 'model')\n",
    "checkpoints_path = os.path.join('/data3/Billy/models', model_id, MODEL_SUBVERSION, 'checkpoints')\n",
    "logs_path = os.path.join('/data3/Billy/logs', model_id, MODEL_SUBVERSION)\n",
    "predictions_path = os.path.join('/data3/Billy/predictions', model_id, MODEL_SUBVERSION)\n",
    "weights_path = model_path + '/data3/Billy/CVAE/weights.h5'\n",
    "\n",
    "os.makedirs(model_path, exist_ok = True)\n",
    "os.makedirs(checkpoints_path, exist_ok = True)\n",
    "os.makedirs(logs_path, exist_ok = True)\n",
    "os.makedirs(predictions_path, exist_ok = True)\n",
    "\n",
    "if write == True:\n",
    "    with open(\"/data3/Billy/models/README.md\", \"a\") as myfile:\n",
    "        myfile.write(MODEL_TYPE + \" \" + MODEL_SUBVERSION + \" - \" + MODEL_DESCRIPTION + \" - B. Li\" + \"\\n\")\n",
    "else:\n",
    "    with open(\"/data3/Billy/models/README.md\", \"a\") as myfile:\n",
    "        myfile.write(MODEL_TYPE + \" \" + MODEL_SUBVERSION + \" - ... \"+ \" - B. Li\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc0eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_gen = {'X_key': 'image',\n",
    "    'y_key': 'specz_redshift',\n",
    "    'scaler': False,\n",
    "    'labels_encoding': False,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'mode': 'train',\n",
    "    'shuffle': False}\n",
    "\n",
    "train_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step3A/64x64_training_z_less_than_2.hdf5', **args_gen)\n",
    "test_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step3A/64x64_testing_z_less_than_2.hdf5', **args_gen)\n",
    "val_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step3A/64x64_validation_z_less_than_2.hdf5', **args_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = BATCH_SIZE\n",
    "l = len(train_gen._indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d75f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(LATENT_DIM), scale = 1.0), reinterpreted_batch_ndims = 1)\n",
    "\n",
    "# Encoder\n",
    "images = Input(shape = IMAGE_SHAPE)\n",
    "redshifts = Input(shape = (1,))\n",
    "conv1 = Conv2D(32, 3, strides = 2, activation = \"relu\", padding = 'same', data_format = 'channels_first')(images)\n",
    "conv2 = Conv2D(64, 3, strides = 2, activation = \"relu\", padding = 'same', data_format = 'channels_first')(conv1)\n",
    "conv3 = Conv2D(128, 2, strides = 1, activation = \"relu\", padding = 'same', data_format = 'channels_first')(conv2)\n",
    "conv4 = Conv2D(256, 2, strides = 1, activation = \"relu\", padding = 'same', data_format = 'channels_first')(conv3)\n",
    "flatten = Flatten()(conv4)\n",
    "concat = tf.keras.layers.Concatenate()([flatten, redshifts])\n",
    "dense1 = Dense(1024, activation = \"relu\")(concat)\n",
    "dense2 = Dense(1024, activation = \"relu\")(dense1)\n",
    "dense3 = Dense(tfpl.MultivariateNormalTriL.params_size(LATENT_DIM), activation = None)(dense2)\n",
    "z = tfpl.MultivariateNormalTriL(LATENT_DIM, activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight = KL_WEIGHT))(dense3)\n",
    "encoder = Model([images, redshifts], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zc = Input(shape = (LATENT_DIM + 1, ))\n",
    "dense4 = Dense(1024, activation = \"relu\")(zc)\n",
    "dense5 = Dense(1024, activation = \"relu\")(dense4) # Size of the dense layer changed\n",
    "dense6 = Dense(256 * 16 * 16, activation = \"relu\")(dense5) # Size of the dense layer changed\n",
    "reshape = Reshape((256, 16, 16))(dense6) # Reshape size changed\n",
    "conv5 = Conv2DTranspose(128, 2, strides = 1, activation = \"relu\", padding = 'same', data_format = 'channels_first')(reshape)\n",
    "conv6 = Conv2DTranspose(64, 2, strides = 1, activation = \"relu\", padding = 'same', data_format = 'channels_first')(conv5)\n",
    "conv7 = Conv2DTranspose(32, 3, strides = 2, activation = \"relu\", padding = 'same', data_format = 'channels_first')(conv6)\n",
    "conv8 = Conv2DTranspose(5, 3, strides = 2, activation = \"sigmoid\", padding = 'same', data_format = 'channels_first')(conv7)\n",
    "outputs = Reshape(IMAGE_SHAPE)(conv8)\n",
    "decoder = Model(zc, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder([images, redshifts])\n",
    "zc = tf.keras.layers.Concatenate()([z, redshifts])  # Assuming you want to concat the output of encoder and redshifts\n",
    "decoder_output = decoder(zc)\n",
    "\n",
    "model = Model(inputs=[images, redshifts], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fe180",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()\n",
    "decoder.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate = 0.0001)\n",
    "model.compile(optimizer = optimizer, loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    name = MODEL_SUBVERSION,\n",
    "    project = \"astro-data-lab/VAE\",\n",
    "    api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxOGFlZGMxOC04MWU5LTQ2NDctYjlhZS05NGE2NGQ0NmIzMmEifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "# logs_callback = TensorBoard(log_dir = logs_path)\n",
    "\n",
    "weights_callback = ModelCheckpoint(filepath = os.path.join(checkpoints_path, 'weights_epoch{epoch}.hdf5'), save_freq = int(EPOCHS/CHECKPOINTS_TO_SAVE), save_weights_only = True)\n",
    "\n",
    "LR_callback = ReduceLROnPlateau()\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch <= 100:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr / 10\n",
    "\n",
    "LR_modify_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose = 0)\n",
    "\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs_path, histogram_freq = 1)\n",
    "\n",
    "neptune_callback = NeptuneCallback(run = run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_gen, epochs = 100, callbacks = [neptune_callback, LR_callback], validation_data = val_gen, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = 3\n",
    "index = 466\n",
    "z = test_gen[lib][0][1][index]\n",
    "z_shifted = 2\n",
    "band = 0\n",
    "print(f'z = {z}')\n",
    "str = np.asarray(encoder([np.array([test_gen[lib][0][0][index]]), np.array([test_gen[lib][0][1][index]])])[0])\n",
    "str = np.concatenate((str, np.array([z])))\n",
    "str = str.reshape(1, LATENT_DIM + 1)\n",
    "str_shifted = np.asarray(encoder([np.array([test_gen[lib][0][0][index]]), np.array([test_gen[lib][0][1][index]])])[0])\n",
    "str_shifted = np.concatenate((str_shifted, np.array([z_shifted])))\n",
    "str_shifted = str_shifted.reshape(1, LATENT_DIM + 1)\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = (10, 5))\n",
    "axes[0].imshow(test_gen[lib][0][0][index][band], cmap = 'afmhot')\n",
    "axes[1].imshow(decoder([str])[0][band], cmap = 'afmhot')\n",
    "axes[2].imshow(decoder([str_shifted])[0][band], cmap = 'afmhot')\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[2].axis('off')\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[1].set_title(\"Regenerated\")\n",
    "axes[2].set_title(\"Regenerated Far\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = 5\n",
    "index = 273\n",
    "z = test_gen[lib][0][1][index]\n",
    "z_shifted = 0.1\n",
    "band = 0\n",
    "print(f'z = {z}')\n",
    "str = np.asarray(encoder([np.array([test_gen[lib][0][0][index]]), np.array([test_gen[lib][0][1][index]])])[0])\n",
    "str = np.concatenate((str, np.array([z])))\n",
    "str = str.reshape(1, LATENT_DIM + 1)\n",
    "str_shifted = np.asarray(encoder([np.array([test_gen[lib][0][0][index]]), np.array([test_gen[lib][0][1][index]])])[0])\n",
    "str_shifted = np.concatenate((str_shifted, np.array([z_shifted])))\n",
    "str_shifted = str_shifted.reshape(1, LATENT_DIM + 1)\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = (10, 5))\n",
    "axes[0].imshow(test_gen[lib][0][0][index][band], cmap = 'afmhot')\n",
    "axes[1].imshow(decoder([str])[0][band], cmap = 'afmhot')\n",
    "axes[2].imshow(decoder([str_shifted])[0][band], cmap = 'afmhot')\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[2].axis('off')\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[1].set_title(\"Regenerated\")\n",
    "axes[2].set_title(\"Regenerated Close\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534af846",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = 0\n",
    "str = prior.sample(1)[0]\n",
    "strlo = np.concatenate((str, np.array([0.1])))\n",
    "strlo = strlo.reshape(1, LATENT_DIM + 1)\n",
    "strhi = np.concatenate((str, np.array([2])))\n",
    "strhi = strhi.reshape(1, LATENT_DIM + 1)\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 5))\n",
    "axes[0].imshow(decoder([strlo])[0][band], cmap = 'afmhot')\n",
    "axes[1].imshow(decoder([strhi])[0][band], cmap = 'afmhot')\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[0].set_title(\"Low z\")\n",
    "axes[1].set_title(\"High z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e3dd1",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 5))\n",
    "x_array = np.arange(5)\n",
    "lo_array = np.array([])\n",
    "hi_array = np.array([])\n",
    "for j in range(0, 5):\n",
    "    lo_array = np.append(lo_array, decoder([strlo])[0][j][63][63])\n",
    "    hi_array = np.append(hi_array, decoder([strhi])[0][j][63][63])\n",
    "axes[0].scatter(x_array, lo_array, c = 'blue', label = 'Low')\n",
    "axes[1].scatter(x_array, hi_array, c = 'red', label = 'High')\n",
    "axes[0].set_xlabel(\"Bands\")\n",
    "axes[0].set_ylabel(\"Central Pixel Value\")\n",
    "axes[0].legend()\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (5, 5))\n",
    "x_array = np.arange(5)\n",
    "lo_array = np.array([])\n",
    "hi_array = np.array([])\n",
    "for j in range(0, 5):\n",
    "    lo_array = np.append(lo_array, np.amax(np.transpose(np.asarray(decoder([strlo])[0][j][60:67]))[60:67].flatten()))\n",
    "    hi_array = np.append(hi_array, np.amax(np.transpose(np.asarray(decoder([strhi])[0][j][60:67]))[60:67].flatten()))\n",
    "axes.scatter(x_array, lo_array, c = 'blue', label = 'Low')\n",
    "axes.scatter(x_array, hi_array, c = 'red', label = 'High')\n",
    "axes.set_xlabel(\"Bands\")\n",
    "axes.set_ylabel(\"Max Pixel Value Near Center\")\n",
    "axes.legend()\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f31360",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(3, 80):\n",
    "    for i in range(0, 512):\n",
    "        if test_gen[j][0][1][i] <= 0.1:\n",
    "            print(j)\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(3, 80):\n",
    "    for i in range(0, 512):\n",
    "        if test_gen[j][0][1][i] >= 3.5:\n",
    "            print(j)\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9448c282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 1 samples of 42960"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m index \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m BATCH_SIZE \u001b[38;5;241m+\u001b[39m j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m stdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mChecking \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m samples of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m42960\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mtest_gen\u001b[49m[i][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][j]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(encoder([np\u001b[38;5;241m.\u001b[39marray([test_gen[i][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][j]]), np\u001b[38;5;241m.\u001b[39marray([test_gen[i][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][j]])])[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39marray([z])))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_gen' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0, 84):\n",
    "    l = 512\n",
    "    if i == 83:\n",
    "        l = 464\n",
    "    for j in range(0, l):\n",
    "        index = i * BATCH_SIZE + j + 1\n",
    "        stdout.write(\"\\rChecking %d samples of \" % (index) + \"42960\")\n",
    "        z = test_gen[i][0][1][j]\n",
    "        str = np.asarray(encoder([np.array([test_gen[i][0][0][j]]), np.array([test_gen[i][0][1][j]])])[0])\n",
    "        str = np.concatenate((str, np.array([z])))\n",
    "        str = str.reshape(1, LATENT_DIM + 1)\n",
    "        hdul = fits.PrimaryHDU(data = test_gen[i][0][0][j])\n",
    "        string = f\"/data/CVAE Generated/Full/Original Galaxy #{index}.fits\"\n",
    "        hdul.writeto(string, overwrite = True)\n",
    "        hdul = fits.PrimaryHDU(data = decoder([str])[0])\n",
    "        string = f\"/data/CVAE Generated/Full/Reconstructed Galaxy #{index}.fits\"\n",
    "        hdul.writeto(string, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8092e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_bands_max_near_center(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show / 5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_array = np.append(true_array, np.amax(np.array([test_gen[0][0][0][i][j]])))\n",
    "            pred_array = np.append(pred_array, np.amax(np.transpose(np.asarray(model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j][60:67]))[60:67].flatten()))\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel(\"Bands\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel(f\"Maximum normalized pixel value z = {np.array([test_gen[0][0][1][i]])[0]}\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "\n",
    "def scatter_bands_max(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show / 5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_array = np.append(true_array, np.amax(np.array([test_gen[0][0][0][i][j]])))\n",
    "            pred_array = np.append(pred_array, np.amax(np.array([model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j]])))\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel(\"Bands\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel(\"Maximum normalized pixel value\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "        \n",
    "def scatter_bands_center(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show / 5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_array = np.append(true_array, np.array([test_gen[0][0][0][i][j]])[0][63][63])\n",
    "            pred_array = np.append(pred_array, model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j][63][63])\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel(\"Bands\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel(f\"Central pixel value z = {np.array([test_gen[0][0][1][i]])}\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "        \n",
    "def scatter_bands_center_shift(lib = 3, i = 37, z = 1):\n",
    "    print(z)\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (5, 5))\n",
    "    x_array = np.arange(5)\n",
    "    true_array = np.array([])\n",
    "    pred_array = np.array([])\n",
    "    for j in range(0, 5):\n",
    "        true_array = np.append(true_array, np.array([test_gen[lib][0][0][i][j]])[0][63][63])\n",
    "        pred_array = np.append(pred_array, model([np.array([test_gen[lib][0][0][i]]), np.array([z])])[0][j][63][63])\n",
    "    axes.scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "    axes.scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "    axes.set_xlabel(\"Bands\")\n",
    "    axes.set_ylabel(\"Central pixel value\")\n",
    "    axes.legend()\n",
    "    \n",
    "def scatter_bands_max_shift(lib = 3, i = 37, z = 1):\n",
    "    print(z)\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (5, 5))\n",
    "    x_array = np.arange(5)\n",
    "    true_array = np.array([])\n",
    "    pred_array = np.array([])\n",
    "    for j in range(0, 5):\n",
    "        true_array = np.append(true_array, np.amax(np.array([test_gen[lib][0][0][i][j]])))\n",
    "        pred_array = np.append(pred_array, np.amax(np.array([model([np.array([test_gen[lib][0][0][i]]), np.array([z])])[0][j]])))\n",
    "    axes.scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "    axes.scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "    axes.set_xlabel(\"Bands\")\n",
    "    axes.set_ylabel(\"Central pixel value\")\n",
    "    axes.legend()\n",
    "\n",
    "def scatter_bands_percentile(percentile = 90, num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show /  5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_90 = np.percentile(np.array([test_gen[0][0][0][i][j]]).flatten(), percentile)\n",
    "            pred_90 = np.percentile(np.asarray(model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j]).flatten(), percentile)\n",
    "            true_array = np.append(true_array, true_90)\n",
    "            pred_array = np.append(pred_array, pred_90)\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel('Bands')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel(f'{percentile}th percentile normalized pixel value')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "\n",
    "def scatter_bands_mean(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show / 5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_mean = np.mean(np.array([test_gen[0][0][0][i][j]]).flatten())\n",
    "            pred_mean = np.mean(np.asarray(model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j]).flatten())\n",
    "            true_array = np.append(true_array, true_mean)\n",
    "            pred_array = np.append(pred_array, pred_mean)\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel('Bands')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel('Mean normalized pixel value')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "\n",
    "def display_histograms(num_to_show = 2, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = num_to_show, ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        for j in range(0, 5):\n",
    "            true_arr = sorted(np.array([test_gen[0][0][0][i][j]]).flatten())\n",
    "            pred_arr = sorted(np.asarray(model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j]).flatten())\n",
    "            true_arr = true_arr[0 : int(len(true_arr) * .99)]\n",
    "            pred_arr = pred_arr[0 : int(len(pred_arr) * .99)]\n",
    "            axes[i][j].hist(true_arr, 100, color = 'blue', label = 'True', alpha = 0.5)\n",
    "            axes[i][j].hist(pred_arr, 100, color = 'red', label = 'Predicted', alpha = 0.5)\n",
    "            axes[i][j].set_xlabel(\"Pixel Values\")\n",
    "            axes[i][j].set_ylabel(\"Count\")\n",
    "            axes[i][j].legend()\n",
    "    fig.suptitle('Histograms of Predicted vs. True Image, Horizontal are Bands')\n",
    "    \n",
    "def display_5_bands(index):\n",
    "    fig, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (20, 10))\n",
    "    loss = round(model.evaluate([np.array([test_gen[0][0][0][index]]), np.array([test_gen[0][0][1][index]])], np.array([test_gen[0][0][0][index]]), verbose = 0), 2)\n",
    "    for i in range(0, 5):\n",
    "        axes[0][i].imshow(np.array([test_gen[0][0][0][index]])[0][i], cmap = 'afmhot')\n",
    "        max_pixel_true = round(np.amax(np.array([test_gen[0][0][0][index]])[0][i]), 2)\n",
    "        # axes[0][i].set_title(f'True band {i} max = {max_pixel_true}')\n",
    "        pred = model([np.array([test_gen[0][0][0][index]]), np.array([test_gen[0][0][1][index]])])[0][i]\n",
    "        axes[1][i].imshow(pred, cmap = 'afmhot')\n",
    "        max_pixel_pred = round(np.amax(pred), 2)\n",
    "        axes[1][i].set_title(f'\\n loss = {loss}') # f'Pred band {i} max = {max_pixel_pred}' + \n",
    "        \n",
    "def display_high_loss(num_to_show, min_loss):\n",
    "    fig, axes = plt.subplots(nrows = num_to_show, ncols = 5, figsize = (20, 5 * num_to_show))\n",
    "    r = 0\n",
    "    for i in range(BATCH_SIZE):\n",
    "        loss = round(model.evaluate([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])], np.array([test_gen[0][0][0][i]]), verbose = 0), 2)\n",
    "        if loss >= min_loss:\n",
    "            print(i)\n",
    "            for j in range(0, 5):\n",
    "                axes[r][j].imshow(np.array([test_gen[0][0][0][i]])[0][j], cmap = 'afmhot')\n",
    "                axes[r][j].set_title(f'Loss = {loss}')\n",
    "            r += 1\n",
    "        if r >= num_to_show:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77067c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_max_near_center(num_to_show = 10, index = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = np.array([])\n",
    "y_array = np.array([])\n",
    "l = len(test_gen[0][0][0])\n",
    "print(l)\n",
    "for i in range(0, l):\n",
    "    x_array = np.append(x_array, np.amax(np.array([test_gen[0][0][0][i][4]])))\n",
    "    y_array = np.append(y_array, np.array([test_gen[0][0][1][i]])[0])\n",
    "    print(i)\n",
    "plt.scatter(y_array, x_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81015273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "xy = np.asarray(np.vstack([y_array, x_array])).astype('float32')\n",
    "z = gaussian_kde(xy)(xy)\n",
    "plt.scatter(y_array, x_array, s = 5, c = z)\n",
    "plt.xlim(0,4)\n",
    "plt.ylim(0, 80)\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"Maximum Pixel Value near the Center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac674f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_center_shift(lib = 3, i = 267, z = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_max_shift(lib = 3, i = 37, z = test_gen[3][0][1][37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_max_shift(lib = 3, i = 37, z = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_mean(10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c48efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_percentile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eeb40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_5_bands(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b7232",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_high_loss(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d688ff7-5416-4f2d-8c46-f9d20a2fb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207b9cd-6a04-443f-b56f-8ba3b3bf56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('/data3/Billy/64x64_cvae_generated_11.hdf5', 'a')\n",
    "str = prior.sample(1)[0]\n",
    "str = np.concatenate((str, np.array([0.1])))\n",
    "str = str.reshape(1, LATENT_DIM + 1)\n",
    "image_g = decoder([str])[0][0]\n",
    "image_r = decoder([str])[0][1]\n",
    "image_i = decoder([str])[0][2]\n",
    "image_z = decoder([str])[0][3]\n",
    "image_y = decoder([str])[0][4]\n",
    "five_band_image = []\n",
    "five_band_image.append(image_g)\n",
    "five_band_image.append(image_r)\n",
    "five_band_image.append(image_i)\n",
    "five_band_image.append(image_z)\n",
    "five_band_image.append(image_y)\n",
    "five_band_image_reshape = np.reshape(np.array(five_band_image), [1, 5, 64, 64])\n",
    "hf.create_dataset('image', data = five_band_image_reshape, chunks = True, maxshape = (None, 5, 64, 64))\n",
    "hf.create_dataset('specz_redshift', data = [0.1], chunks = True, maxshape = (None, ))\n",
    "for i in tqdm(range(1999)):\n",
    "    z = random.uniform(0, 2)\n",
    "    str = prior.sample(1)[0]\n",
    "    str = np.concatenate((str, np.array([z])))\n",
    "    str = str.reshape(1, LATENT_DIM + 1)\n",
    "    image_g = decoder([str])[0][0]\n",
    "    image_r = decoder([str])[0][1]\n",
    "    image_i = decoder([str])[0][2]\n",
    "    image_z = decoder([str])[0][3]\n",
    "    image_y = decoder([str])[0][4]\n",
    "    five_band_image = []\n",
    "    five_band_image.append(image_g)\n",
    "    five_band_image.append(image_r)\n",
    "    five_band_image.append(image_i)\n",
    "    five_band_image.append(image_z)\n",
    "    five_band_image.append(image_y)\n",
    "    five_band_image_reshape = np.reshape(np.array(five_band_image), [1, 5, 64, 64])\n",
    "    hf['specz_redshift'].resize((hf['specz_redshift'].shape[0] + 1), axis = 0)\n",
    "    hf['specz_redshift'][hf['specz_redshift'].shape[0] - 1] = [z]\n",
    "    hf['image'].resize((hf['image'].shape[0] + 1), axis = 0)\n",
    "    hf['image'][hf['image'].shape[0] - 1, :, :, :] = five_band_image\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ccccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffusion",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
