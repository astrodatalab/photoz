{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import stdout\n",
    "from datetime import datetime\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2032fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Cropping2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import layers as tfpl\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.nn import leaky_relu\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128\n",
    "BASE_DEPTH = 8\n",
    "IMAGE_SHAPE = (5, 127, 127)\n",
    "EPOCHS = 200\n",
    "CHECKPOINTS_TO_SAVE = 4\n",
    "KL_WEIGHT = 1e-5\n",
    "# Good value: 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU');\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 20000)]);\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU');\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41928b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "DATASET_NAME = \"HSC_v6_small\"\n",
    "MODEL_TYPE = \"VAE\"\n",
    "MODEL_VERSION = \"v1.0\"\n",
    "MODEL_SUBVERSION = \"v1.0.20221118_14_03_25\"\n",
    "\n",
    "model_id = '_'.join([DATASET_NAME, MODEL_TYPE, MODEL_VERSION])\n",
    "\n",
    "model_path = os.path.join('/models', model_id, MODEL_SUBVERSION, 'model')\n",
    "\n",
    "weights_path = model_path + '/weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c90a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = h5py.File('/data/HSC/HSC_v6/step2A/127x127/5x127x127_training_small.hdf5', 'r')\n",
    "hf_test = h5py.File('/data/HSC/HSC_v6/step2A/127x127/5x127x127_testing_small.hdf5', 'r')\n",
    "hf_validation = h5py.File('/data/HSC/HSC_v6/step2A/127x127/5x127x127_validation_small.hdf5', 'r')\n",
    "x_train = np.asarray(hf_train['image'][0:])\n",
    "x_test = np.asarray(hf_test['image'][0:])\n",
    "x_validation = np.asarray(hf_validation['image'][0:])\n",
    "max_value = 4.16\n",
    "x_train = np.true_divide(x_train, max_value)\n",
    "x_test = np.true_divide(x_test, max_value)\n",
    "x_validation = np.true_divide(x_validation, max_value)\n",
    "y_train = np.asarray(hf_train['specz_redshift'][0:])[..., None]\n",
    "y_test = np.asarray(hf_test['specz_redshift'][0:])[..., None]\n",
    "y_validation = np.asarray(hf_validation['specz_redshift'][0:])[..., None]\n",
    "# object_id_train = np.asarray(hf_train['object_id'][0:])\n",
    "# object_id = np.asarray(hf_test['object_id'][0:])\n",
    "# object_id_validation = np.asarray(hf_validation['object_id'][0:])\n",
    "hf_train.close()\n",
    "hf_test.close()\n",
    "hf_validation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(LATENT_DIM), scale = 0.1), reinterpreted_batch_ndims = 1)\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        images = Input(shape = IMAGE_SHAPE)\n",
    "        x = Conv2D(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                   padding = 'same', data_format = 'channels_first')(images)\n",
    "        x = Conv2D(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                   padding = 'same', data_format = 'channels_first')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(tfpl.MultivariateNormalTriL.params_size(LATENT_DIM), activation = None)(x)\n",
    "        z = tfpl.MultivariateNormalTriL(LATENT_DIM,\n",
    "                  activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight = KL_WEIGHT))(x)\n",
    "        self.encoder = Model(images, z, name = 'encoder')\n",
    "\n",
    "        latents = Input(shape = LATENT_DIM)\n",
    "        x = Dense(8 * LATENT_DIM * 32 * 32, activation = None)(latents)\n",
    "        x = Reshape((8 * LATENT_DIM, 32, 32))(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                            padding = 'same', data_format = 'channels_first')(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                            padding = 'same', data_format = 'channels_first')(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides = 1, activation = leaky_relu,\n",
    "                            padding = 'same', data_format = 'channels_first')(x)\n",
    "        x = Conv2D(IMAGE_SHAPE[0], 3, strides = 1, activation = None, \n",
    "                   padding = 'same', data_format = 'channels_first')(x)\n",
    "        outputs = Cropping2D(cropping=((0, 1), (0, 1)), data_format = 'channels_first')(x)\n",
    "        self.decoder = Model(latents, outputs, name = 'decoder')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def summary(self):\n",
    "        print(self.encoder.summary())\n",
    "        print(self.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c729aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init(\n",
    "    name = MODEL_SUBVERSION,\n",
    "    project = \"astro-data-lab/VAE\",\n",
    "    api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxOGFlZGMxOC04MWU5LTQ2NDctYjlhZS05NGE2NGQ0NmIzMmEifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "# logs_callback = TensorBoard(log_dir = logs_path)\n",
    "\n",
    "LR_callback = ReduceLROnPlateau()\n",
    "\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs_path, histogram_freq = 1)\n",
    "\n",
    "neptune_callback = NeptuneCallback(run = run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02153ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = x_train, y = x_train, epochs = EPOCHS, callbacks = [LR_callback, neptune_callback], validation_data = (x_validation, x_validation), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca798a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_galaxies(num_to_generate = 10):\n",
    "    z = prior.sample(num_to_generate)\n",
    "    xhat = model.decoder(z)\n",
    "    fig, axes = plt.subplots(nrows = num_to_generate, ncols = 5, figsize = (4 * 5, 4 * num_to_generate))\n",
    "    for i in range(num_to_generate):\n",
    "        for j in range(0,5):\n",
    "            axes[i][j].imshow(xhat[i][j], cmap = 'afmhot')\n",
    "            axes[i][j].set_title(f'Generated image {i} band {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_example_galaxies(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af68910",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list_g = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/g_band\"))\n",
    "image_list_r = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/r_band\"))\n",
    "image_list_i = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/i_band\"))\n",
    "image_list_z = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/z_band\"))\n",
    "image_list_y = sorted(os.listdir(\"/mnt/data/HSC/HSC_v6/step1/y_band\"))\n",
    "\n",
    "photozdata = pd.read_csv('/mnt/data/HSC/HSC_v6/HSC_v6.csv')\n",
    "photozdata.describe()\n",
    "\n",
    "b = np.argsort(photozdata['object_id'])\n",
    "sorted_photozdata = photozdata.iloc[b][:]\n",
    "photozdata = sorted_photozdata\n",
    "\n",
    "hf_in = h5py.File('/mnt/data/HSC/HSC_v6/step3/127x127_in.hdf5', 'a')\n",
    "hf_out = h5py.File('/mnt/data/HSC/HSC_v6/step3/127x127_out.hdf5', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1202eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (columnName, columnData) in photozdata.iteritems():\n",
    "    print(columnName)\n",
    "    if columnName == 'specz_name' or columnName == 'coord':\n",
    "        a = np.array(photozdata[columnName]).astype(str)\n",
    "        b = np.reshape(a, [286401, 1])\n",
    "        hf_in.create_dataset(columnName, data = b.astype('S'))\n",
    "        hf_out.create_dataset(columnName, data = b.astype('S'))\n",
    "\n",
    "        continue\n",
    "    hf_in.create_dataset(columnName, data = photozdata[columnName])\n",
    "    hf_out.create_dataset(columnName, data = photozdata[columnName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01132a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_in = 0\n",
    "count_out = 0\n",
    "size = len(image_list_g)\n",
    "for i in range(size):\n",
    "    \n",
    "    stdout.write(\"\\rChecking %d samples of \" % (i + 1) + str(size))\n",
    "    \n",
    "    object_id = image_list_g[i][0:17]\n",
    "\n",
    "    five_band_image = []\n",
    "\n",
    "    image_g = fits.open(\"/mnt/data/HSC/HSC_v6/step1/g_band/\" + image_list_g[i])\n",
    "    image_r = fits.open(\"/mnt/data/HSC/HSC_v6/step1/r_band/\" + image_list_r[i])\n",
    "    image_i = fits.open(\"/mnt/data/HSC/HSC_v6/step1/i_band/\" + image_list_i[i])\n",
    "    image_z = fits.open(\"/mnt/data/HSC/HSC_v6/step1/z_band/\" + image_list_z[i])\n",
    "    image_y = fits.open(\"/mnt/data/HSC/HSC_v6/step1/y_band/\" + image_list_y[i])\n",
    "\n",
    "    image_g_data = image_g[1].data\n",
    "    image_r_data = image_r[1].data\n",
    "    image_i_data = image_i[1].data\n",
    "    image_z_data = image_z[1].data\n",
    "    image_y_data = image_y[1].data\n",
    "\n",
    "    pad1 = int((127 - len(image_g_data)) / 2)\n",
    "    pad2 = 127 - len(image_g_data) - pad1\n",
    "    pad3 = int((127 - len(image_g_data[0])) / 2)\n",
    "    pad4 = 127 - len(image_g_data[0]) - pad3\n",
    "\n",
    "\n",
    "    im_g = np.pad(image_g_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "    im_r = np.pad(image_r_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "    im_i = np.pad(image_i_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "    im_z = np.pad(image_z_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "    im_y = np.pad(image_y_data, ((pad1, pad2), (pad3, pad4)), \"constant\", constant_values = ((0, 0), (0, 0)))\n",
    "\n",
    "    im = np.true_divide(np.array([im_g, im_r, im_i, im_z, im_y]), max_value)\n",
    "    loss = model.evaluate(np.array([im]), np.array([im]), verbose = 0)\n",
    "    if loss <= 0.1:\n",
    "        five_band_image.append(im_g)\n",
    "        five_band_image.append(im_r)\n",
    "        five_band_image.append(im_i)\n",
    "        five_band_image.append(im_z)\n",
    "        five_band_image.append(im_y)\n",
    "\n",
    "        five_band_image_reshape = np.reshape(np.array(five_band_image), [1, 5, 127, 127])\n",
    "\n",
    "        photozdata_subset = photozdata.iloc[i]\n",
    "\n",
    "        specz = photozdata_subset[\"specz_redshift\"]\n",
    "        specz_reshape = np.reshape(specz, [1, 1])\n",
    "\n",
    "        if count_in == 0:\n",
    "            hf_in.create_dataset('image', data = five_band_image_reshape, chunks = True, maxshape = (None, 5, 127, 127))\n",
    "\n",
    "        else:\n",
    "            hf_in['image'].resize((hf_in['image'].shape[0] + 1), axis = 0)\n",
    "            hf_in['image'][hf_in[\"image\"].shape[0] - 1, :, :, :] = five_band_image\n",
    "        \n",
    "        count_in = count_in + 1\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        five_band_image.append(im_g)\n",
    "        five_band_image.append(im_r)\n",
    "        five_band_image.append(im_i)\n",
    "        five_band_image.append(im_z)\n",
    "        five_band_image.append(im_y)\n",
    "\n",
    "        five_band_image_reshape = np.reshape(np.array(five_band_image), [1, 5, 127, 127])\n",
    "\n",
    "        photozdata_subset = photozdata.iloc[i]\n",
    "\n",
    "        specz = photozdata_subset[\"specz_redshift\"]\n",
    "        specz_reshape = np.reshape(specz, [1, 1])\n",
    "\n",
    "        if count_out == 0:\n",
    "            hf__out.create_dataset('image', data = five_band_image_reshape, chunks = True, maxshape = (None, 5, 127, 127))\n",
    "\n",
    "        else:\n",
    "            hf_out['image'].resize((hf_out['image'].shape[0] + 1), axis = 0)\n",
    "            hf_out['image'][hf_out[\"image\"].shape[0] - 1, :, :, :] = five_band_image\n",
    "        \n",
    "        count_out = count_out + 1\n",
    "\n",
    "    image_g.close()\n",
    "    image_r.close()\n",
    "    image_i.close()\n",
    "    image_z.close()\n",
    "    image_y.close()\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900333ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca76399",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('/data/HSC/HSC_v6/step3/127x127_in.hdf5', 'r')\n",
    "y_array = np.asarray(hf['specz_redshift'][0:])[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14fce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_array, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
