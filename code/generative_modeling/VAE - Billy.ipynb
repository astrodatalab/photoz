{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f923d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "write = True # log this version as new\n",
    "MODEL_DESCRIPTION = \"Modify Archetecture, Latent back to 128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23f862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import stdout\n",
    "from datetime import datetime\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2032fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Cropping2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import layers as tfpl\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.nn import leaky_relu\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e0800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128\n",
    "BASE_DEPTH = 8\n",
    "IMAGE_SHAPE = (5, 127, 127)\n",
    "EPOCHS = 200\n",
    "CHECKPOINTS_TO_SAVE = 4\n",
    "KL_WEIGHT = 1e-6\n",
    "# Good value: between 1e-6 and 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a44e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU');\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 20000)]);\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU');\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb7cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "DATASET_NAME = \"HSC_v6_small\"\n",
    "MODEL_TYPE = \"VAE\"\n",
    "MODEL_VERSION = \"v1.0\"\n",
    "MODEL_SUBVERSION = \"v1.0.\" + now.strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "\n",
    "model_id = '_'.join([DATASET_NAME, MODEL_TYPE, MODEL_VERSION])\n",
    "model_path = os.path.join('/models', model_id, MODEL_SUBVERSION, 'model')\n",
    "checkpoints_path = os.path.join('/models', model_id, MODEL_SUBVERSION, 'checkpoints')\n",
    "logs_path = os.path.join('/logs', model_id, MODEL_SUBVERSION)\n",
    "predictions_path = os.path.join('/predictions', model_id, MODEL_SUBVERSION)\n",
    "weights_path = model_path + '/weights.h5'\n",
    "\n",
    "os.makedirs(model_path, exist_ok = True)\n",
    "os.makedirs(checkpoints_path, exist_ok = True)\n",
    "os.makedirs(logs_path, exist_ok = True)\n",
    "os.makedirs(predictions_path, exist_ok = True)\n",
    "\n",
    "if write == True:\n",
    "    with open(\"/models/README.md\", \"a\") as myfile:\n",
    "        myfile.write(MODEL_TYPE + \" \" + MODEL_SUBVERSION + \" - \" + MODEL_DESCRIPTION + \" - B. Li\" + \"\\n\")\n",
    "else:\n",
    "    with open(\"/models/README.md\", \"a\") as myfile:\n",
    "        myfile.write(MODEL_TYPE + \" \" + MODEL_SUBVERSION + \" - ... \"+ \" - B. Li\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06c90a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = h5py.File('/data/HSC/HSC_v6/step2A/127x127/5x127x127_training_zmax0.1_small.hdf5', 'r')\n",
    "hf_test = h5py.File('/data/HSC/HSC_v6/step2A/127x127/5x127x127_testing_zmax0.1_small.hdf5', 'r')\n",
    "hf_validation = h5py.File('/data/HSC/HSC_v6/step2A/127x127/5x127x127_validation_zmax0.1_small.hdf5', 'r')\n",
    "x_train = np.asarray(hf_train['image'][0:])\n",
    "x_test = np.asarray(hf_test['image'][0:])\n",
    "x_validation = np.asarray(hf_validation['image'][0:])\n",
    "max_value = 4.16\n",
    "x_train = np.true_divide(x_train, max_value)\n",
    "x_test = np.true_divide(x_test, max_value)\n",
    "x_validation = np.true_divide(x_validation, max_value)\n",
    "y_train = np.asarray(hf_train['specz_redshift'][0:])[..., None]\n",
    "y_test = np.asarray(hf_test['specz_redshift'][0:])[..., None]\n",
    "y_validation = np.asarray(hf_validation['specz_redshift'][0:])[..., None]\n",
    "# object_id_train = np.asarray(hf_train['object_id'][0:])\n",
    "# object_id = np.asarray(hf_test['object_id'][0:])\n",
    "# object_id_validation = np.asarray(hf_validation['object_id'][0:])\n",
    "hf_train.close()\n",
    "hf_test.close()\n",
    "hf_validation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a73521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y_train)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9f1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = len(x_train) - 1\n",
    "# while i >= 0:\n",
    "#     if y_train[i]>=0.1:\n",
    "#         x_train = np.delete(x_train, i)\n",
    "#         y_train = np.delete(y_train, i)\n",
    "#     i -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e33b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(LATENT_DIM), scale = 0.1), reinterpreted_batch_ndims=1)\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        images = Input(shape=IMAGE_SHAPE)\n",
    "        x = Conv2D(BASE_DEPTH, 3, strides=1, activation=leaky_relu,\n",
    "                   padding='same', data_format='channels_first')(images)\n",
    "        x = Conv2D(BASE_DEPTH, 3, strides=2, activation=leaky_relu,\n",
    "                   padding='same', data_format='channels_first')(x)\n",
    "        x = Conv2D(BASE_DEPTH, 3, strides=1, activation=leaky_relu,\n",
    "                   padding='same', data_format='channels_first')(x)\n",
    "        x = Conv2D(BASE_DEPTH, 3, strides=2, activation=leaky_relu,\n",
    "                   padding='same', data_format='channels_first')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(tfpl.MultivariateNormalTriL.params_size(LATENT_DIM), activation=None)(x)\n",
    "        z = tfpl.MultivariateNormalTriL(LATENT_DIM,\n",
    "                  activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=KL_WEIGHT))(x)\n",
    "        self.encoder = Model(images, z, name='encoder')\n",
    "\n",
    "        latents = Input(shape=LATENT_DIM)\n",
    "        x = Dense(8*LATENT_DIM*32*32, activation=None)(latents)\n",
    "        x = Reshape((8*LATENT_DIM,32,32))(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides=2, activation=leaky_relu,\n",
    "                            padding='same', data_format='channels_first')(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides=1, activation=leaky_relu,\n",
    "                            padding='same', data_format='channels_first')(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides=2, activation=leaky_relu,\n",
    "                            padding='same', data_format='channels_first')(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides=1, activation=leaky_relu,\n",
    "                            padding='same', data_format='channels_first')(x)\n",
    "        x = Conv2DTranspose(BASE_DEPTH, 3, strides=1, activation=leaky_relu,\n",
    "                            padding='same', data_format='channels_first')(x)\n",
    "        x = Conv2D(IMAGE_SHAPE[0], 3, strides=1, activation=None, \n",
    "                   padding='same', data_format='channels_first')(x)\n",
    "        outputs = Cropping2D(cropping=((0,1),(0,1)), data_format='channels_first')(x)\n",
    "        self.decoder = Model(latents, outputs, name='decoder')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def summary(self):\n",
    "        print(self.encoder.summary())\n",
    "        print(self.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ce4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5, 127, 127)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 8, 127, 127)       368       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 64, 64)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 64, 64)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 32, 32)         584       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8384)              68690112  \n",
      "_________________________________________________________________\n",
      "multivariate_normal_tri_l (M multiple                  0         \n",
      "=================================================================\n",
      "Total params: 68,692,232\n",
      "Trainable params: 68,692,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1048576)           135266304 \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 1024, 32, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 64, 64)         73736     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 64, 64)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 128, 128)       584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 8, 128, 128)       584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 8, 128, 128)       584       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 128, 128)       365       \n",
      "_________________________________________________________________\n",
      "cropping2d (Cropping2D)      (None, 5, 127, 127)       0         \n",
      "=================================================================\n",
      "Total params: 135,342,741\n",
      "Trainable params: 135,342,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcbb9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/astro-data-lab/VAE/e/VAE-26\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    name = MODEL_SUBVERSION,\n",
    "    project = \"astro-data-lab/VAE\",\n",
    "    api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxOGFlZGMxOC04MWU5LTQ2NDctYjlhZS05NGE2NGQ0NmIzMmEifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "# logs_callback = TensorBoard(log_dir = logs_path)\n",
    "\n",
    "weights_callback = ModelCheckpoint(filepath = os.path.join(checkpoints_path, 'weights_epoch{epoch}.hdf5'), save_freq = int(EPOCHS/CHECKPOINTS_TO_SAVE), save_weights_only = True)\n",
    "\n",
    "LR_callback = ReduceLROnPlateau()\n",
    "\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs_path, histogram_freq = 1)\n",
    "\n",
    "neptune_callback = NeptuneCallback(run = run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f61a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 454s 142ms/step - loss: 0.2661 - val_loss: 0.1625\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.1707 - val_loss: 0.1364\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 38s 122ms/step - loss: 0.1522 - val_loss: 0.1429\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.1667 - val_loss: 0.1251\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.1429 - val_loss: 0.1216\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 42s 134ms/step - loss: 0.1379 - val_loss: 0.1199\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 41s 130ms/step - loss: 0.1339 - val_loss: 0.1157\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 0.1309 - val_loss: 0.1150\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 0.1295 - val_loss: 0.1189\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.1271 - val_loss: 0.1120\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 34s 110ms/step - loss: 0.1293 - val_loss: 0.1128\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 38s 121ms/step - loss: 0.1267 - val_loss: 0.1184\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.1197 - val_loss: 0.1076\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.1193 - val_loss: 0.1156\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.1228 - val_loss: 0.1103\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 38s 122ms/step - loss: 0.1177 - val_loss: 0.1101\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.1142 - val_loss: 0.1093\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.1122 - val_loss: 0.1054\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.1122 - val_loss: 0.1055\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 38s 122ms/step - loss: 0.1094 - val_loss: 0.1079\n",
      "Epoch 21/200\n",
      " 31/313 [=>............................] - ETA: 8s - loss: 0.0946"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = x_train, y = x_train, epochs = EPOCHS, callbacks = [weights_callback, LR_callback, neptune_callback], validation_data = (x_validation, x_validation), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c09e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(dataset = 'train', num_to_save = None):\n",
    "    if dataset == 'train':\n",
    "        datagen = x_train\n",
    "    elif dataset == 'val':\n",
    "        datagen = x_validation\n",
    "    else:\n",
    "        dataset = 'test'\n",
    "        datagen = x_test\n",
    "    preds_path = os.path.join(predictions_path, dataset + \"_preds.hdf5\")\n",
    "    print(\"Saving predictions for\", dataset, \"dataset in\", preds_path)\n",
    "    \n",
    "    if num_to_save is None:\n",
    "        size = len(datagen)\n",
    "    else:\n",
    "        size = num_to_save\n",
    "\n",
    "    with h5py.File(preds_path, 'w') as f:\n",
    "        f.create_dataset('true', (size, ) + IMAGE_SHAPE)\n",
    "        f.create_dataset('pred', (size, ) + IMAGE_SHAPE)\n",
    "        f.create_dataset('loss', (size, ))\n",
    "        for i in range(size):\n",
    "            x = datagen[i]\n",
    "            f['true'][i] = x\n",
    "            f['pred'][i] = model(np.array([x]))\n",
    "            f['loss'][i] = model.evaluate(np.array([x]), np.array([x]), verbose = 0)\n",
    "            stdout.write(\"\\rSaved %d samples of \" % (i + 1) + str(size))\n",
    "            stdout.flush()\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0619570",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcda568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_bands_max(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show/5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_array = np.append(true_array, np.amax(np.array([x_test[i]])[0][j]))\n",
    "            pred_array = np.append(pred_array, np.amax(model(np.array([x_test[i]]))[0][j]))\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].set_xlabel(\"Bands\")\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].set_ylabel(\"Maximum normalized pixel value\")\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].legend()\n",
    "        \n",
    "def scatter_bands_center(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show/5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_array = np.append(true_array, np.array([x_test[i]])[0][j][63][63])\n",
    "            pred_array = np.append(pred_array, model(np.array([x_test[i]]))[0][j][63][63])\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].set_xlabel(\"Bands\")\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].set_ylabel(\"Central pixel value\")\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].legend()\n",
    "\n",
    "def scatter_bands_percentile(percentile = 90, num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show/5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_90 = np.percentile(np.array([x_test[i]])[0][j].flatten(), percentile)\n",
    "            pred_90 = np.percentile(np.asarray(model(np.array([x_test[i]]))[0][j]).flatten(), percentile)\n",
    "            true_array = np.append(true_array, true_90)\n",
    "            pred_array = np.append(pred_array, pred_90)\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].set_xlabel('Bands')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].set_ylabel(f'{percentile}th percentile normalized pixel value')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].legend()\n",
    "\n",
    "def scatter_bands_mean(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show/5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_mean = np.mean(np.array([x_test[i]])[0][j].flatten())\n",
    "            pred_mean = np.mean(np.asarray(model(np.array([x_test[i]]))[0][j]).flatten())\n",
    "            true_array = np.append(true_array, true_mean)\n",
    "            pred_array = np.append(pred_array, pred_mean)\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].set_xlabel('Bands')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].set_ylabel('Mean normalized pixel value')\n",
    "        axes[int((i - index)/5)][int((i - index)%5)].legend()\n",
    "\n",
    "def display_histograms(num_to_show = 2, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = num_to_show, ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        for j in range(0, 5):\n",
    "            true_arr = sorted(np.array([x_test[i]])[0][j].flatten())\n",
    "            pred_arr = sorted(np.asarray(model(np.array([x_test[i]]))[0][j]).flatten())\n",
    "            true_arr = true_arr[0:int(len(true_arr)*.99)]\n",
    "            pred_arr = pred_arr[0:int(len(pred_arr)*.99)]\n",
    "            axes[i][j].hist(true_arr, 100, color = 'blue', label = 'True', alpha=0.5)\n",
    "            axes[i][j].hist(pred_arr, 100, color = 'red', label = 'Predicted', alpha=0.5)\n",
    "            axes[i][j].set_xlabel(\"Pixel Values\")\n",
    "            axes[i][j].set_ylabel(\"Count\")\n",
    "            axes[i][j].legend()\n",
    "    fig.suptitle('Histograms of Predicted vs. True Image, Horizontal are Bands')\n",
    "    \n",
    "def display_5_bands(index):\n",
    "    fig, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (20, 10))\n",
    "    for i in range(0, 5):\n",
    "        axes[0][i].imshow(np.array([x_test[index]])[0][i])\n",
    "        max_pixel_true = round(np.amax(np.array([x_test[index]])[0][i]), 2)\n",
    "        axes[0][i].set_title(f'True band {i} max = ' + str(max_pixel_true))\n",
    "        pred = model(np.array([x_test[index]]))[0][i]\n",
    "        axes[1][i].imshow(pred, cmap = 'afmhot')\n",
    "        max_pixel_pred = round(np.amax(pred), 2)\n",
    "        axes[1][i].set_title(f'Pred band {i} max = ' + str(max_pixel_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfce4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter_bands_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_center(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter_bands_mean(10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d13de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter_bands_percentile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df1749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0abdaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_5_bands(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca798a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_galaxies(num_to_generate = 10):\n",
    "    z = prior.sample(num_to_generate)\n",
    "    xhat = model.decoder(z)\n",
    "    fig, axes = plt.subplots(nrows = num_to_generate, ncols = 5, figsize = (4 * 5, 4 * num_to_generate))\n",
    "    for i in range(num_to_generate):\n",
    "        for j in range(0,5):\n",
    "            axes[i][j].imshow(xhat[i][j], cmap = 'afmhot')\n",
    "            axes[i][j].set_title(f'Generated image {i} band {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_example_galaxies(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
