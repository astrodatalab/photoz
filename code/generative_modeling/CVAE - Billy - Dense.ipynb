{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f923d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "write = True # log this version as new\n",
    "MODEL_DESCRIPTION = \"Back to Dense Layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23f862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import stdout\n",
    "from datetime import datetime\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "from astropy.io import fits\n",
    "import random\n",
    "from DataMaker import HDF5ImageGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2032fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Cropping2D, ZeroPadding2D, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import layers as tfpl\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.nn import leaky_relu\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e0800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 256\n",
    "BASE_DEPTH = 8\n",
    "IMAGE_SHAPE = (5, 127, 127)\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 250\n",
    "CHECKPOINTS_TO_SAVE = 4\n",
    "KL_WEIGHT = 1e-6\n",
    "# Good value: 1e-6\n",
    "# With KL 0 val_loss = 0.4763\n",
    "# With KL 1e-6 val loss = 1.0732\n",
    "# With KL 1e-8 val loss = 0.5257\n",
    "# With KL 1e-10 val loss = 0.4466 0.4291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a44e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 17:24:19.198968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-17 17:24:19.221461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-17 17:24:19.224220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-17 17:24:19.228441: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-17 17:24:19.230113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-17 17:24:19.231717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-17 17:24:19.233294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-17 17:24:19.688567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-17 17:24:19.690136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-17 17:24:19.691661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-17 17:24:19.693162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20000 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU');\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 20000)]);\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU');\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb7cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "DATASET_NAME = \"HSC_v6\"\n",
    "MODEL_TYPE = \"CVAE\"\n",
    "MODEL_VERSION = \"v1.0\"\n",
    "MODEL_SUBVERSION = \"v1.0.\" + now.strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "\n",
    "model_id = '_'.join([DATASET_NAME, MODEL_TYPE, MODEL_VERSION])\n",
    "model_path = os.path.join('/models', model_id, MODEL_SUBVERSION, 'model')\n",
    "checkpoints_path = os.path.join('/models', model_id, MODEL_SUBVERSION, 'checkpoints')\n",
    "logs_path = os.path.join('/logs', model_id, MODEL_SUBVERSION)\n",
    "predictions_path = os.path.join('/predictions', model_id, MODEL_SUBVERSION)\n",
    "weights_path = model_path + '/weights.h5'\n",
    "\n",
    "os.makedirs(model_path, exist_ok = True)\n",
    "os.makedirs(checkpoints_path, exist_ok = True)\n",
    "os.makedirs(logs_path, exist_ok = True)\n",
    "os.makedirs(predictions_path, exist_ok = True)\n",
    "\n",
    "if write == True:\n",
    "    with open(\"/models/README.md\", \"a\") as myfile:\n",
    "        myfile.write(MODEL_TYPE + \" \" + MODEL_SUBVERSION + \" - \" + MODEL_DESCRIPTION + \" - B. Li\" + \"\\n\")\n",
    "else:\n",
    "    with open(\"/models/README.md\", \"a\") as myfile:\n",
    "        myfile.write(MODEL_TYPE + \" \" + MODEL_SUBVERSION + \" - ... \"+ \" - B. Li\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc0eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_gen = {'X_key': 'image',\n",
    "    'y_key': 'specz_redshift',\n",
    "    'scaler': False,\n",
    "    'labels_encoding': False,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'mode': 'train',\n",
    "    'shuffle': False}\n",
    "\n",
    "train_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step2A/127x127/5x127x127_training.hdf5', **args_gen)\n",
    "val_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step2A/127x127/5x127x127_validation.hdf5', **args_gen)\n",
    "test_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step2A/127x127/5x127x127_testing.hdf5', **args_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588c99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = BATCH_SIZE\n",
    "l = len(train_gen._indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d31d75f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(LATENT_DIM), scale = 0.1), reinterpreted_batch_ndims = 1)\n",
    "\n",
    "images = Input(shape = IMAGE_SHAPE)\n",
    "redshifts = Input(shape = 1,)\n",
    "conv1 = Conv2D(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "            padding = 'same', data_format = 'channels_first')(images)\n",
    "conv2 = Conv2D(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "            padding = 'same', data_format = 'channels_first')(conv1)\n",
    "flatten = Flatten()(conv2)\n",
    "dense1 = Dense(1024, activation = None)(flatten)\n",
    "dense2 = tf.keras.layers.Dense(512, activation = \"relu\")(redshifts)\n",
    "dense3 = tf.keras.layers.Dense(512, activation = \"relu\")(dense2)\n",
    "dense4 = tf.keras.layers.Dense(512, activation = \"relu\")(dense3)\n",
    "concat = tf.keras.layers.Concatenate()([dense1, dense4])\n",
    "dense5 = Dense(tfpl.MultivariateNormalTriL.params_size(LATENT_DIM), activation = None)(concat)\n",
    "z = tfpl.MultivariateNormalTriL(LATENT_DIM,\n",
    "            activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight = KL_WEIGHT))(dense5)\n",
    "encoder = Model([images, redshifts], z)\n",
    "\n",
    "zc = tf.keras.layers.Concatenate()([z, redshifts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25f1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense6 = Dense(8 * LATENT_DIM * 32 * 32, activation = None)\n",
    "reshape = Reshape((8 * LATENT_DIM, 32, 32))\n",
    "conv3 = Conv2DTranspose(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                    padding = 'same', data_format = 'channels_first')\n",
    "conv4 = Conv2DTranspose(BASE_DEPTH, 3, strides = 2, activation = leaky_relu,\n",
    "                    padding = 'same', data_format = 'channels_first')\n",
    "conv5 = Conv2DTranspose(BASE_DEPTH, 3, strides = 1, activation = leaky_relu, \n",
    "                    padding = 'same', data_format = 'channels_first')\n",
    "conv6 = Conv2D(IMAGE_SHAPE[0], 3, strides = 1, activation = None, \n",
    "           padding = 'same', data_format = 'channels_first')\n",
    "cropping = Cropping2D(cropping=((0, 1), (0, 1)), data_format = 'channels_first')\n",
    "outputs = Reshape(IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcac6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense6_model = dense6(zc)\n",
    "reshape_model = reshape(dense6_model)\n",
    "conv3_model = conv3(reshape_model)\n",
    "conv4_model = conv4(conv3_model)\n",
    "conv5_model = conv5(conv4_model)\n",
    "conv6_model = conv6(conv5_model)\n",
    "cropping_model = cropping(conv6_model)\n",
    "outputs_model = outputs(cropping_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c11d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = [images, redshifts], outputs = [outputs_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e036f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = Input(shape = (LATENT_DIM + 1, ))\n",
    "dense6_decoder = dense6(latents)\n",
    "reshape_decoder = reshape(dense6_decoder)\n",
    "conv3_decoder = conv3(reshape_decoder)\n",
    "conv4_decoder = conv4(conv3_decoder)\n",
    "conv5_decoder = conv5(conv4_decoder)\n",
    "conv6_decoder = conv6(conv5_decoder)\n",
    "cropping_decoder = cropping(conv6_decoder)\n",
    "outputs_decoder = outputs(cropping_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17a65d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Model(latents, outputs_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c25fe180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 127, 127  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 8, 64, 64)    368         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 8, 32, 32)    584         ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          1024        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 8192)         0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          262656      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         8389632     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 512)          262656      ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1536)         0           ['dense[0][0]',                  \n",
      "                                                                  'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 33152)        50954624    ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " multivariate_normal_tri_l (Mul  ((None, 256),       0           ['dense_4[0][0]']                \n",
      " tivariateNormalTriL)            (None, 256))                                                     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 257)          0           ['multivariate_normal_tri_l[0][0]\n",
      "                                                                 ',                               \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 2097152)      541065216   ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 2048, 32, 32  0           ['dense_5[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 8, 64, 64)   147464      ['reshape[0][0]']                \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 8, 128, 128)  584        ['conv2d_transpose[0][0]']       \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 8, 128, 128)  584        ['conv2d_transpose_1[0][0]']     \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 5, 128, 128)  365         ['conv2d_transpose_2[0][0]']     \n",
      "                                                                                                  \n",
      " cropping2d (Cropping2D)        (None, 5, 127, 127)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 5, 127, 127)  0           ['cropping2d[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 601,085,757\n",
      "Trainable params: 601,085,757\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d463c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billyli/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr = 0.0001)\n",
    "model.compile(optimizer = optimizer, loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcbb9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/astro-data-lab/VAE/e/VAE-130\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    name = MODEL_SUBVERSION,\n",
    "    project = \"astro-data-lab/VAE\",\n",
    "    api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxOGFlZGMxOC04MWU5LTQ2NDctYjlhZS05NGE2NGQ0NmIzMmEifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "# logs_callback = TensorBoard(log_dir = logs_path)\n",
    "\n",
    "weights_callback = ModelCheckpoint(filepath = os.path.join(checkpoints_path, 'weights_epoch{epoch}.hdf5'), save_freq = int(EPOCHS/CHECKPOINTS_TO_SAVE), save_weights_only = True)\n",
    "\n",
    "LR_callback = ReduceLROnPlateau()\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch <= 100:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr / 10\n",
    "\n",
    "LR_modify_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose = 0)\n",
    "\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs_path, histogram_freq = 1)\n",
    "\n",
    "neptune_callback = NeptuneCallback(run = run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b7c72",
   "metadata": {},
   "source": [
    "train_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step2A/127x127/5x127x127_training.hdf5', **args_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871e88f",
   "metadata": {},
   "source": [
    "history = model.fit(train_gen, epochs = 50, callbacks = [neptune_callback], validation_data = val_gen, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d488d57",
   "metadata": {},
   "source": [
    "train_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step3/5x127x127_training_min_3.hdf5', **args_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f8a28",
   "metadata": {},
   "source": [
    "history = model.fit(train_gen, epochs = 75, callbacks = [neptune_callback], validation_data = val_gen, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc4991",
   "metadata": {},
   "source": [
    "train_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step3/5x127x127_training_min_2.hdf5', **args_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5ea43",
   "metadata": {},
   "source": [
    "history = model.fit(train_gen, epochs = 100, callbacks = [weights_callback, neptune_callback], validation_data = val_gen, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a2d02",
   "metadata": {},
   "source": [
    "train_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step3/5x127x127_training_min_1.hdf5', **args_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72dc89",
   "metadata": {},
   "source": [
    "history = model.fit(train_gen, epochs = 100, callbacks = [weights_callback, neptune_callback], validation_data = val_gen, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c53ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = HDF5ImageGenerator(src = '/data/HSC/HSC_v6/step2A/127x127/5x127x127_training.hdf5', **args_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24912cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 17:24:27.609791: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen, epochs = 200, callbacks = [neptune_callback, LR_callback], validation_data = val_gen, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = 3\n",
    "index = 466\n",
    "z = test_gen[lib][0][1][index]\n",
    "z_shifted = 4\n",
    "band = 0\n",
    "print(f'z = {z}')\n",
    "str = np.asarray(encoder([np.array([test_gen[lib][0][0][index]]), np.array([test_gen[lib][0][1][index]])])[0])\n",
    "str = np.concatenate((str, np.array([z])))\n",
    "str = str.reshape(1, LATENT_DIM + 1)\n",
    "str_shifted = np.asarray(encoder([np.array([test_gen[lib][0][0][index]]), np.array([test_gen[lib][0][1][index]])])[0])\n",
    "str_shifted = np.concatenate((str_shifted, np.array([z_shifted])))\n",
    "str_shifted = str_shifted.reshape(1, LATENT_DIM + 1)\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = (10, 5))\n",
    "axes[0].imshow(test_gen[lib][0][0][index][band], cmap = 'afmhot')\n",
    "axes[1].imshow(decoder([str])[0][band], cmap = 'afmhot')\n",
    "axes[2].imshow(decoder([str_shifted])[0][band], cmap = 'afmhot')\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[2].axis('off')\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[1].set_title(\"Regenerated\")\n",
    "axes[2].set_title(\"Regenerated Far\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = 5\n",
    "index = 271\n",
    "z = test_gen[lib][0][1][index]\n",
    "z_shifted = 0.1\n",
    "band = 0\n",
    "print(f'z = {z}')\n",
    "str = np.asarray(encoder([np.array([test_gen[lib][0][0][index]]), np.array([test_gen[lib][0][1][index]])])[0])\n",
    "str = np.concatenate((str, np.array([z])))\n",
    "str = str.reshape(1, LATENT_DIM + 1)\n",
    "str_shifted = np.asarray(encoder([np.array([test_gen[lib][0][0][index]]), np.array([test_gen[lib][0][1][index]])])[0])\n",
    "str_shifted = np.concatenate((str_shifted, np.array([z_shifted])))\n",
    "str_shifted = str_shifted.reshape(1, LATENT_DIM + 1)\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = (10, 5))\n",
    "axes[0].imshow(test_gen[lib][0][0][index][band], cmap = 'afmhot')\n",
    "axes[1].imshow(decoder([str])[0][band], cmap = 'afmhot')\n",
    "axes[2].imshow(decoder([str_shifted])[0][band], cmap = 'afmhot')\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[2].axis('off')\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[1].set_title(\"Regenerated\")\n",
    "axes[2].set_title(\"Regenerated Close\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534af846",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = 0\n",
    "str = prior.sample(1)[0]\n",
    "strlo = np.concatenate((str, np.array([0.1])))\n",
    "strlo = strlo.reshape(1, LATENT_DIM + 1)\n",
    "strhi = np.concatenate((str, np.array([4])))\n",
    "strhi = strhi.reshape(1, LATENT_DIM + 1)\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 5))\n",
    "axes[0].imshow(decoder([strlo])[0][band], cmap = 'afmhot')\n",
    "axes[1].imshow(decoder([strhi])[0][band], cmap = 'afmhot')\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[0].set_title(\"Low z\")\n",
    "axes[1].set_title(\"High z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e3dd1",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 5))\n",
    "x_array = np.arange(5)\n",
    "lo_array = np.array([])\n",
    "hi_array = np.array([])\n",
    "for j in range(0, 5):\n",
    "    lo_array = np.append(lo_array, decoder([strlo])[0][j][63][63])\n",
    "    hi_array = np.append(hi_array, decoder([strhi])[0][j][63][63])\n",
    "axes[0].scatter(x_array, lo_array, c = 'blue', label = 'Low')\n",
    "axes[1].scatter(x_array, hi_array, c = 'red', label = 'High')\n",
    "axes[0].set_xlabel(\"Bands\")\n",
    "axes[0].set_ylabel(\"Central Pixel Value\")\n",
    "axes[0].legend()\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (5, 5))\n",
    "x_array = np.arange(5)\n",
    "lo_array = np.array([])\n",
    "hi_array = np.array([])\n",
    "for j in range(0, 5):\n",
    "    lo_array = np.append(lo_array, np.amax(np.transpose(np.asarray(decoder([strlo])[0][j][60:67]))[60:67].flatten()))\n",
    "    hi_array = np.append(hi_array, np.amax(np.transpose(np.asarray(decoder([strhi])[0][j][60:67]))[60:67].flatten()))\n",
    "axes.scatter(x_array, lo_array, c = 'blue', label = 'Low')\n",
    "axes.scatter(x_array, hi_array, c = 'red', label = 'High')\n",
    "axes.set_xlabel(\"Bands\")\n",
    "axes.set_ylabel(\"Max Pixel Value Near Center\")\n",
    "axes.legend()\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b218b7",
   "metadata": {},
   "source": [
    "for j in range(3, 80):\n",
    "    for i in range(0, 512):\n",
    "        if test_gen[j][0][1][i] <= 0.1:\n",
    "            print(j)\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e737db36",
   "metadata": {},
   "source": [
    "for j in range(3, 80):\n",
    "    for i in range(0, 512):\n",
    "        if test_gen[j][0][1][i] >= 3.5:\n",
    "            print(j)\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab0405",
   "metadata": {},
   "source": [
    "for i in range(0, 84):\n",
    "    l = 512\n",
    "    if i == 83:\n",
    "        l = 464\n",
    "    for j in range(0, l):\n",
    "        index = i * BATCH_SIZE + j + 1\n",
    "        stdout.write(\"\\rChecking %d samples of \" % (index) + \"42960\")\n",
    "        z = test_gen[i][0][1][j]\n",
    "        str = np.asarray(encoder([np.array([test_gen[i][0][0][j]]), np.array([test_gen[i][0][1][j]])])[0])\n",
    "        str = np.concatenate((str, np.array([z])))\n",
    "        str = str.reshape(1, LATENT_DIM + 1)\n",
    "        hdul = fits.PrimaryHDU(data = test_gen[i][0][0][j])\n",
    "        string = f\"/data/CVAE Generated/Full/Original Galaxy #{index}.fits\"\n",
    "        hdul.writeto(string, overwrite = True)\n",
    "        hdul = fits.PrimaryHDU(data = decoder([str])[0])\n",
    "        string = f\"/data/CVAE Generated/Full/Reconstructed Galaxy #{index}.fits\"\n",
    "        hdul.writeto(string, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8092e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_bands_max_near_center(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show / 5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_array = np.append(true_array, np.amax(np.array([test_gen[0][0][0][i][j]])))\n",
    "            pred_array = np.append(pred_array, np.amax(np.transpose(np.asarray(model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j][60:67]))[60:67].flatten()))\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel(\"Bands\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel(f\"Maximum normalized pixel value z = {np.array([test_gen[0][0][1][i]])[0]}\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "\n",
    "def scatter_bands_max(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show / 5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_array = np.append(true_array, np.amax(np.array([test_gen[0][0][0][i][j]])))\n",
    "            pred_array = np.append(pred_array, np.amax(np.array([model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j]])))\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel(\"Bands\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel(\"Maximum normalized pixel value\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "        \n",
    "def scatter_bands_center(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show / 5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_array = np.append(true_array, np.array([test_gen[0][0][0][i][j]])[0][63][63])\n",
    "            pred_array = np.append(pred_array, model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j][63][63])\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel(\"Bands\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel(f\"Central pixel value z = {np.array([test_gen[0][0][1][i]])}\")\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "        \n",
    "def scatter_bands_center_shift(lib = 3, i = 37, z = 1):\n",
    "    print(z)\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (5, 5))\n",
    "    x_array = np.arange(5)\n",
    "    true_array = np.array([])\n",
    "    pred_array = np.array([])\n",
    "    for j in range(0, 5):\n",
    "        true_array = np.append(true_array, np.array([test_gen[lib][0][0][i][j]])[0][63][63])\n",
    "        pred_array = np.append(pred_array, model([np.array([test_gen[lib][0][0][i]]), np.array([z])])[0][j][63][63])\n",
    "    axes.scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "    axes.scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "    axes.set_xlabel(\"Bands\")\n",
    "    axes.set_ylabel(\"Central pixel value\")\n",
    "    axes.legend()\n",
    "    \n",
    "def scatter_bands_max_shift(lib = 3, i = 37, z = 1):\n",
    "    print(z)\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (5, 5))\n",
    "    x_array = np.arange(5)\n",
    "    true_array = np.array([])\n",
    "    pred_array = np.array([])\n",
    "    for j in range(0, 5):\n",
    "        true_array = np.append(true_array, np.amax(np.array([test_gen[lib][0][0][i][j]])))\n",
    "        pred_array = np.append(pred_array, np.amax(np.array([model([np.array([test_gen[lib][0][0][i]]), np.array([z])])[0][j]])))\n",
    "    axes.scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "    axes.scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "    axes.set_xlabel(\"Bands\")\n",
    "    axes.set_ylabel(\"Central pixel value\")\n",
    "    axes.legend()\n",
    "\n",
    "def scatter_bands_percentile(percentile = 90, num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show /  5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_90 = np.percentile(np.array([test_gen[0][0][0][i][j]]).flatten(), percentile)\n",
    "            pred_90 = np.percentile(np.asarray(model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j]).flatten(), percentile)\n",
    "            true_array = np.append(true_array, true_90)\n",
    "            pred_array = np.append(pred_array, pred_90)\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel('Bands')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel(f'{percentile}th percentile normalized pixel value')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "\n",
    "def scatter_bands_mean(num_to_show = 10, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = int(num_to_show / 5), ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        true_array = np.array([])\n",
    "        pred_array = np.array([])\n",
    "        for j in range(0, 5):\n",
    "            true_mean = np.mean(np.array([test_gen[0][0][0][i][j]]).flatten())\n",
    "            pred_mean = np.mean(np.asarray(model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j]).flatten())\n",
    "            true_array = np.append(true_array, true_mean)\n",
    "            pred_array = np.append(pred_array, pred_mean)\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, true_array, c = 'blue', label = 'True')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].scatter(x_array, pred_array, c = 'red', label = 'Predicted')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_xlabel('Bands')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].set_ylabel('Mean normalized pixel value')\n",
    "        axes[int((i - index) / 5)][int((i - index) % 5)].legend()\n",
    "\n",
    "def display_histograms(num_to_show = 2, index = 0):\n",
    "    fig, axes = plt.subplots(nrows = num_to_show, ncols = 5, figsize = (20, int(num_to_show)))\n",
    "    for i in range(index, index + num_to_show):\n",
    "        x_array = np.arange(5)\n",
    "        for j in range(0, 5):\n",
    "            true_arr = sorted(np.array([test_gen[0][0][0][i][j]]).flatten())\n",
    "            pred_arr = sorted(np.asarray(model([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])])[0][j]).flatten())\n",
    "            true_arr = true_arr[0 : int(len(true_arr) * .99)]\n",
    "            pred_arr = pred_arr[0 : int(len(pred_arr) * .99)]\n",
    "            axes[i][j].hist(true_arr, 100, color = 'blue', label = 'True', alpha = 0.5)\n",
    "            axes[i][j].hist(pred_arr, 100, color = 'red', label = 'Predicted', alpha = 0.5)\n",
    "            axes[i][j].set_xlabel(\"Pixel Values\")\n",
    "            axes[i][j].set_ylabel(\"Count\")\n",
    "            axes[i][j].legend()\n",
    "    fig.suptitle('Histograms of Predicted vs. True Image, Horizontal are Bands')\n",
    "    \n",
    "def display_5_bands(index):\n",
    "    fig, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (20, 10))\n",
    "    loss = round(model.evaluate([np.array([test_gen[0][0][0][index]]), np.array([test_gen[0][0][1][index]])], np.array([test_gen[0][0][0][index]]), verbose = 0), 2)\n",
    "    for i in range(0, 5):\n",
    "        axes[0][i].imshow(np.array([test_gen[0][0][0][index]])[0][i], cmap = 'afmhot')\n",
    "        max_pixel_true = round(np.amax(np.array([test_gen[0][0][0][index]])[0][i]), 2)\n",
    "        # axes[0][i].set_title(f'True band {i} max = {max_pixel_true}')\n",
    "        pred = model([np.array([test_gen[0][0][0][index]]), np.array([test_gen[0][0][1][index]])])[0][i]\n",
    "        axes[1][i].imshow(pred, cmap = 'afmhot')\n",
    "        max_pixel_pred = round(np.amax(pred), 2)\n",
    "        axes[1][i].set_title(f'\\n loss = {loss}') # f'Pred band {i} max = {max_pixel_pred}' + \n",
    "        \n",
    "def display_high_loss(num_to_show, min_loss):\n",
    "    fig, axes = plt.subplots(nrows = num_to_show, ncols = 5, figsize = (20, 5 * num_to_show))\n",
    "    r = 0\n",
    "    for i in range(BATCH_SIZE):\n",
    "        loss = round(model.evaluate([np.array([test_gen[0][0][0][i]]), np.array([test_gen[0][0][1][i]])], np.array([test_gen[0][0][0][i]]), verbose = 0), 2)\n",
    "        if loss >= min_loss:\n",
    "            print(i)\n",
    "            for j in range(0, 5):\n",
    "                axes[r][j].imshow(np.array([test_gen[0][0][0][i]])[0][j], cmap = 'afmhot')\n",
    "                axes[r][j].set_title(f'Loss = {loss}')\n",
    "            r += 1\n",
    "        if r >= num_to_show:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77067c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_max_near_center(num_to_show = 10, index = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = np.array([])\n",
    "y_array = np.array([])\n",
    "l = len(test_gen[0][0][0])\n",
    "print(l)\n",
    "for i in range(0, l):\n",
    "    x_array = np.append(x_array, np.amax(np.array([test_gen[0][0][0][i][4]])))\n",
    "    y_array = np.append(y_array, np.array([test_gen[0][0][1][i]])[0])\n",
    "    print(i)\n",
    "plt.scatter(y_array, x_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81015273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "xy = np.asarray(np.vstack([y_array, x_array])).astype('float32')\n",
    "z = gaussian_kde(xy)(xy)\n",
    "plt.scatter(y_array, x_array, s = 5, c = z)\n",
    "plt.xlim(0,4)\n",
    "plt.ylim(0, 80)\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"Maximum Pixel Value near the Center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac674f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_center_shift(lib = 3, i = 267, z = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_max_shift(lib = 3, i = 37, z = test_gen[3][0][1][37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_max_shift(lib = 3, i = 37, z = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_mean(10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c48efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bands_percentile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eeb40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_5_bands(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b7232",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_high_loss(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ccccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "bnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
