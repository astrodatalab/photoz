{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1091fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import h5py\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b6b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfpl = tfp.layers\n",
    "tf1 = tf.compat.v1\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2c3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.python.distributions import kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d8bd5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 10GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28deb299",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8000\n",
    "t = 2000\n",
    "hf_train = h5py.File('/mnt/data/HSC/HSC_v6/step3/127x127/training_8000_linear.hdf5', 'r')\n",
    "hf_test = h5py.File('/mnt/data/HSC/HSC_v6/step3/127x127/testing_2000_linear.hdf5', 'r')\n",
    "x_train = hf_train['image'][0:n]\n",
    "x_test = hf_test['image'][0:t]\n",
    "y_train = hf_train['specz'][0:n]\n",
    "y_test = hf_test['specz'][0:t]\n",
    "oid_train = hf_train['object_id'][0:n]\n",
    "oid_test = hf_test['object_id'][0:t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f187c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Evan's code, didn't completely understand this part\n",
    "def posterior_mean_field(kernel_size: int, bias_size: int, dtype: any) -> tf.keras.Model:\n",
    "    \"\"\"Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\"\"\"\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype, initializer=lambda shape, dtype: random_gaussian_initializer(shape, dtype), trainable=True),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :n],\n",
    "                       scale= 0 + tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "def prior_trainable(kernel_size: int, bias_size: int, dtype: any) -> tf.keras.Model:\n",
    "    \"\"\"Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\"\"\"\n",
    "    n = kernel_size + bias_size\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),  # Returns a trainable variable of shape n, regardless of input\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t, scale=0.1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "def random_gaussian_initializer(shape, dtype):\n",
    "    n = int(shape / 2)\n",
    "    loc_norm = tf.random_normal_initializer(mean=0., stddev=0.1)\n",
    "    loc = tf.Variable(\n",
    "        initial_value=loc_norm(shape=(n,), dtype=dtype)\n",
    "    )\n",
    "    scale_norm = tf.random_normal_initializer(mean=-3., stddev=0.1)\n",
    "    scale = tf.Variable(\n",
    "        initial_value=scale_norm(shape=(n,), dtype=dtype)\n",
    "    )\n",
    "    return tf.concat([loc, scale], 0)\n",
    "\n",
    "def negative_loglikelihood(targets, estimated_distribution):\n",
    "    return -estimated_distribution.log_prob(targets)\n",
    "\n",
    "kl_divergence_function = lambda q, p, _: kl_divergence(q, p) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69ccd934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2beada9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_flipout_2 (Conv2DFlip (None, 126, 126, 32)      1312      \n",
      "_________________________________________________________________\n",
      "module_wrapper_7 (ModuleWrap (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_8 (ModuleWrap (None, 62, 62, 32)        4128      \n",
      "_________________________________________________________________\n",
      "module_wrapper_9 (ModuleWrap (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_10 (ModuleWra (None, 30, 30, 32)        4128      \n",
      "_________________________________________________________________\n",
      "module_wrapper_11 (ModuleWra (None, 29, 29, 32)        4128      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 26912)             0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_12 (ModuleWra (None, 26912)             0         \n",
      "_________________________________________________________________\n",
      "dense_variational_2 (DenseVa (None, 64)                5167296   \n",
      "_________________________________________________________________\n",
      "dense_variational_3 (DenseVa (None, 64)                12480     \n",
      "_________________________________________________________________\n",
      "module_wrapper_13 (ModuleWra (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "independent_normal_1 (Indepe multiple                  0         \n",
      "=================================================================\n",
      "Total params: 5,193,602\n",
      "Trainable params: 5,193,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_variational = Sequential([\n",
    "    tf.keras.layers.Input(shape=(127,127,5)),\n",
    "    tfp.layers.Convolution2DFlipout(32, kernel_size=(2,2), activation='tanh', kernel_divergence_fn=kl_divergence_function),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(32, kernel_size=(2,2),activation='tanh'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(32, kernel_size=(2,2),activation='relu'),\n",
    "    #tfp.layers.Convolution2DFlipout(32, kernel_size=(2,2), activation='relu', kernel_divergence_fn=kl_divergence_function),\n",
    "    Conv2D(32, kernel_size=(2,2),activation='relu'),\n",
    "    #tfp.layers.Convolution2DFlipout(32, kernel_size=(2,2), activation='relu', kernel_divergence_fn=kl_divergence_function),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    Dropout(0.2),\n",
    "    # tfp.layers.DenseVariational(512, activation='tanh', make_posterior_fn=posterior_mean_field, make_prior_fn=prior_trainable, kl_weight=1/n),\n",
    "    tfp.layers.DenseVariational(64, \n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1/n),\n",
    "    # tfp.layers.DenseVariational(128, activation='tanh', make_posterior_fn=posterior_mean_field, make_prior_fn=prior_trainable, kl_weight=1/n),\n",
    "    tfp.layers.DenseVariational(64, \n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1/n),\n",
    "    # Dense(2)\n",
    "    Dense(tfpl.IndependentNormal.params_size(1)),\n",
    "    tfpl.IndependentNormal(1)\n",
    "])\n",
    "\n",
    "model_variational.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de5fcf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(targets, estimated_distribution):\n",
    "    return -estimated_distribution.log_prob(targets)*1/(1 + targets)\n",
    "\n",
    "model_variational.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss = loss_function, metrics = [keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21918a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/900\n",
      "250/250 [==============================] - 416s 14ms/step - loss: 32.0853 - root_mean_squared_error: 4.7611 - val_loss: 8.9060 - val_root_mean_squared_error: 3.2618\n",
      "Epoch 2/900\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 7.0392 - root_mean_squared_error: 2.6990 - val_loss: 5.9556 - val_root_mean_squared_error: 2.3411\n",
      "Epoch 3/900\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 5.3918 - root_mean_squared_error: 2.1267 - val_loss: 4.9049 - val_root_mean_squared_error: 1.9630\n",
      "Epoch 4/900\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 4.5567 - root_mean_squared_error: 1.8461 - val_loss: 4.2403 - val_root_mean_squared_error: 1.7499\n",
      "Epoch 5/900\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 3.9906 - root_mean_squared_error: 1.6743 - val_loss: 3.7722 - val_root_mean_squared_error: 1.6101\n",
      "Epoch 6/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 3.5935 - root_mean_squared_error: 1.5581 - val_loss: 3.4309 - val_root_mean_squared_error: 1.5103\n",
      "Epoch 7/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 3.2904 - root_mean_squared_error: 1.4710 - val_loss: 3.1452 - val_root_mean_squared_error: 1.4346\n",
      "Epoch 8/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 3.0516 - root_mean_squared_error: 1.4039 - val_loss: 2.9296 - val_root_mean_squared_error: 1.3748\n",
      "Epoch 9/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 2.8446 - root_mean_squared_error: 1.3498 - val_loss: 2.7681 - val_root_mean_squared_error: 1.3265\n",
      "Epoch 10/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 2.6712 - root_mean_squared_error: 1.3057 - val_loss: 2.5905 - val_root_mean_squared_error: 1.2863\n",
      "Epoch 11/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 2.5343 - root_mean_squared_error: 1.2686 - val_loss: 2.4555 - val_root_mean_squared_error: 1.2518\n",
      "Epoch 12/900\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 2.4037 - root_mean_squared_error: 1.2373 - val_loss: 2.3401 - val_root_mean_squared_error: 1.2234\n",
      "Epoch 13/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 2.2884 - root_mean_squared_error: 1.2107 - val_loss: 2.2285 - val_root_mean_squared_error: 1.1982\n",
      "Epoch 14/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 2.2036 - root_mean_squared_error: 1.1873 - val_loss: 2.1521 - val_root_mean_squared_error: 1.1768\n",
      "Epoch 15/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 2.1127 - root_mean_squared_error: 1.1668 - val_loss: 2.0696 - val_root_mean_squared_error: 1.1574\n",
      "Epoch 16/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 2.0477 - root_mean_squared_error: 1.1490 - val_loss: 2.0203 - val_root_mean_squared_error: 1.1403\n",
      "Epoch 17/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.9807 - root_mean_squared_error: 1.1321 - val_loss: 1.9537 - val_root_mean_squared_error: 1.1244\n",
      "Epoch 18/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.9339 - root_mean_squared_error: 1.1175 - val_loss: 1.9053 - val_root_mean_squared_error: 1.1103\n",
      "Epoch 19/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.8899 - root_mean_squared_error: 1.1034 - val_loss: 1.8697 - val_root_mean_squared_error: 1.0971\n",
      "Epoch 20/900\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 1.8590 - root_mean_squared_error: 1.0910 - val_loss: 1.8235 - val_root_mean_squared_error: 1.0852\n",
      "Epoch 21/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.8293 - root_mean_squared_error: 1.0800 - val_loss: 1.8001 - val_root_mean_squared_error: 1.0746\n",
      "Epoch 22/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.8014 - root_mean_squared_error: 1.0701 - val_loss: 1.7805 - val_root_mean_squared_error: 1.0650\n",
      "Epoch 23/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7839 - root_mean_squared_error: 1.0603 - val_loss: 1.7647 - val_root_mean_squared_error: 1.0559\n",
      "Epoch 24/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7678 - root_mean_squared_error: 1.0519 - val_loss: 1.7454 - val_root_mean_squared_error: 1.0477\n",
      "Epoch 25/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7517 - root_mean_squared_error: 1.0436 - val_loss: 1.7470 - val_root_mean_squared_error: 1.0397\n",
      "Epoch 26/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7486 - root_mean_squared_error: 1.0360 - val_loss: 1.7286 - val_root_mean_squared_error: 1.0322\n",
      "Epoch 27/900\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 1.7405 - root_mean_squared_error: 1.0287 - val_loss: 1.7252 - val_root_mean_squared_error: 1.0253\n",
      "Epoch 28/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7419 - root_mean_squared_error: 1.0222 - val_loss: 1.7299 - val_root_mean_squared_error: 1.0190\n",
      "Epoch 29/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7334 - root_mean_squared_error: 1.0161 - val_loss: 1.7159 - val_root_mean_squared_error: 1.0130\n",
      "Epoch 30/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7331 - root_mean_squared_error: 1.0097 - val_loss: 1.7204 - val_root_mean_squared_error: 1.0070\n",
      "Epoch 31/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7319 - root_mean_squared_error: 1.0042 - val_loss: 1.7291 - val_root_mean_squared_error: 1.0013\n",
      "Epoch 32/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7317 - root_mean_squared_error: 0.9986 - val_loss: 1.7238 - val_root_mean_squared_error: 0.9962\n",
      "Epoch 33/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7331 - root_mean_squared_error: 0.9939 - val_loss: 1.7300 - val_root_mean_squared_error: 0.9913\n",
      "Epoch 34/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7412 - root_mean_squared_error: 0.9888 - val_loss: 1.7219 - val_root_mean_squared_error: 0.9864\n",
      "Epoch 35/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7333 - root_mean_squared_error: 0.9846 - val_loss: 1.7284 - val_root_mean_squared_error: 0.9824\n",
      "Epoch 36/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7326 - root_mean_squared_error: 0.9803 - val_loss: 1.7281 - val_root_mean_squared_error: 0.9781\n",
      "Epoch 37/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7372 - root_mean_squared_error: 0.9759 - val_loss: 1.7287 - val_root_mean_squared_error: 0.9740\n",
      "Epoch 38/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7375 - root_mean_squared_error: 0.9722 - val_loss: 1.7371 - val_root_mean_squared_error: 0.9702\n",
      "Epoch 39/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7393 - root_mean_squared_error: 0.9683 - val_loss: 1.7278 - val_root_mean_squared_error: 0.9664\n",
      "Epoch 40/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7429 - root_mean_squared_error: 0.9647 - val_loss: 1.7296 - val_root_mean_squared_error: 0.9630\n",
      "Epoch 41/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7367 - root_mean_squared_error: 0.9615 - val_loss: 1.7328 - val_root_mean_squared_error: 0.9599\n",
      "Epoch 42/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7405 - root_mean_squared_error: 0.9584 - val_loss: 1.7343 - val_root_mean_squared_error: 0.9569\n",
      "Epoch 43/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7410 - root_mean_squared_error: 0.9555 - val_loss: 1.7389 - val_root_mean_squared_error: 0.9540\n",
      "Epoch 44/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7415 - root_mean_squared_error: 0.9525 - val_loss: 1.7390 - val_root_mean_squared_error: 0.9511\n",
      "Epoch 45/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7408 - root_mean_squared_error: 0.9497 - val_loss: 1.7324 - val_root_mean_squared_error: 0.9482\n",
      "Epoch 46/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7416 - root_mean_squared_error: 0.9471 - val_loss: 1.7253 - val_root_mean_squared_error: 0.9457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/900\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 1.7428 - root_mean_squared_error: 0.9446 - val_loss: 1.7389 - val_root_mean_squared_error: 0.9433\n",
      "Epoch 48/900\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 1.7407 - root_mean_squared_error: 0.9420 - val_loss: 1.7322 - val_root_mean_squared_error: 0.9409\n",
      "Epoch 49/900\n",
      "204/250 [=======================>......] - ETA: 0s - loss: 1.7459 - root_mean_squared_error: 0.9399"
     ]
    }
   ],
   "source": [
    "import tensorboard\n",
    "\n",
    "model_name = 'HSC_v6_BCNN_v1'\n",
    "\n",
    "checkpoint_filepath = os.path.join(\"/models/\", model_name)+\"/\"+model_name\n",
    "\n",
    "log_dir = os.path.join(\"/logs/\", model_name)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only = True, verbose = 1, save_freq = 150*250)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)\n",
    "\n",
    "model_variational.fit(x = x_train, y = y_train, epochs = 900, shuffle = True, verbose = 1, validation_data = (x_test, y_test), callbacks = [tensorboard_callback, cp_callback])\n",
    "\n",
    "# model_variational.fit(x = x_train, y = y_train, epochs = 450, shuffle = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variational.save(checkpoint_filepath, save_traces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prediction = []\n",
    "\n",
    "for i in range(len_test):\n",
    "    y_test_prediction.append(model_variational(np.array([x_test[i]])).mean()[0][0])\n",
    "\n",
    "y_test_prediction = np.asarray(y_test_prediction).astype('float32')\n",
    "\n",
    "y_test_spectro = np.asarray(y_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_std = []\n",
    "\n",
    "for i in range(len_test):\n",
    "    y_test_std.append(model_variational(np.array([x_test[i]])).stddev()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "xy = np.asarray(np.vstack([y_test_spectro, y_test_prediction])).astype('float32')\n",
    "z = gaussian_kde(xy)(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d080369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "z_max = 4\n",
    "sns.set(rc={'figure.figsize':(10, 10), 'lines.markersize':20})\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "fig, ax = plt.subplots()\n",
    "scatter_plot = ax.scatter(y_test_spectro, y_test_prediction, c = z, s = 1, edgecolor = None)\n",
    "plt.colorbar(scatter_plot, label = 'Density')\n",
    "plt.title('Prediction')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('photo z')\n",
    "plt.plot([0, z_max], [0, z_max],color = 'black')\n",
    "plt.xlim([0, z_max])\n",
    "plt.ylim([0, z_max])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_variational = 0\n",
    "overlap_array_variational = []\n",
    "for i in range(0,len_test):\n",
    "    if np.abs(y_test_spectro[i]-y_test_prediction[i])<=y_test_std[i]:\n",
    "        overlap_variational += 1\n",
    "        overlap_array_variational.append(1)\n",
    "    else:\n",
    "        overlap_array_variational.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4218c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "display_size = 1000\n",
    "scatter_plot = ax.scatter(y_test_spectro[:display_size], y_test_prediction[:display_size], c = z[:display_size], s = 4, edgecolor = None, zorder = 2)\n",
    "error_plot = ax.errorbar(y_test_spectro[:display_size], y_test_prediction[:display_size], yerr = y_test_std[:display_size], fmt=\"o\", markersize=0, color = 'blue', elinewidth = 1, zorder = 1)\n",
    "plt.title('Prediction with error, Bayesian')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('photo z')\n",
    "plt.plot([0, z_max], [0, z_max],color = 'black')\n",
    "plt.xlim([0, z_max])\n",
    "plt.ylim([0, z_max])\n",
    "print(\"Coverage: \"+str(overlap_variational/len_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5eee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array_variational = np.transpose(np.vstack((y_test_spectro, y_test_prediction, y_test_std, overlap_array_variational))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb236386",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_combined_array_variational = sorted(combined_array_variational, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_variational = int(z_max*5)\n",
    "splitted_sorted_combined_array_variational = np.array_split(sorted_combined_array_variational, bins_variational)\n",
    "coverage_variational = []\n",
    "for i in range(0, bins_variational):\n",
    "    bins_count_variational = 0\n",
    "    for j in range(0,len(splitted_sorted_combined_array_variational[i])):\n",
    "        if splitted_sorted_combined_array_variational[i][j][3] == 1:\n",
    "            bins_count_variational += 1\n",
    "    coverage_variational.append(bins_count_variational/len(splitted_sorted_combined_array_variational[i])/0.6827)\n",
    "x_array = np.arange(0, z_max, z_max/bins_variational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c57bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_array, coverage_variational, c = 'red')\n",
    "plt.title('Coverage, Bayesian')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('coverage')\n",
    "plt.ylim([0, 1.5])\n",
    "print(\"Coverage: \"+str(overlap_variational/len_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7375ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_array = []\n",
    "for i in range(0, bins_variational):\n",
    "    total = 0\n",
    "    for j in range(0, len(splitted_sorted_combined_array_variational[i])):\n",
    "        bias = (splitted_sorted_combined_array_variational[i][j][1]-splitted_sorted_combined_array_variational[i][j][0])\n",
    "        total += bias\n",
    "    mean_array.append(total/len(splitted_sorted_combined_array_variational[i]))\n",
    "x_array = np.arange(0, z_max, z_max/bins_variational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_array = []\n",
    "for i in range(0,bins_variational):\n",
    "    error_total = 0\n",
    "    for j in range(0,len(splitted_sorted_combined_array_variational[i])):\n",
    "        error_total += splitted_sorted_combined_array_variational[i][j][2]\n",
    "    error_array.append(error_total/len(splitted_sorted_combined_array_variational[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(5, 5), 'lines.markersize':40})\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.errorbar(x_array, mean_array, yerr = error_array, fmt = \"o\", color = 'blue', markersize = 4, elinewidth = 2)\n",
    "plt.title('Bias Plot for Probabilistic Model')\n",
    "plt.xlabel('True Redshift')\n",
    "plt.ylabel('Bias (Predicted - True)')\n",
    "plt.plot([0, z_max], [0, 0],color = 'black')\n",
    "plt.ylim([-1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outlier_array = np.transpose(np.vstack((y_test_spectro,np.subtract(y_test_prediction, y_test_spectro))))\n",
    "sorted_combined_outlier_array = sorted(combined_outlier_array, key=lambda x: x[0])\n",
    "bins_outlier = int(z_max*10)\n",
    "splitted_sorted_combined_outlier_array = np.array_split(sorted_combined_outlier_array,bins_outlier)\n",
    "outlier_array = []\n",
    "outlier_count_total = 0\n",
    "for i in range(0,bins_outlier):\n",
    "    outlier_count = 0\n",
    "    for j in range(0,len(splitted_sorted_combined_outlier_array[i])):\n",
    "        if np.abs(splitted_sorted_combined_outlier_array[i][j][1]/(splitted_sorted_combined_outlier_array[i][j][0] + 1)) >= .15:\n",
    "            outlier_count += 1\n",
    "            outlier_count_total += 1\n",
    "    outlier_array.append(outlier_count/len(y_test_spectro)*bins_outlier)\n",
    "x_array_outlier = np.arange(0,z_max,z_max/bins_outlier)\n",
    "plt.plot(x_array_outlier, outlier_array, c = 'blue')\n",
    "plt.title('Outlier Rate')\n",
    "plt.xlabel('spec z')\n",
    "plt.ylabel('outlier rate')\n",
    "plt.xlim([0,z_max])\n",
    "plt.ylim([0,1/2])\n",
    "plt.show()\n",
    "\n",
    "print(\"Outlier Rate: \"+str(outlier_count_total/len(y_test_spectro)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec169d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
