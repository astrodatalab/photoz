{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd5cea9",
   "metadata": {},
   "source": [
    "# This notebook uses the Inception-ResNet v2 architecture. The stem has been modified to accommodate for 64x64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da1d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 09:26:14.744799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-02 09:26:15.318638: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-02 09:26:15.318683: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-02 09:26:15.318687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import keras\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Input, concatenate, add, Activation, BatchNormalization, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from photoz_utils import *\n",
    "from DataMakerPlus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c6416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "GB_LIMIT = 17\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(GB_LIMIT*1000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dece0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (5, 64, 64)\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 300\n",
    "LEARNING_RATE = 0.00001\n",
    "Z_MAX = 4\n",
    "hparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'z_max': Z_MAX\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a546612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = f'/data/HSC/HSC_v6/step3A/64x64_training_small.hdf5'\n",
    "VAL_PATH = f'/data/HSC/HSC_v6/step3A/64x64_validation_small.hdf5'\n",
    "TEST_PATH = f'/data/HSC/HSC_v6/step3A/64x64_testing_small.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e5e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_args = {\n",
    "    'image_key': 'image',\n",
    "    'numerical_keys': None,\n",
    "    'y_key': 'specz_redshift',\n",
    "    'scaler': True,\n",
    "    'labels_encoding': False,\n",
    "    'batch_size': hparams['batch_size'],\n",
    "    'shuffle': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8e667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = HDF5DataGenerator(TRAIN_PATH, mode='train', **gen_args)\n",
    "val_gen = HDF5DataGenerator(VAL_PATH, mode='train', **gen_args)\n",
    "test_gen = HDF5DataGenerator(TEST_PATH, mode='test', **gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b1e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def calculate_loss(z_photo, z_spec):\n",
    "    \"\"\"\n",
    "    HSC METRIC. Returns an array. Loss is accuracy metric defined by HSC, meant\n",
    "    to capture the effects of bias, scatter, and outlier all in one. This has\n",
    "    uses for both point and density estimation.\n",
    "    z_photo: array\n",
    "        Photometric or predicted redshifts.\n",
    "    z_spec: array\n",
    "        Spectroscopic or actual redshifts.\n",
    "    \"\"\"\n",
    "    dz = delz(z_photo, z_spec)\n",
    "    gamma = 0.15\n",
    "    denominator = 1.0 + K.square(dz/gamma)\n",
    "    loss = 1 - 1.0 / denominator\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf11cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1,1)):\n",
    "    out = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding, data_format='channels_first')(x)\n",
    "    out = BatchNormalization(axis=1, scale=False)(out)\n",
    "    out = Activation('relu')(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e5ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_a(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 32, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 32, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 32, 3, 3)\n",
    "\n",
    "    branch3 = conv2d_bn(x, 32, 1, 1)\n",
    "    branch3 = conv2d_bn(branch3, 48, 3, 3)\n",
    "    branch3 = conv2d_bn(branch3, 64, 3, 3)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2, branch3], axis=1)\n",
    "    inc_block_out = Conv2D(384, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eac7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_a(x):\n",
    "    branch1 = conv2d_bn(x, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch2 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 256, 3, 3)\n",
    "    branch2 = conv2d_bn(branch2, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch3 = MaxPooling2D((3,3), strides=(2,2), padding='valid', data_format='channels_first')(x)\n",
    "\n",
    "    out = concatenate([branch1, branch2, branch3], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50a430e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_b(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 160, 1, 7)\n",
    "    branch2 = conv2d_bn(branch2, 192, 7, 1)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2], axis=1)\n",
    "    inc_block_out = Conv2D(1152, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b4a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_b(x):\n",
    "    branch1 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch1 = conv2d_bn(branch1, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch2 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 288, 3, 3)\n",
    "    branch2 = conv2d_bn(branch2, 320, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch3 = MaxPooling2D((3,3), strides=(2,2), padding='valid', data_format='channels_first')(x)\n",
    "\n",
    "    branch4 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch4 = conv2d_bn(branch4, 288, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    out = concatenate([branch1, branch2, branch3, branch4], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19668269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_c(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 224, 1, 3)\n",
    "    branch2 = conv2d_bn(branch2, 256, 3, 1)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2], axis=1)\n",
    "    inc_block_out = Conv2D(2144, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3383ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x):\n",
    "    out = conv2d_bn(x, 32, 3, 3)\n",
    "    out = conv2d_bn(out, 32, 3, 3)\n",
    "    out = conv2d_bn(out, 64, 3, 3)\n",
    "\n",
    "    branch1 = MaxPooling2D((3, 3), strides=(2,2), padding='same', data_format='channels_first')(out)\n",
    "    branch2 = conv2d_bn(out, 96, 3, 3, strides=(2,2))\n",
    "    out = concatenate([branch1, branch2], axis=1)\n",
    "\n",
    "    branch1 = conv2d_bn(out, 64, 1, 1)\n",
    "    branch1 = conv2d_bn(branch1, 96, 3, 3)\n",
    "    branch2 = conv2d_bn(out, 64, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 64, 7, 1)\n",
    "    branch2 = conv2d_bn(branch2, 64, 1, 7)\n",
    "    branch2 = conv2d_bn(branch2, 96, 3, 3)\n",
    "    out = concatenate([branch1, branch2], axis=1)\n",
    "\n",
    "    out = conv2d_bn(out, 384, 3, 3)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f962e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=IMAGE_SHAPE)\n",
    "x = stem(input_)\n",
    "\n",
    "x = inc_block_a(x)\n",
    "x = inc_block_a(x)\n",
    "x = reduction_block_a(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = reduction_block_b(x)\n",
    "x = inc_block_c(x)\n",
    "x = inc_block_c(x)\n",
    "x = GlobalAveragePooling2D(data_format='channels_first')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model(input_, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a32f9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 64, 64)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 64, 64)   1472        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 64, 64)  96          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 64, 64)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 64, 64)   9248        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 64, 64)  96          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 64, 64)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 64)   18496       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 64)  192         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64, 64, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, 32, 32)   0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 160, 32, 32)  0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 32, 32)   10304       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 32, 32)   28736       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 32, 32)   10304       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 32, 32)   28736       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 192, 32, 32)  0           ['activation_5[0][0]',           \n",
      "                                                                  'activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 384, 32, 32)  663936      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 384, 32, 32)  1152       ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 384, 32, 32)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 32)  96          ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 48, 32, 32)   13872       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 32)  96          ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 48, 32, 32)  144         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 48, 32, 32)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 32)   9248        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 64, 32, 32)   27712       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 32)  96          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 32)  96          ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 64, 32, 32)  192         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 64, 32, 32)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 32, 32)  0           ['activation_11[0][0]',          \n",
      "                                                                  'activation_13[0][0]',          \n",
      "                                                                  'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 384, 32, 32)  49536       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 384, 32, 32)  0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 384, 32, 32)  0           ['activation_10[0][0]',          \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 384, 32, 32)  0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 32)  96          ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 48, 32, 32)   13872       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 32)  96          ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 48, 32, 32)  144         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 48, 32, 32)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 32, 32, 32)   9248        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 64, 32, 32)   27712       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 32)  96          ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 32)  96          ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 64, 32, 32)  192         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 64, 32, 32)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 128, 32, 32)  0           ['activation_18[0][0]',          \n",
      "                                                                  'activation_20[0][0]',          \n",
      "                                                                  'activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 384, 32, 32)  49536       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 384, 32, 32)  0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 384, 32, 32)  0           ['activation_17[0][0]',          \n",
      "                                                                  'lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 384, 32, 32)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 256, 32, 32)  98560       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 256, 32, 32)  768        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 256, 32, 32)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 256, 32, 32)  590080      ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 256, 32, 32)  768        ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 256, 32, 32)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 384, 15, 15)  1327488     ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 384, 15, 15)  885120      ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 384, 15, 15)  1152       ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 384, 15, 15)  1152       ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 384, 15, 15)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 384, 15, 15)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 384, 15, 15)  0          ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 1152, 15, 15  0           ['activation_25[0][0]',          \n",
      "                                )                                 'activation_28[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 128, 15, 15)  147584      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 128, 15, 15)  384        ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 160, 15, 15)  480        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 192, 15, 15)  221376      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 192, 15, 15)  576        ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 192, 15, 15)  576        ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 384, 15, 15)  0           ['activation_29[0][0]',          \n",
      "                                                                  'activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_5[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_33[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 1152, 15, 15  0           ['concatenate_4[0][0]',          \n",
      "                                )                                 'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 1152, 15, 15  0           ['add_2[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 128, 15, 15)  384        ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 160, 15, 15)  480        ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 192, 15, 15)  576        ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 192, 15, 15)  576        ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 384, 15, 15)  0           ['activation_34[0][0]',          \n",
      "                                                                  'activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_6[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_38[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 1152, 15, 15  0           ['activation_33[0][0]',          \n",
      "                                )                                 'lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 1152, 15, 15  0           ['add_3[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 128, 15, 15)  384        ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 160, 15, 15)  480        ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 192, 15, 15)  576        ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 192, 15, 15)  576        ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 384, 15, 15)  0           ['activation_39[0][0]',          \n",
      "                                                                  'activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_43[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 1152, 15, 15  0           ['activation_38[0][0]',          \n",
      "                                )                                 'lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 1152, 15, 15  0           ['add_4[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 128, 15, 15)  384        ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 160, 15, 15)  480        ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 192, 15, 15)  576        ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 192, 15, 15)  576        ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 384, 15, 15)  0           ['activation_44[0][0]',          \n",
      "                                                                  'activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_48[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 1152, 15, 15  0           ['activation_43[0][0]',          \n",
      "                                )                                 'lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 1152, 15, 15  0           ['add_5[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 256, 15, 15)  768        ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 288, 15, 15)  663840      ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 256, 15, 15)  768        ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 288, 15, 15)  864        ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 256, 15, 15)  768        ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 288, 15, 15)  0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 384, 7, 7)    885120      ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 320, 7, 7)    829760      ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 288, 7, 7)    663840      ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 384, 7, 7)   1152        ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 320, 7, 7)   960         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 288, 7, 7)   864         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 384, 7, 7)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 320, 7, 7)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1152, 7, 7)  0           ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 288, 7, 7)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 2144, 7, 7)   0           ['activation_50[0][0]',          \n",
      "                                                                  'activation_53[0][0]',          \n",
      "                                                                  'max_pooling2d_2[0][0]',        \n",
      "                                                                  'activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 192, 7, 7)    411840      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 192, 7, 7)   576         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 224, 7, 7)    129248      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 224, 7, 7)   672         ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 224, 7, 7)    0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 192, 7, 7)    411840      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 256, 7, 7)    172288      ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 192, 7, 7)   576         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 256, 7, 7)   768         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 256, 7, 7)    0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 448, 7, 7)    0           ['activation_56[0][0]',          \n",
      "                                                                  'activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 2144, 7, 7)   962656      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 2144, 7, 7)   0           ['conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 2144, 7, 7)   0           ['concatenate_9[0][0]',          \n",
      "                                                                  'lambda_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 2144, 7, 7)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 192, 7, 7)    411840      ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 192, 7, 7)   576         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 224, 7, 7)    129248      ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 224, 7, 7)   672         ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 224, 7, 7)    0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 192, 7, 7)    411840      ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 256, 7, 7)    172288      ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 192, 7, 7)   576         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 256, 7, 7)   768         ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 256, 7, 7)    0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 448, 7, 7)    0           ['activation_61[0][0]',          \n",
      "                                                                  'activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 2144, 7, 7)   962656      ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 2144, 7, 7)   0           ['conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 2144, 7, 7)   0           ['activation_60[0][0]',          \n",
      "                                                                  'lambda_7[0][0]']               \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 2144, 7, 7)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2144)        0           ['activation_65[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2144)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2145        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,932,033\n",
      "Trainable params: 16,913,473\n",
      "Non-trainable params: 18,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e5173bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=hparams['learning_rate']), loss='mse', metrics='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e468ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'inception_resnet_2_64x64_small_v5' # added cooldown\n",
    "\n",
    "checkpoint_filepath = f'/data2/models/{model_name}/checkpoints/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
    "log_dir = os.path.join('/data2/logs/', model_name)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_freq='epoch',\n",
    "    save_best_only=True,\n",
    "    verbose=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "hparam_callback = hp.KerasCallback(log_dir, hparams)\n",
    "\n",
    "plateau_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    cooldown=50,\n",
    "    min_delta=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb6ef640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7592 - mse: 0.7592\n",
      "Epoch 1: loss improved from inf to 0.75917, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 47s 589ms/step - loss: 0.7592 - mse: 0.7592 - val_loss: 0.5038 - val_mse: 0.5038 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6264 - mse: 0.6264\n",
      "Epoch 2: loss improved from 0.75917 to 0.62643, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 20s 513ms/step - loss: 0.6264 - mse: 0.6264 - val_loss: 0.4668 - val_mse: 0.4668 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5949 - mse: 0.5949\n",
      "Epoch 3: loss improved from 0.62643 to 0.59494, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 515ms/step - loss: 0.5949 - mse: 0.5949 - val_loss: 0.4657 - val_mse: 0.4657 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5511 - mse: 0.5511\n",
      "Epoch 4: loss improved from 0.59494 to 0.55115, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.5511 - mse: 0.5511 - val_loss: 0.4332 - val_mse: 0.4332 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5010 - mse: 0.5010\n",
      "Epoch 5: loss improved from 0.55115 to 0.50103, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.5010 - mse: 0.5010 - val_loss: 0.3985 - val_mse: 0.3985 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4964 - mse: 0.4964\n",
      "Epoch 6: loss improved from 0.50103 to 0.49639, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 515ms/step - loss: 0.4964 - mse: 0.4964 - val_loss: 0.3713 - val_mse: 0.3713 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4849 - mse: 0.4849\n",
      "Epoch 7: loss improved from 0.49639 to 0.48495, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.4849 - mse: 0.4849 - val_loss: 0.3464 - val_mse: 0.3464 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5007 - mse: 0.5007\n",
      "Epoch 8: loss did not improve from 0.48495\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.5007 - mse: 0.5007 - val_loss: 0.3435 - val_mse: 0.3435 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4584 - mse: 0.4584\n",
      "Epoch 9: loss improved from 0.48495 to 0.45839, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.4584 - mse: 0.4584 - val_loss: 0.3250 - val_mse: 0.3250 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4524 - mse: 0.4524\n",
      "Epoch 10: loss improved from 0.45839 to 0.45239, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 523ms/step - loss: 0.4524 - mse: 0.4524 - val_loss: 0.3118 - val_mse: 0.3118 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4300 - mse: 0.4300\n",
      "Epoch 11: loss improved from 0.45239 to 0.42997, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.4300 - mse: 0.4300 - val_loss: 0.2919 - val_mse: 0.2919 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4003 - mse: 0.4003\n",
      "Epoch 12: loss improved from 0.42997 to 0.40026, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 526ms/step - loss: 0.4003 - mse: 0.4003 - val_loss: 0.2933 - val_mse: 0.2933 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4036 - mse: 0.4036\n",
      "Epoch 13: loss did not improve from 0.40026\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.4036 - mse: 0.4036 - val_loss: 0.2675 - val_mse: 0.2675 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3928 - mse: 0.3928\n",
      "Epoch 14: loss improved from 0.40026 to 0.39278, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.3928 - mse: 0.3928 - val_loss: 0.2367 - val_mse: 0.2367 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3964 - mse: 0.3964\n",
      "Epoch 15: loss did not improve from 0.39278\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.3964 - mse: 0.3964 - val_loss: 0.2234 - val_mse: 0.2234 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3663 - mse: 0.3663\n",
      "Epoch 16: loss improved from 0.39278 to 0.36627, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.3663 - mse: 0.3663 - val_loss: 0.1877 - val_mse: 0.1877 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4075 - mse: 0.4075\n",
      "Epoch 17: loss did not improve from 0.36627\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.4075 - mse: 0.4075 - val_loss: 0.1619 - val_mse: 0.1619 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3761 - mse: 0.3761\n",
      "Epoch 18: loss did not improve from 0.36627\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.3761 - mse: 0.3761 - val_loss: 0.1520 - val_mse: 0.1520 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3643 - mse: 0.3643\n",
      "Epoch 19: loss improved from 0.36627 to 0.36428, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.3643 - mse: 0.3643 - val_loss: 0.1619 - val_mse: 0.1619 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3730 - mse: 0.3730\n",
      "Epoch 20: loss did not improve from 0.36428\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.3730 - mse: 0.3730 - val_loss: 0.1431 - val_mse: 0.1431 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3350 - mse: 0.3350\n",
      "Epoch 21: loss improved from 0.36428 to 0.33499, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.3350 - mse: 0.3350 - val_loss: 0.1255 - val_mse: 0.1255 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3449 - mse: 0.3449\n",
      "Epoch 22: loss did not improve from 0.33499\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.3449 - mse: 0.3449 - val_loss: 0.3599 - val_mse: 0.3599 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3446 - mse: 0.3446\n",
      "Epoch 23: loss did not improve from 0.33499\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.3446 - mse: 0.3446 - val_loss: 0.1538 - val_mse: 0.1538 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3091 - mse: 0.3091\n",
      "Epoch 24: loss improved from 0.33499 to 0.30912, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.3091 - mse: 0.3091 - val_loss: 0.1614 - val_mse: 0.1614 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3216 - mse: 0.3216\n",
      "Epoch 25: loss did not improve from 0.30912\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.3216 - mse: 0.3216 - val_loss: 0.1680 - val_mse: 0.1680 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3221 - mse: 0.3221\n",
      "Epoch 26: loss did not improve from 0.30912\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.3221 - mse: 0.3221 - val_loss: 0.2145 - val_mse: 0.2145 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3050 - mse: 0.3050\n",
      "Epoch 27: loss improved from 0.30912 to 0.30503, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.3050 - mse: 0.3050 - val_loss: 0.1869 - val_mse: 0.1869 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2955 - mse: 0.2955\n",
      "Epoch 28: loss improved from 0.30503 to 0.29554, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 531ms/step - loss: 0.2955 - mse: 0.2955 - val_loss: 0.1332 - val_mse: 0.1332 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2852 - mse: 0.2852\n",
      "Epoch 29: loss improved from 0.29554 to 0.28522, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 525ms/step - loss: 0.2852 - mse: 0.2852 - val_loss: 0.1266 - val_mse: 0.1266 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2910 - mse: 0.2910\n",
      "Epoch 30: loss did not improve from 0.28522\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.2910 - mse: 0.2910 - val_loss: 0.1253 - val_mse: 0.1253 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2759 - mse: 0.2759\n",
      "Epoch 31: loss improved from 0.28522 to 0.27586, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.2759 - mse: 0.2759 - val_loss: 0.1327 - val_mse: 0.1327 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2900 - mse: 0.2900\n",
      "Epoch 32: loss did not improve from 0.27586\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.2900 - mse: 0.2900 - val_loss: 0.1223 - val_mse: 0.1223 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2812 - mse: 0.2812\n",
      "Epoch 33: loss did not improve from 0.27586\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.2812 - mse: 0.2812 - val_loss: 0.1158 - val_mse: 0.1158 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2694 - mse: 0.2694\n",
      "Epoch 34: loss improved from 0.27586 to 0.26937, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.2694 - mse: 0.2694 - val_loss: 0.1437 - val_mse: 0.1437 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3012 - mse: 0.3012\n",
      "Epoch 35: loss did not improve from 0.26937\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.3012 - mse: 0.3012 - val_loss: 0.1141 - val_mse: 0.1141 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2867 - mse: 0.2867\n",
      "Epoch 36: loss did not improve from 0.26937\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 0.1493 - val_mse: 0.1493 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2513 - mse: 0.2513\n",
      "Epoch 37: loss improved from 0.26937 to 0.25130, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.1447 - val_mse: 0.1447 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2526 - mse: 0.2526\n",
      "Epoch 38: loss did not improve from 0.25130\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.2526 - mse: 0.2526 - val_loss: 0.1279 - val_mse: 0.1279 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2628 - mse: 0.2628\n",
      "Epoch 39: loss did not improve from 0.25130\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.2628 - mse: 0.2628 - val_loss: 0.1646 - val_mse: 0.1646 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2425 - mse: 0.2425\n",
      "Epoch 40: loss improved from 0.25130 to 0.24255, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.1343 - val_mse: 0.1343 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2557 - mse: 0.2557\n",
      "Epoch 41: loss did not improve from 0.24255\n",
      "40/40 [==============================] - 19s 488ms/step - loss: 0.2557 - mse: 0.2557 - val_loss: 0.1321 - val_mse: 0.1321 - lr: 1.0000e-05\n",
      "Epoch 42/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2452 - mse: 0.2452\n",
      "Epoch 42: loss did not improve from 0.24255\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.1242 - val_mse: 0.1242 - lr: 1.0000e-05\n",
      "Epoch 43/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2398 - mse: 0.2398\n",
      "Epoch 43: loss improved from 0.24255 to 0.23977, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.2398 - mse: 0.2398 - val_loss: 0.1343 - val_mse: 0.1343 - lr: 1.0000e-05\n",
      "Epoch 44/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2331 - mse: 0.2331\n",
      "Epoch 44: loss improved from 0.23977 to 0.23310, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 538ms/step - loss: 0.2331 - mse: 0.2331 - val_loss: 0.1302 - val_mse: 0.1302 - lr: 1.0000e-05\n",
      "Epoch 45/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2312 - mse: 0.2312\n",
      "Epoch 45: loss improved from 0.23310 to 0.23119, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 525ms/step - loss: 0.2312 - mse: 0.2312 - val_loss: 0.1222 - val_mse: 0.1222 - lr: 1.0000e-05\n",
      "Epoch 46/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2275 - mse: 0.2275\n",
      "Epoch 46: loss improved from 0.23119 to 0.22746, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 527ms/step - loss: 0.2275 - mse: 0.2275 - val_loss: 0.1171 - val_mse: 0.1171 - lr: 1.0000e-05\n",
      "Epoch 47/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2351 - mse: 0.2351\n",
      "Epoch 47: loss did not improve from 0.22746\n",
      "40/40 [==============================] - 19s 480ms/step - loss: 0.2351 - mse: 0.2351 - val_loss: 0.1181 - val_mse: 0.1181 - lr: 1.0000e-05\n",
      "Epoch 48/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2306 - mse: 0.2306\n",
      "Epoch 48: loss did not improve from 0.22746\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.2306 - mse: 0.2306 - val_loss: 0.1066 - val_mse: 0.1066 - lr: 1.0000e-05\n",
      "Epoch 49/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2279 - mse: 0.2279\n",
      "Epoch 49: loss did not improve from 0.22746\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.2279 - mse: 0.2279 - val_loss: 0.1033 - val_mse: 0.1033 - lr: 1.0000e-05\n",
      "Epoch 50/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2272 - mse: 0.2272\n",
      "Epoch 50: loss improved from 0.22746 to 0.22716, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.2272 - mse: 0.2272 - val_loss: 0.1050 - val_mse: 0.1050 - lr: 1.0000e-05\n",
      "Epoch 51/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2074 - mse: 0.2074\n",
      "Epoch 51: loss improved from 0.22716 to 0.20741, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 525ms/step - loss: 0.2074 - mse: 0.2074 - val_loss: 0.1029 - val_mse: 0.1029 - lr: 1.0000e-05\n",
      "Epoch 52/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2263 - mse: 0.2263\n",
      "Epoch 52: loss did not improve from 0.20741\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.2263 - mse: 0.2263 - val_loss: 0.1185 - val_mse: 0.1185 - lr: 1.0000e-05\n",
      "Epoch 53/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2421 - mse: 0.2421\n",
      "Epoch 53: loss did not improve from 0.20741\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.2421 - mse: 0.2421 - val_loss: 0.1047 - val_mse: 0.1047 - lr: 1.0000e-05\n",
      "Epoch 54/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2089 - mse: 0.2089\n",
      "Epoch 54: loss did not improve from 0.20741\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.2089 - mse: 0.2089 - val_loss: 0.1127 - val_mse: 0.1127 - lr: 1.0000e-05\n",
      "Epoch 55/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2048 - mse: 0.2048\n",
      "Epoch 55: loss improved from 0.20741 to 0.20479, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 525ms/step - loss: 0.2048 - mse: 0.2048 - val_loss: 0.0979 - val_mse: 0.0979 - lr: 1.0000e-05\n",
      "Epoch 56/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2132 - mse: 0.2132\n",
      "Epoch 56: loss did not improve from 0.20479\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.2132 - mse: 0.2132 - val_loss: 0.1127 - val_mse: 0.1127 - lr: 1.0000e-05\n",
      "Epoch 57/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1998 - mse: 0.1998\n",
      "Epoch 57: loss improved from 0.20479 to 0.19978, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 526ms/step - loss: 0.1998 - mse: 0.1998 - val_loss: 0.1019 - val_mse: 0.1019 - lr: 1.0000e-05\n",
      "Epoch 58/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1998 - mse: 0.1998\n",
      "Epoch 58: loss did not improve from 0.19978\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.1998 - mse: 0.1998 - val_loss: 0.1322 - val_mse: 0.1322 - lr: 1.0000e-05\n",
      "Epoch 59/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2115 - mse: 0.2115\n",
      "Epoch 59: loss did not improve from 0.19978\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.2115 - mse: 0.2115 - val_loss: 0.1450 - val_mse: 0.1450 - lr: 1.0000e-05\n",
      "Epoch 60/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2041 - mse: 0.2041\n",
      "Epoch 60: loss did not improve from 0.19978\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.2041 - mse: 0.2041 - val_loss: 0.1592 - val_mse: 0.1592 - lr: 1.0000e-05\n",
      "Epoch 61/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1947 - mse: 0.1947\n",
      "Epoch 61: loss improved from 0.19978 to 0.19468, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.1947 - mse: 0.1947 - val_loss: 0.1151 - val_mse: 0.1151 - lr: 1.0000e-05\n",
      "Epoch 62/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1824 - mse: 0.1824\n",
      "Epoch 62: loss improved from 0.19468 to 0.18243, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.1824 - mse: 0.1824 - val_loss: 0.1037 - val_mse: 0.1037 - lr: 1.0000e-05\n",
      "Epoch 63/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2031 - mse: 0.2031\n",
      "Epoch 63: loss did not improve from 0.18243\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.2031 - mse: 0.2031 - val_loss: 0.1444 - val_mse: 0.1444 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1915 - mse: 0.1915\n",
      "Epoch 64: loss did not improve from 0.18243\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.1915 - mse: 0.1915 - val_loss: 0.1532 - val_mse: 0.1532 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1780 - mse: 0.1780\n",
      "Epoch 65: loss improved from 0.18243 to 0.17797, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.1780 - mse: 0.1780 - val_loss: 0.1285 - val_mse: 0.1285 - lr: 1.0000e-05\n",
      "Epoch 66/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1826 - mse: 0.1826\n",
      "Epoch 66: loss did not improve from 0.17797\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.1826 - mse: 0.1826 - val_loss: 0.0973 - val_mse: 0.0973 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1659 - mse: 0.1659\n",
      "Epoch 67: loss improved from 0.17797 to 0.16585, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.1659 - mse: 0.1659 - val_loss: 0.1391 - val_mse: 0.1391 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1686 - mse: 0.1686\n",
      "Epoch 68: loss did not improve from 0.16585\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.1686 - mse: 0.1686 - val_loss: 0.1143 - val_mse: 0.1143 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1678 - mse: 0.1678\n",
      "Epoch 69: loss did not improve from 0.16585\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.1678 - mse: 0.1678 - val_loss: 0.1202 - val_mse: 0.1202 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1737 - mse: 0.1737\n",
      "Epoch 70: loss did not improve from 0.16585\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.1737 - mse: 0.1737 - val_loss: 0.1013 - val_mse: 0.1013 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1973 - mse: 0.1973\n",
      "Epoch 71: loss did not improve from 0.16585\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.1973 - mse: 0.1973 - val_loss: 0.1043 - val_mse: 0.1043 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1877 - mse: 0.1877\n",
      "Epoch 72: loss did not improve from 0.16585\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.1877 - mse: 0.1877 - val_loss: 0.1068 - val_mse: 0.1068 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1702 - mse: 0.1702\n",
      "Epoch 73: loss did not improve from 0.16585\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.1702 - mse: 0.1702 - val_loss: 0.1053 - val_mse: 0.1053 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1671 - mse: 0.1671\n",
      "Epoch 74: loss did not improve from 0.16585\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.1671 - mse: 0.1671 - val_loss: 0.1040 - val_mse: 0.1040 - lr: 1.0000e-05\n",
      "Epoch 75/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1733 - mse: 0.1733\n",
      "Epoch 75: loss did not improve from 0.16585\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.1733 - mse: 0.1733 - val_loss: 0.0998 - val_mse: 0.0998 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1574 - mse: 0.1574\n",
      "Epoch 76: loss improved from 0.16585 to 0.15738, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.1574 - mse: 0.1574 - val_loss: 0.0950 - val_mse: 0.0950 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1673 - mse: 0.1673\n",
      "Epoch 77: loss did not improve from 0.15738\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.1673 - mse: 0.1673 - val_loss: 0.1023 - val_mse: 0.1023 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1638 - mse: 0.1638\n",
      "Epoch 78: loss did not improve from 0.15738\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.1638 - mse: 0.1638 - val_loss: 0.0881 - val_mse: 0.0881 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1632 - mse: 0.1632\n",
      "Epoch 79: loss did not improve from 0.15738\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.1632 - mse: 0.1632 - val_loss: 0.1139 - val_mse: 0.1139 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1548 - mse: 0.1548\n",
      "Epoch 80: loss improved from 0.15738 to 0.15483, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.1548 - mse: 0.1548 - val_loss: 0.1201 - val_mse: 0.1201 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1437 - mse: 0.1437\n",
      "Epoch 81: loss improved from 0.15483 to 0.14368, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.1437 - mse: 0.1437 - val_loss: 0.0969 - val_mse: 0.0969 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1469 - mse: 0.1469\n",
      "Epoch 82: loss did not improve from 0.14368\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.1469 - mse: 0.1469 - val_loss: 0.0906 - val_mse: 0.0906 - lr: 1.0000e-05\n",
      "Epoch 83/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1385 - mse: 0.1385\n",
      "Epoch 83: loss improved from 0.14368 to 0.13846, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 528ms/step - loss: 0.1385 - mse: 0.1385 - val_loss: 0.0980 - val_mse: 0.0980 - lr: 1.0000e-05\n",
      "Epoch 84/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1328 - mse: 0.1328\n",
      "Epoch 84: loss improved from 0.13846 to 0.13284, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.1328 - mse: 0.1328 - val_loss: 0.0918 - val_mse: 0.0918 - lr: 1.0000e-05\n",
      "Epoch 85/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1349 - mse: 0.1349\n",
      "Epoch 85: loss did not improve from 0.13284\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.1349 - mse: 0.1349 - val_loss: 0.0864 - val_mse: 0.0864 - lr: 1.0000e-05\n",
      "Epoch 86/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1349 - mse: 0.1349\n",
      "Epoch 86: loss did not improve from 0.13284\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.1349 - mse: 0.1349 - val_loss: 0.0920 - val_mse: 0.0920 - lr: 1.0000e-05\n",
      "Epoch 87/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1369 - mse: 0.1369\n",
      "Epoch 87: loss did not improve from 0.13284\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.1369 - mse: 0.1369 - val_loss: 0.0923 - val_mse: 0.0923 - lr: 1.0000e-05\n",
      "Epoch 88/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1270 - mse: 0.1270\n",
      "Epoch 88: loss improved from 0.13284 to 0.12703, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 525ms/step - loss: 0.1270 - mse: 0.1270 - val_loss: 0.4092 - val_mse: 0.4092 - lr: 1.0000e-05\n",
      "Epoch 89/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1355 - mse: 0.1355\n",
      "Epoch 89: loss did not improve from 0.12703\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.1355 - mse: 0.1355 - val_loss: 0.0987 - val_mse: 0.0987 - lr: 1.0000e-05\n",
      "Epoch 90/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1219 - mse: 0.1219\n",
      "Epoch 90: loss improved from 0.12703 to 0.12193, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.1219 - mse: 0.1219 - val_loss: 0.1425 - val_mse: 0.1425 - lr: 1.0000e-05\n",
      "Epoch 91/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1234 - mse: 0.1234\n",
      "Epoch 91: loss did not improve from 0.12193\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.1234 - mse: 0.1234 - val_loss: 0.1013 - val_mse: 0.1013 - lr: 1.0000e-05\n",
      "Epoch 92/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1193 - mse: 0.1193\n",
      "Epoch 92: loss improved from 0.12193 to 0.11931, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 526ms/step - loss: 0.1193 - mse: 0.1193 - val_loss: 0.1095 - val_mse: 0.1095 - lr: 1.0000e-05\n",
      "Epoch 93/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1177 - mse: 0.1177\n",
      "Epoch 93: loss improved from 0.11931 to 0.11768, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 523ms/step - loss: 0.1177 - mse: 0.1177 - val_loss: 0.1225 - val_mse: 0.1225 - lr: 1.0000e-05\n",
      "Epoch 94/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1104 - mse: 0.1104\n",
      "Epoch 94: loss improved from 0.11768 to 0.11040, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.1104 - mse: 0.1104 - val_loss: 0.1033 - val_mse: 0.1033 - lr: 1.0000e-05\n",
      "Epoch 95/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 95: loss improved from 0.11040 to 0.10303, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.1030 - mse: 0.1030 - val_loss: 0.1050 - val_mse: 0.1050 - lr: 1.0000e-05\n",
      "Epoch 96/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1045 - mse: 0.1045\n",
      "Epoch 96: loss did not improve from 0.10303\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.1045 - mse: 0.1045 - val_loss: 0.1055 - val_mse: 0.1055 - lr: 1.0000e-05\n",
      "Epoch 97/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0988 - mse: 0.0988\n",
      "Epoch 97: loss improved from 0.10303 to 0.09882, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.1602 - val_mse: 0.1602 - lr: 1.0000e-05\n",
      "Epoch 98/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1020 - mse: 0.1020\n",
      "Epoch 98: loss did not improve from 0.09882\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.1020 - mse: 0.1020 - val_loss: 0.1021 - val_mse: 0.1021 - lr: 1.0000e-05\n",
      "Epoch 99/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1053 - mse: 0.1053\n",
      "Epoch 99: loss did not improve from 0.09882\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.1053 - mse: 0.1053 - val_loss: 0.1478 - val_mse: 0.1478 - lr: 1.0000e-05\n",
      "Epoch 100/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0931 - mse: 0.0931\n",
      "Epoch 100: loss improved from 0.09882 to 0.09309, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0931 - mse: 0.0931 - val_loss: 0.1145 - val_mse: 0.1145 - lr: 1.0000e-05\n",
      "Epoch 101/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0989 - mse: 0.0989\n",
      "Epoch 101: loss did not improve from 0.09309\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.1067 - val_mse: 0.1067 - lr: 1.0000e-05\n",
      "Epoch 102/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0950 - mse: 0.0950\n",
      "Epoch 102: loss did not improve from 0.09309\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0950 - mse: 0.0950 - val_loss: 0.1323 - val_mse: 0.1323 - lr: 1.0000e-05\n",
      "Epoch 103/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0932 - mse: 0.0932\n",
      "Epoch 103: loss did not improve from 0.09309\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0932 - mse: 0.0932 - val_loss: 0.1328 - val_mse: 0.1328 - lr: 1.0000e-05\n",
      "Epoch 104/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0915 - mse: 0.0915\n",
      "Epoch 104: loss improved from 0.09309 to 0.09153, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.1203 - val_mse: 0.1203 - lr: 1.0000e-05\n",
      "Epoch 105/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0880 - mse: 0.0880\n",
      "Epoch 105: loss improved from 0.09153 to 0.08801, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.0961 - val_mse: 0.0961 - lr: 1.0000e-05\n",
      "Epoch 106/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0781 - mse: 0.0781\n",
      "Epoch 106: loss improved from 0.08801 to 0.07811, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.0781 - mse: 0.0781 - val_loss: 0.0973 - val_mse: 0.0973 - lr: 1.0000e-06\n",
      "Epoch 107/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0775 - mse: 0.0775\n",
      "Epoch 107: loss improved from 0.07811 to 0.07753, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 523ms/step - loss: 0.0775 - mse: 0.0775 - val_loss: 0.0974 - val_mse: 0.0974 - lr: 1.0000e-06\n",
      "Epoch 108/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0825 - mse: 0.0825\n",
      "Epoch 108: loss did not improve from 0.07753\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.0965 - val_mse: 0.0965 - lr: 1.0000e-06\n",
      "Epoch 109/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0788 - mse: 0.0788\n",
      "Epoch 109: loss did not improve from 0.07753\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0788 - mse: 0.0788 - val_loss: 0.0987 - val_mse: 0.0987 - lr: 1.0000e-06\n",
      "Epoch 110/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0757 - mse: 0.0757\n",
      "Epoch 110: loss improved from 0.07753 to 0.07574, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.0757 - mse: 0.0757 - val_loss: 0.0997 - val_mse: 0.0997 - lr: 1.0000e-06\n",
      "Epoch 111/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0756 - mse: 0.0756\n",
      "Epoch 111: loss improved from 0.07574 to 0.07556, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 529ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0983 - val_mse: 0.0983 - lr: 1.0000e-06\n",
      "Epoch 112/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0776 - mse: 0.0776\n",
      "Epoch 112: loss did not improve from 0.07556\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0776 - mse: 0.0776 - val_loss: 0.0957 - val_mse: 0.0957 - lr: 1.0000e-06\n",
      "Epoch 113/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0709 - mse: 0.0709\n",
      "Epoch 113: loss improved from 0.07556 to 0.07094, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0709 - mse: 0.0709 - val_loss: 0.0990 - val_mse: 0.0990 - lr: 1.0000e-06\n",
      "Epoch 114/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0775 - mse: 0.0775\n",
      "Epoch 114: loss did not improve from 0.07094\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0775 - mse: 0.0775 - val_loss: 0.0994 - val_mse: 0.0994 - lr: 1.0000e-06\n",
      "Epoch 115/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0744 - mse: 0.0744\n",
      "Epoch 115: loss did not improve from 0.07094\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.0744 - mse: 0.0744 - val_loss: 0.0968 - val_mse: 0.0968 - lr: 1.0000e-06\n",
      "Epoch 116/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0746 - mse: 0.0746\n",
      "Epoch 116: loss did not improve from 0.07094\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0746 - mse: 0.0746 - val_loss: 0.1043 - val_mse: 0.1043 - lr: 1.0000e-06\n",
      "Epoch 117/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0765 - mse: 0.0765\n",
      "Epoch 117: loss did not improve from 0.07094\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0765 - mse: 0.0765 - val_loss: 0.0976 - val_mse: 0.0976 - lr: 1.0000e-06\n",
      "Epoch 118/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0708 - mse: 0.0708\n",
      "Epoch 118: loss improved from 0.07094 to 0.07081, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 525ms/step - loss: 0.0708 - mse: 0.0708 - val_loss: 0.0982 - val_mse: 0.0982 - lr: 1.0000e-06\n",
      "Epoch 119/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0709 - mse: 0.0709\n",
      "Epoch 119: loss did not improve from 0.07081\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0709 - mse: 0.0709 - val_loss: 0.0985 - val_mse: 0.0985 - lr: 1.0000e-06\n",
      "Epoch 120/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0703 - mse: 0.0703\n",
      "Epoch 120: loss improved from 0.07081 to 0.07030, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 523ms/step - loss: 0.0703 - mse: 0.0703 - val_loss: 0.0965 - val_mse: 0.0965 - lr: 1.0000e-06\n",
      "Epoch 121/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0697 - mse: 0.0697\n",
      "Epoch 121: loss improved from 0.07030 to 0.06966, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.0697 - mse: 0.0697 - val_loss: 0.1027 - val_mse: 0.1027 - lr: 1.0000e-06\n",
      "Epoch 122/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0709 - mse: 0.0709\n",
      "Epoch 122: loss did not improve from 0.06966\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0709 - mse: 0.0709 - val_loss: 0.0976 - val_mse: 0.0976 - lr: 1.0000e-06\n",
      "Epoch 123/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0758 - mse: 0.0758\n",
      "Epoch 123: loss did not improve from 0.06966\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0758 - mse: 0.0758 - val_loss: 0.1000 - val_mse: 0.1000 - lr: 1.0000e-06\n",
      "Epoch 124/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0780 - mse: 0.0780\n",
      "Epoch 124: loss did not improve from 0.06966\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0780 - mse: 0.0780 - val_loss: 0.0979 - val_mse: 0.0979 - lr: 1.0000e-06\n",
      "Epoch 125/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0759 - mse: 0.0759\n",
      "Epoch 125: loss did not improve from 0.06966\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0759 - mse: 0.0759 - val_loss: 0.1044 - val_mse: 0.1044 - lr: 1.0000e-06\n",
      "Epoch 126/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0709 - mse: 0.0709\n",
      "Epoch 126: loss did not improve from 0.06966\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0709 - mse: 0.0709 - val_loss: 0.1014 - val_mse: 0.1014 - lr: 1.0000e-06\n",
      "Epoch 127/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0734 - mse: 0.0734\n",
      "Epoch 127: loss did not improve from 0.06966\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0734 - mse: 0.0734 - val_loss: 0.0991 - val_mse: 0.0991 - lr: 1.0000e-06\n",
      "Epoch 128/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0756 - mse: 0.0756\n",
      "Epoch 128: loss did not improve from 0.06966\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.1057 - val_mse: 0.1057 - lr: 1.0000e-06\n",
      "Epoch 129/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0714 - mse: 0.0714\n",
      "Epoch 129: loss did not improve from 0.06966\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0714 - mse: 0.0714 - val_loss: 0.1056 - val_mse: 0.1056 - lr: 1.0000e-06\n",
      "Epoch 130/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0691 - mse: 0.0691\n",
      "Epoch 130: loss improved from 0.06966 to 0.06907, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0691 - mse: 0.0691 - val_loss: 0.1061 - val_mse: 0.1061 - lr: 1.0000e-06\n",
      "Epoch 131/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0710 - mse: 0.0710\n",
      "Epoch 131: loss did not improve from 0.06907\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0710 - mse: 0.0710 - val_loss: 0.0975 - val_mse: 0.0975 - lr: 1.0000e-06\n",
      "Epoch 132/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0726 - mse: 0.0726\n",
      "Epoch 132: loss did not improve from 0.06907\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0726 - mse: 0.0726 - val_loss: 0.0971 - val_mse: 0.0971 - lr: 1.0000e-06\n",
      "Epoch 133/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0710 - mse: 0.0710\n",
      "Epoch 133: loss did not improve from 0.06907\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0710 - mse: 0.0710 - val_loss: 0.1014 - val_mse: 0.1014 - lr: 1.0000e-06\n",
      "Epoch 134/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0764 - mse: 0.0764\n",
      "Epoch 134: loss did not improve from 0.06907\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0764 - mse: 0.0764 - val_loss: 0.1032 - val_mse: 0.1032 - lr: 1.0000e-06\n",
      "Epoch 135/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0714 - mse: 0.0714\n",
      "Epoch 135: loss did not improve from 0.06907\n",
      "40/40 [==============================] - 19s 492ms/step - loss: 0.0714 - mse: 0.0714 - val_loss: 0.0953 - val_mse: 0.0953 - lr: 1.0000e-06\n",
      "Epoch 136/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0662 - mse: 0.0662\n",
      "Epoch 136: loss improved from 0.06907 to 0.06616, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0662 - mse: 0.0662 - val_loss: 0.1055 - val_mse: 0.1055 - lr: 1.0000e-06\n",
      "Epoch 137/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0688 - mse: 0.0688\n",
      "Epoch 137: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0688 - mse: 0.0688 - val_loss: 0.0997 - val_mse: 0.0997 - lr: 1.0000e-06\n",
      "Epoch 138/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0711 - mse: 0.0711\n",
      "Epoch 138: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0711 - mse: 0.0711 - val_loss: 0.0992 - val_mse: 0.0992 - lr: 1.0000e-06\n",
      "Epoch 139/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0672 - mse: 0.0672\n",
      "Epoch 139: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0672 - mse: 0.0672 - val_loss: 0.0979 - val_mse: 0.0979 - lr: 1.0000e-06\n",
      "Epoch 140/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0795 - mse: 0.0795\n",
      "Epoch 140: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0795 - mse: 0.0795 - val_loss: 0.1042 - val_mse: 0.1042 - lr: 1.0000e-06\n",
      "Epoch 141/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0693 - mse: 0.0693\n",
      "Epoch 141: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0693 - mse: 0.0693 - val_loss: 0.1042 - val_mse: 0.1042 - lr: 1.0000e-06\n",
      "Epoch 142/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0705 - mse: 0.0705\n",
      "Epoch 142: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0705 - mse: 0.0705 - val_loss: 0.0975 - val_mse: 0.0975 - lr: 1.0000e-06\n",
      "Epoch 143/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0664 - mse: 0.0664\n",
      "Epoch 143: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0664 - mse: 0.0664 - val_loss: 0.0989 - val_mse: 0.0989 - lr: 1.0000e-06\n",
      "Epoch 144/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0667 - mse: 0.0667\n",
      "Epoch 144: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0667 - mse: 0.0667 - val_loss: 0.1022 - val_mse: 0.1022 - lr: 1.0000e-06\n",
      "Epoch 145/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0680 - mse: 0.0680\n",
      "Epoch 145: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0680 - mse: 0.0680 - val_loss: 0.1077 - val_mse: 0.1077 - lr: 1.0000e-06\n",
      "Epoch 146/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0690 - mse: 0.0690\n",
      "Epoch 146: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0690 - mse: 0.0690 - val_loss: 0.0980 - val_mse: 0.0980 - lr: 1.0000e-06\n",
      "Epoch 147/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0673 - mse: 0.0673\n",
      "Epoch 147: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 486ms/step - loss: 0.0673 - mse: 0.0673 - val_loss: 0.0998 - val_mse: 0.0998 - lr: 1.0000e-06\n",
      "Epoch 148/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0683 - mse: 0.0683\n",
      "Epoch 148: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0683 - mse: 0.0683 - val_loss: 0.0988 - val_mse: 0.0988 - lr: 1.0000e-06\n",
      "Epoch 149/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0741 - mse: 0.0741\n",
      "Epoch 149: loss did not improve from 0.06616\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0741 - mse: 0.0741 - val_loss: 0.1075 - val_mse: 0.1075 - lr: 1.0000e-06\n",
      "Epoch 150/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0647 - mse: 0.0647\n",
      "Epoch 150: loss improved from 0.06616 to 0.06469, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 523ms/step - loss: 0.0647 - mse: 0.0647 - val_loss: 0.1065 - val_mse: 0.1065 - lr: 1.0000e-06\n",
      "Epoch 151/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0700 - mse: 0.0700\n",
      "Epoch 151: loss did not improve from 0.06469\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0700 - mse: 0.0700 - val_loss: 0.1045 - val_mse: 0.1045 - lr: 1.0000e-06\n",
      "Epoch 152/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0671 - mse: 0.0671\n",
      "Epoch 152: loss did not improve from 0.06469\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0671 - mse: 0.0671 - val_loss: 0.0959 - val_mse: 0.0959 - lr: 1.0000e-06\n",
      "Epoch 153/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0679 - mse: 0.0679\n",
      "Epoch 153: loss did not improve from 0.06469\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0679 - mse: 0.0679 - val_loss: 0.1044 - val_mse: 0.1044 - lr: 1.0000e-06\n",
      "Epoch 154/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0658 - mse: 0.0658\n",
      "Epoch 154: loss did not improve from 0.06469\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0658 - mse: 0.0658 - val_loss: 0.1066 - val_mse: 0.1066 - lr: 1.0000e-06\n",
      "Epoch 155/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0670 - mse: 0.0670\n",
      "Epoch 155: loss did not improve from 0.06469\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0670 - mse: 0.0670 - val_loss: 0.1072 - val_mse: 0.1072 - lr: 1.0000e-06\n",
      "Epoch 156/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0674 - mse: 0.0674\n",
      "Epoch 156: loss did not improve from 0.06469\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0674 - mse: 0.0674 - val_loss: 0.0998 - val_mse: 0.0998 - lr: 1.0000e-06\n",
      "Epoch 157/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0672 - mse: 0.0672\n",
      "Epoch 157: loss did not improve from 0.06469\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0672 - mse: 0.0672 - val_loss: 0.1012 - val_mse: 0.1012 - lr: 1.0000e-06\n",
      "Epoch 158/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0620 - mse: 0.0620\n",
      "Epoch 158: loss improved from 0.06469 to 0.06196, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.0620 - mse: 0.0620 - val_loss: 0.1009 - val_mse: 0.1009 - lr: 1.0000e-06\n",
      "Epoch 159/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0694 - mse: 0.0694\n",
      "Epoch 159: loss did not improve from 0.06196\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0694 - mse: 0.0694 - val_loss: 0.1111 - val_mse: 0.1111 - lr: 1.0000e-06\n",
      "Epoch 160/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0695 - mse: 0.0695\n",
      "Epoch 160: loss did not improve from 0.06196\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0695 - mse: 0.0695 - val_loss: 0.1079 - val_mse: 0.1079 - lr: 1.0000e-06\n",
      "Epoch 161/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0655 - mse: 0.0655\n",
      "Epoch 161: loss did not improve from 0.06196\n",
      "40/40 [==============================] - 19s 492ms/step - loss: 0.0655 - mse: 0.0655 - val_loss: 0.1111 - val_mse: 0.1111 - lr: 1.0000e-06\n",
      "Epoch 162/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0647 - mse: 0.0647\n",
      "Epoch 162: loss did not improve from 0.06196\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0647 - mse: 0.0647 - val_loss: 0.0996 - val_mse: 0.0996 - lr: 1.0000e-06\n",
      "Epoch 163/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0611 - mse: 0.0611\n",
      "Epoch 163: loss improved from 0.06196 to 0.06115, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0611 - mse: 0.0611 - val_loss: 0.0980 - val_mse: 0.0980 - lr: 1.0000e-06\n",
      "Epoch 164/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0661 - mse: 0.0661\n",
      "Epoch 164: loss did not improve from 0.06115\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0661 - mse: 0.0661 - val_loss: 0.1005 - val_mse: 0.1005 - lr: 1.0000e-06\n",
      "Epoch 165/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0701 - mse: 0.0701\n",
      "Epoch 165: loss did not improve from 0.06115\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0701 - mse: 0.0701 - val_loss: 0.0977 - val_mse: 0.0977 - lr: 1.0000e-06\n",
      "Epoch 166/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0655 - mse: 0.0655\n",
      "Epoch 166: loss did not improve from 0.06115\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0655 - mse: 0.0655 - val_loss: 0.1011 - val_mse: 0.1011 - lr: 1.0000e-06\n",
      "Epoch 167/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0641 - mse: 0.0641\n",
      "Epoch 167: loss did not improve from 0.06115\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0641 - mse: 0.0641 - val_loss: 0.0989 - val_mse: 0.0989 - lr: 1.0000e-06\n",
      "Epoch 168/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0612 - mse: 0.0612\n",
      "Epoch 168: loss did not improve from 0.06115\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0612 - mse: 0.0612 - val_loss: 0.1051 - val_mse: 0.1051 - lr: 1.0000e-06\n",
      "Epoch 169/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0644 - mse: 0.0644\n",
      "Epoch 169: loss did not improve from 0.06115\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0644 - mse: 0.0644 - val_loss: 0.0996 - val_mse: 0.0996 - lr: 1.0000e-06\n",
      "Epoch 170/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0670 - mse: 0.0670\n",
      "Epoch 170: loss did not improve from 0.06115\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0670 - mse: 0.0670 - val_loss: 0.1091 - val_mse: 0.1091 - lr: 1.0000e-06\n",
      "Epoch 171/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0643 - mse: 0.0643\n",
      "Epoch 171: loss did not improve from 0.06115\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0643 - mse: 0.0643 - val_loss: 0.1007 - val_mse: 0.1007 - lr: 1.0000e-06\n",
      "Epoch 172/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0609 - mse: 0.0609\n",
      "Epoch 172: loss improved from 0.06115 to 0.06093, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.0609 - mse: 0.0609 - val_loss: 0.1040 - val_mse: 0.1040 - lr: 1.0000e-06\n",
      "Epoch 173/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0612 - mse: 0.0612\n",
      "Epoch 173: loss did not improve from 0.06093\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0612 - mse: 0.0612 - val_loss: 0.1004 - val_mse: 0.1004 - lr: 1.0000e-06\n",
      "Epoch 174/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0647 - mse: 0.0647\n",
      "Epoch 174: loss did not improve from 0.06093\n",
      "\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0647 - mse: 0.0647 - val_loss: 0.1072 - val_mse: 0.1072 - lr: 1.0000e-06\n",
      "Epoch 175/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0632 - mse: 0.0632\n",
      "Epoch 175: loss did not improve from 0.06093\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0632 - mse: 0.0632 - val_loss: 0.1025 - val_mse: 0.1025 - lr: 1.0000e-07\n",
      "Epoch 176/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0650 - mse: 0.0650\n",
      "Epoch 176: loss did not improve from 0.06093\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0650 - mse: 0.0650 - val_loss: 0.1003 - val_mse: 0.1003 - lr: 1.0000e-07\n",
      "Epoch 177/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0635 - mse: 0.0635\n",
      "Epoch 177: loss did not improve from 0.06093\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0635 - mse: 0.0635 - val_loss: 0.1007 - val_mse: 0.1007 - lr: 1.0000e-07\n",
      "Epoch 178/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0596 - mse: 0.0596\n",
      "Epoch 178: loss improved from 0.06093 to 0.05961, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0596 - mse: 0.0596 - val_loss: 0.1026 - val_mse: 0.1026 - lr: 1.0000e-07\n",
      "Epoch 179/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0635 - mse: 0.0635\n",
      "Epoch 179: loss did not improve from 0.05961\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0635 - mse: 0.0635 - val_loss: 0.1029 - val_mse: 0.1029 - lr: 1.0000e-07\n",
      "Epoch 180/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0616 - mse: 0.0616\n",
      "Epoch 180: loss did not improve from 0.05961\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0616 - mse: 0.0616 - val_loss: 0.1027 - val_mse: 0.1027 - lr: 1.0000e-07\n",
      "Epoch 181/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0614 - mse: 0.0614\n",
      "Epoch 181: loss did not improve from 0.05961\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0614 - mse: 0.0614 - val_loss: 0.1017 - val_mse: 0.1017 - lr: 1.0000e-07\n",
      "Epoch 182/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0641 - mse: 0.0641\n",
      "Epoch 182: loss did not improve from 0.05961\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0641 - mse: 0.0641 - val_loss: 0.1011 - val_mse: 0.1011 - lr: 1.0000e-07\n",
      "Epoch 183/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0633 - mse: 0.0633\n",
      "Epoch 183: loss did not improve from 0.05961\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0633 - mse: 0.0633 - val_loss: 0.1011 - val_mse: 0.1011 - lr: 1.0000e-07\n",
      "Epoch 184/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0623 - mse: 0.0623\n",
      "Epoch 184: loss did not improve from 0.05961\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0623 - mse: 0.0623 - val_loss: 0.1012 - val_mse: 0.1012 - lr: 1.0000e-07\n",
      "Epoch 185/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0661 - mse: 0.0661\n",
      "Epoch 185: loss did not improve from 0.05961\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0661 - mse: 0.0661 - val_loss: 0.1023 - val_mse: 0.1023 - lr: 1.0000e-07\n",
      "Epoch 186/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0593 - mse: 0.0593\n",
      "Epoch 186: loss improved from 0.05961 to 0.05931, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0593 - mse: 0.0593 - val_loss: 0.1027 - val_mse: 0.1027 - lr: 1.0000e-07\n",
      "Epoch 187/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0586 - mse: 0.0586\n",
      "Epoch 187: loss improved from 0.05931 to 0.05864, saving model to /data2/models/inception_resnet_2_64x64_small_v5/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0586 - mse: 0.0586 - val_loss: 0.1038 - val_mse: 0.1038 - lr: 1.0000e-07\n",
      "Epoch 188/300\n",
      " 7/40 [====>.........................] - ETA: 13s - loss: 0.0607 - mse: 0.0607"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparam_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplateau_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterhub/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_gen, batch_size=hparams['batch_size'], epochs=hparams['num_epochs'], shuffle=True, verbose=1, validation_data=val_gen, callbacks=[tensorboard_callback, model_checkpoint_callback, hparam_callback, plateau_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec5e631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f2bc5738e20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f37d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 153ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92956d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(TEST_PATH, 'r') as file:\n",
    "    y_test = np.asarray(file['specz_redshift'][:])\n",
    "    oid_test = np.asarray(file['object_id'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8cf4741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(np.ravel(pred), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e952dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_point_metrics(pd.Series(np.ravel(pred)), pd.Series(y_test), binned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63a46536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zspec_bin</th>\n",
       "      <th>count</th>\n",
       "      <th>L</th>\n",
       "      <th>bias_bw</th>\n",
       "      <th>bias_conv</th>\n",
       "      <th>scatter_bw</th>\n",
       "      <th>scatter_conv</th>\n",
       "      <th>outlier_bw</th>\n",
       "      <th>outlier_conv</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 4.0]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.273158</td>\n",
       "      <td>0.037813</td>\n",
       "      <td>0.039435</td>\n",
       "      <td>0.109634</td>\n",
       "      <td>0.087085</td>\n",
       "      <td>0.1195</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.110356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    zspec_bin  count         L   bias_bw  bias_conv  scatter_bw  scatter_conv  \\\n",
       "0  (0.0, 4.0]   2000  0.273158  0.037813   0.039435    0.109634      0.087085   \n",
       "\n",
       "   outlier_bw  outlier_conv       mse  \n",
       "0      0.1195        0.2235  0.110356  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1820f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pred, columns=['photoz'])\n",
    "df['specz'] = pd.Series(y_test)\n",
    "df['object_id'] = pd.Series(oid_test)\n",
    "os.makedirs(f'/data2/predictions/{model_name}', exist_ok=True)\n",
    "df.to_csv(f'/data2/predictions/{model_name}/testing_predictions.csv', index=False)\n",
    "metrics.to_csv(f'/data2/predictions/{model_name}/testing_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c98be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
