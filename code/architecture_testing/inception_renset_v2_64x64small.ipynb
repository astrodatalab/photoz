{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd5cea9",
   "metadata": {},
   "source": [
    "# This notebook uses the Inception-ResNet v2 architecture. The stem has been modified to accommodate for 64x64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da1d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 12:53:38.877183: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-01 12:53:39.421621: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-01 12:53:39.421666: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-01 12:53:39.421670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import keras\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Input, concatenate, add, Activation, BatchNormalization, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from photoz_utils import *\n",
    "from DataMakerPlus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c6416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "GB_LIMIT = 17\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(GB_LIMIT*1000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dece0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (5, 64, 64)\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 300\n",
    "LEARNING_RATE = 0.0001\n",
    "Z_MAX = 4\n",
    "hparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'z_max': Z_MAX\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a546612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = f'/data/HSC/HSC_v6/step3A/64x64_training_small.hdf5'\n",
    "VAL_PATH = f'/data/HSC/HSC_v6/step3A/64x64_validation_small.hdf5'\n",
    "TEST_PATH = f'/data/HSC/HSC_v6/step3A/64x64_testing_small.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e5e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_args = {\n",
    "    'image_key': 'image',\n",
    "    'numerical_keys': None,\n",
    "    'y_key': 'specz_redshift',\n",
    "    'scaler': True,\n",
    "    'labels_encoding': False,\n",
    "    'batch_size': hparams['batch_size'],\n",
    "    'shuffle': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8e667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = HDF5DataGenerator(TRAIN_PATH, mode='train', **gen_args)\n",
    "val_gen = HDF5DataGenerator(VAL_PATH, mode='train', **gen_args)\n",
    "test_gen = HDF5DataGenerator(TEST_PATH, mode='test', **gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b1e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def calculate_loss(z_photo, z_spec):\n",
    "    \"\"\"\n",
    "    HSC METRIC. Returns an array. Loss is accuracy metric defined by HSC, meant\n",
    "    to capture the effects of bias, scatter, and outlier all in one. This has\n",
    "    uses for both point and density estimation.\n",
    "    z_photo: array\n",
    "        Photometric or predicted redshifts.\n",
    "    z_spec: array\n",
    "        Spectroscopic or actual redshifts.\n",
    "    \"\"\"\n",
    "    dz = delz(z_photo, z_spec)\n",
    "    gamma = 0.15\n",
    "    denominator = 1.0 + K.square(dz/gamma)\n",
    "    loss = 1 - 1.0 / denominator\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf11cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1,1)):\n",
    "    out = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding, data_format='channels_first')(x)\n",
    "    out = BatchNormalization(axis=1, scale=False)(out)\n",
    "    out = Activation('relu')(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e5ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_a(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 32, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 32, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 32, 3, 3)\n",
    "\n",
    "    branch3 = conv2d_bn(x, 32, 1, 1)\n",
    "    branch3 = conv2d_bn(branch3, 48, 3, 3)\n",
    "    branch3 = conv2d_bn(branch3, 64, 3, 3)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2, branch3], axis=1)\n",
    "    inc_block_out = Conv2D(384, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eac7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_a(x):\n",
    "    branch1 = conv2d_bn(x, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch2 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 256, 3, 3)\n",
    "    branch2 = conv2d_bn(branch2, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch3 = MaxPooling2D((3,3), strides=(2,2), padding='valid', data_format='channels_first')(x)\n",
    "\n",
    "    out = concatenate([branch1, branch2, branch3], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50a430e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_b(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 160, 1, 7)\n",
    "    branch2 = conv2d_bn(branch2, 192, 7, 1)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2], axis=1)\n",
    "    inc_block_out = Conv2D(1152, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b4a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_b(x):\n",
    "    branch1 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch1 = conv2d_bn(branch1, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch2 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 288, 3, 3)\n",
    "    branch2 = conv2d_bn(branch2, 320, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch3 = MaxPooling2D((3,3), strides=(2,2), padding='valid', data_format='channels_first')(x)\n",
    "\n",
    "    branch4 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch4 = conv2d_bn(branch4, 288, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    out = concatenate([branch1, branch2, branch3, branch4], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19668269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_c(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 224, 1, 3)\n",
    "    branch2 = conv2d_bn(branch2, 256, 3, 1)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2], axis=1)\n",
    "    inc_block_out = Conv2D(2144, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3383ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x):\n",
    "    out = conv2d_bn(x, 32, 3, 3)\n",
    "    out = conv2d_bn(out, 32, 3, 3)\n",
    "    out = conv2d_bn(out, 64, 3, 3)\n",
    "\n",
    "    branch1 = MaxPooling2D((3, 3), strides=(2,2), padding='same', data_format='channels_first')(out)\n",
    "    branch2 = conv2d_bn(out, 96, 3, 3, strides=(2,2))\n",
    "    out = concatenate([branch1, branch2], axis=1)\n",
    "\n",
    "    branch1 = conv2d_bn(out, 64, 1, 1)\n",
    "    branch1 = conv2d_bn(branch1, 96, 3, 3)\n",
    "    branch2 = conv2d_bn(out, 64, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 64, 7, 1)\n",
    "    branch2 = conv2d_bn(branch2, 64, 1, 7)\n",
    "    branch2 = conv2d_bn(branch2, 96, 3, 3)\n",
    "    out = concatenate([branch1, branch2], axis=1)\n",
    "\n",
    "    out = conv2d_bn(out, 384, 3, 3)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f962e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=IMAGE_SHAPE)\n",
    "x = stem(input_)\n",
    "\n",
    "x = inc_block_a(x)\n",
    "x = inc_block_a(x)\n",
    "x = reduction_block_a(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = reduction_block_b(x)\n",
    "x = inc_block_c(x)\n",
    "x = inc_block_c(x)\n",
    "x = GlobalAveragePooling2D(data_format='channels_first')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model(input_, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a32f9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 64, 64)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 64, 64)   1472        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 64, 64)  96          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 64, 64)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 64, 64)   9248        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 64, 64)  96          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 64, 64)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 64)   18496       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 64)  192         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64, 64, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, 32, 32)   0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 160, 32, 32)  0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 32, 32)   10304       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 32, 32)   28736       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 32, 32)   10304       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 32, 32)   28736       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 192, 32, 32)  0           ['activation_5[0][0]',           \n",
      "                                                                  'activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 384, 32, 32)  663936      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 384, 32, 32)  1152       ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 384, 32, 32)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 32)  96          ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 48, 32, 32)   13872       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 32)  96          ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 48, 32, 32)  144         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 48, 32, 32)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 32)   9248        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 64, 32, 32)   27712       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 32)  96          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 32)  96          ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 64, 32, 32)  192         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 64, 32, 32)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 32, 32)  0           ['activation_11[0][0]',          \n",
      "                                                                  'activation_13[0][0]',          \n",
      "                                                                  'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 384, 32, 32)  49536       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 384, 32, 32)  0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 384, 32, 32)  0           ['activation_10[0][0]',          \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 384, 32, 32)  0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 32)  96          ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 48, 32, 32)   13872       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 32)  96          ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 48, 32, 32)  144         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 48, 32, 32)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 32, 32, 32)   9248        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 64, 32, 32)   27712       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 32)  96          ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 32)  96          ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 64, 32, 32)  192         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 64, 32, 32)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 128, 32, 32)  0           ['activation_18[0][0]',          \n",
      "                                                                  'activation_20[0][0]',          \n",
      "                                                                  'activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 384, 32, 32)  49536       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 384, 32, 32)  0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 384, 32, 32)  0           ['activation_17[0][0]',          \n",
      "                                                                  'lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 384, 32, 32)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 256, 32, 32)  98560       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 256, 32, 32)  768        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 256, 32, 32)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 256, 32, 32)  590080      ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 256, 32, 32)  768        ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 256, 32, 32)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 384, 15, 15)  1327488     ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 384, 15, 15)  885120      ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 384, 15, 15)  1152       ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 384, 15, 15)  1152       ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 384, 15, 15)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 384, 15, 15)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 384, 15, 15)  0          ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 1152, 15, 15  0           ['activation_25[0][0]',          \n",
      "                                )                                 'activation_28[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 128, 15, 15)  147584      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 128, 15, 15)  384        ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 160, 15, 15)  480        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 192, 15, 15)  221376      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 192, 15, 15)  576        ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 192, 15, 15)  576        ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 384, 15, 15)  0           ['activation_29[0][0]',          \n",
      "                                                                  'activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_5[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_33[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 1152, 15, 15  0           ['concatenate_4[0][0]',          \n",
      "                                )                                 'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 1152, 15, 15  0           ['add_2[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 128, 15, 15)  384        ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 160, 15, 15)  480        ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 192, 15, 15)  576        ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 192, 15, 15)  576        ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 384, 15, 15)  0           ['activation_34[0][0]',          \n",
      "                                                                  'activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_6[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_38[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 1152, 15, 15  0           ['activation_33[0][0]',          \n",
      "                                )                                 'lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 1152, 15, 15  0           ['add_3[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 128, 15, 15)  384        ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 160, 15, 15)  480        ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 192, 15, 15)  576        ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 192, 15, 15)  576        ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 384, 15, 15)  0           ['activation_39[0][0]',          \n",
      "                                                                  'activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_43[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 1152, 15, 15  0           ['activation_38[0][0]',          \n",
      "                                )                                 'lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 1152, 15, 15  0           ['add_4[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 128, 15, 15)  384        ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 160, 15, 15)  480        ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 192, 15, 15)  576        ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 192, 15, 15)  576        ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 384, 15, 15)  0           ['activation_44[0][0]',          \n",
      "                                                                  'activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_48[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 1152, 15, 15  0           ['activation_43[0][0]',          \n",
      "                                )                                 'lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 1152, 15, 15  0           ['add_5[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 256, 15, 15)  768        ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 288, 15, 15)  663840      ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 256, 15, 15)  768        ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 288, 15, 15)  864        ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 256, 15, 15)  768        ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 288, 15, 15)  0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 384, 7, 7)    885120      ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 320, 7, 7)    829760      ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 288, 7, 7)    663840      ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 384, 7, 7)   1152        ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 320, 7, 7)   960         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 288, 7, 7)   864         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 384, 7, 7)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 320, 7, 7)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1152, 7, 7)  0           ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 288, 7, 7)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 2144, 7, 7)   0           ['activation_50[0][0]',          \n",
      "                                                                  'activation_53[0][0]',          \n",
      "                                                                  'max_pooling2d_2[0][0]',        \n",
      "                                                                  'activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 192, 7, 7)    411840      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 192, 7, 7)   576         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 224, 7, 7)    129248      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 224, 7, 7)   672         ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 224, 7, 7)    0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 192, 7, 7)    411840      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 256, 7, 7)    172288      ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 192, 7, 7)   576         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 256, 7, 7)   768         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 256, 7, 7)    0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 448, 7, 7)    0           ['activation_56[0][0]',          \n",
      "                                                                  'activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 2144, 7, 7)   962656      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 2144, 7, 7)   0           ['conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 2144, 7, 7)   0           ['concatenate_9[0][0]',          \n",
      "                                                                  'lambda_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 2144, 7, 7)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 192, 7, 7)    411840      ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 192, 7, 7)   576         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 224, 7, 7)    129248      ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 224, 7, 7)   672         ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 224, 7, 7)    0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 192, 7, 7)    411840      ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 256, 7, 7)    172288      ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 192, 7, 7)   576         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 256, 7, 7)   768         ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 256, 7, 7)    0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 448, 7, 7)    0           ['activation_61[0][0]',          \n",
      "                                                                  'activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 2144, 7, 7)   962656      ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 2144, 7, 7)   0           ['conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 2144, 7, 7)   0           ['activation_60[0][0]',          \n",
      "                                                                  'lambda_7[0][0]']               \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 2144, 7, 7)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2144)        0           ['activation_65[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2144)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2145        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,932,033\n",
      "Trainable params: 16,913,473\n",
      "Non-trainable params: 18,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e5173bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=hparams['learning_rate']), loss='mse', metrics='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e468ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'inception_resnet_2_64x64_small_v2'\n",
    "\n",
    "checkpoint_filepath = f'/data2/models/{model_name}/checkpoints/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
    "log_dir = os.path.join('/data2/logs/', model_name)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_freq='epoch',\n",
    "    save_best_only=True,\n",
    "    verbose=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "hparam_callback = hp.KerasCallback(log_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb6ef640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2772 - mse: 1.2772\n",
      "Epoch 1: loss improved from inf to 1.27720, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 47s 571ms/step - loss: 1.2772 - mse: 1.2772 - val_loss: 0.6316 - val_mse: 0.6316\n",
      "Epoch 2/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5321 - mse: 0.5321\n",
      "Epoch 2: loss improved from 1.27720 to 0.53206, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 20s 513ms/step - loss: 0.5321 - mse: 0.5321 - val_loss: 0.6322 - val_mse: 0.6322\n",
      "Epoch 3/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4391 - mse: 0.4391\n",
      "Epoch 3: loss improved from 0.53206 to 0.43911, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.4391 - mse: 0.4391 - val_loss: 0.3699 - val_mse: 0.3699\n",
      "Epoch 4/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4646 - mse: 0.4646\n",
      "Epoch 4: loss did not improve from 0.43911\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.4646 - mse: 0.4646 - val_loss: 0.3703 - val_mse: 0.3703\n",
      "Epoch 5/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5010 - mse: 0.5010\n",
      "Epoch 5: loss did not improve from 0.43911\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.5010 - mse: 0.5010 - val_loss: 0.3297 - val_mse: 0.3297\n",
      "Epoch 6/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3985 - mse: 0.3985\n",
      "Epoch 6: loss improved from 0.43911 to 0.39846, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.3985 - mse: 0.3985 - val_loss: 0.3256 - val_mse: 0.3256\n",
      "Epoch 7/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3475 - mse: 0.3475\n",
      "Epoch 7: loss improved from 0.39846 to 0.34753, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.3475 - mse: 0.3475 - val_loss: 0.3175 - val_mse: 0.3175\n",
      "Epoch 8/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3140 - mse: 0.3140\n",
      "Epoch 8: loss improved from 0.34753 to 0.31398, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.3140 - mse: 0.3140 - val_loss: 0.3144 - val_mse: 0.3144\n",
      "Epoch 9/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3039 - mse: 0.3039\n",
      "Epoch 9: loss improved from 0.31398 to 0.30391, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.3039 - mse: 0.3039 - val_loss: 0.3113 - val_mse: 0.3113\n",
      "Epoch 10/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2971 - mse: 0.2971\n",
      "Epoch 10: loss improved from 0.30391 to 0.29705, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.2971 - mse: 0.2971 - val_loss: 0.3033 - val_mse: 0.3033\n",
      "Epoch 11/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2738 - mse: 0.2738\n",
      "Epoch 11: loss improved from 0.29705 to 0.27380, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 525ms/step - loss: 0.2738 - mse: 0.2738 - val_loss: 0.2871 - val_mse: 0.2871\n",
      "Epoch 12/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2723 - mse: 0.2723\n",
      "Epoch 12: loss improved from 0.27380 to 0.27227, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.2723 - mse: 0.2723 - val_loss: 0.2784 - val_mse: 0.2784\n",
      "Epoch 13/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2406 - mse: 0.2406\n",
      "Epoch 13: loss improved from 0.27227 to 0.24064, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.2406 - mse: 0.2406 - val_loss: 0.2605 - val_mse: 0.2605\n",
      "Epoch 14/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2540 - mse: 0.2540\n",
      "Epoch 14: loss did not improve from 0.24064\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.2540 - mse: 0.2540 - val_loss: 0.2814 - val_mse: 0.2814\n",
      "Epoch 15/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2553 - mse: 0.2553\n",
      "Epoch 15: loss did not improve from 0.24064\n",
      "40/40 [==============================] - 19s 486ms/step - loss: 0.2553 - mse: 0.2553 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 16/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2221 - mse: 0.2221\n",
      "Epoch 16: loss improved from 0.24064 to 0.22212, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.2221 - mse: 0.2221 - val_loss: 0.2562 - val_mse: 0.2562\n",
      "Epoch 17/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2092 - mse: 0.2092\n",
      "Epoch 17: loss improved from 0.22212 to 0.20921, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.2092 - mse: 0.2092 - val_loss: 0.2054 - val_mse: 0.2054\n",
      "Epoch 18/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2184 - mse: 0.2184\n",
      "Epoch 18: loss did not improve from 0.20921\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.2184 - mse: 0.2184 - val_loss: 0.2951 - val_mse: 0.2951\n",
      "Epoch 19/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1966 - mse: 0.1966\n",
      "Epoch 19: loss improved from 0.20921 to 0.19665, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.1966 - mse: 0.1966 - val_loss: 0.2359 - val_mse: 0.2359\n",
      "Epoch 20/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1830 - mse: 0.1830\n",
      "Epoch 20: loss improved from 0.19665 to 0.18299, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.1830 - mse: 0.1830 - val_loss: 0.6167 - val_mse: 0.6167\n",
      "Epoch 21/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2126 - mse: 0.2126\n",
      "Epoch 21: loss did not improve from 0.18299\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.2126 - mse: 0.2126 - val_loss: 0.1607 - val_mse: 0.1607\n",
      "Epoch 22/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1862 - mse: 0.1862\n",
      "Epoch 22: loss did not improve from 0.18299\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.1862 - mse: 0.1862 - val_loss: 0.1612 - val_mse: 0.1612\n",
      "Epoch 23/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1660 - mse: 0.1660\n",
      "Epoch 23: loss improved from 0.18299 to 0.16603, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.1660 - mse: 0.1660 - val_loss: 0.2191 - val_mse: 0.2191\n",
      "Epoch 24/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1568 - mse: 0.1568\n",
      "Epoch 24: loss improved from 0.16603 to 0.15676, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.1568 - mse: 0.1568 - val_loss: 0.1950 - val_mse: 0.1950\n",
      "Epoch 25/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1555 - mse: 0.1555\n",
      "Epoch 25: loss improved from 0.15676 to 0.15549, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 525ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.0918 - val_mse: 0.0918\n",
      "Epoch 26/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1620 - mse: 0.1620\n",
      "Epoch 26: loss did not improve from 0.15549\n",
      "40/40 [==============================] - 19s 493ms/step - loss: 0.1620 - mse: 0.1620 - val_loss: 0.2753 - val_mse: 0.2753\n",
      "Epoch 27/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1505 - mse: 0.1505\n",
      "Epoch 27: loss improved from 0.15549 to 0.15052, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.1505 - mse: 0.1505 - val_loss: 0.0996 - val_mse: 0.0996\n",
      "Epoch 28/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1490 - mse: 0.1490\n",
      "Epoch 28: loss improved from 0.15052 to 0.14901, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.1490 - mse: 0.1490 - val_loss: 0.1124 - val_mse: 0.1124\n",
      "Epoch 29/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1409 - mse: 0.1409\n",
      "Epoch 29: loss improved from 0.14901 to 0.14087, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 539ms/step - loss: 0.1409 - mse: 0.1409 - val_loss: 0.5197 - val_mse: 0.5197\n",
      "Epoch 30/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1322 - mse: 0.1322\n",
      "Epoch 30: loss improved from 0.14087 to 0.13217, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 28s 705ms/step - loss: 0.1322 - mse: 0.1322 - val_loss: 0.1952 - val_mse: 0.1952\n",
      "Epoch 31/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1292 - mse: 0.1292\n",
      "Epoch 31: loss improved from 0.13217 to 0.12922, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 553ms/step - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1243 - val_mse: 0.1243\n",
      "Epoch 32/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1204 - mse: 0.1204\n",
      "Epoch 32: loss improved from 0.12922 to 0.12038, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 25s 629ms/step - loss: 0.1204 - mse: 0.1204 - val_loss: 0.1140 - val_mse: 0.1140\n",
      "Epoch 33/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1295 - mse: 0.1295\n",
      "Epoch 33: loss did not improve from 0.12038\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.1295 - mse: 0.1295 - val_loss: 0.4789 - val_mse: 0.4789\n",
      "Epoch 34/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1120 - mse: 0.1120\n",
      "Epoch 34: loss improved from 0.12038 to 0.11203, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.1120 - mse: 0.1120 - val_loss: 0.1816 - val_mse: 0.1816\n",
      "Epoch 35/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0971 - mse: 0.0971\n",
      "Epoch 35: loss improved from 0.11203 to 0.09706, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.1346 - val_mse: 0.1346\n",
      "Epoch 36/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0954 - mse: 0.0954\n",
      "Epoch 36: loss improved from 0.09706 to 0.09540, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0954 - mse: 0.0954 - val_loss: 0.4716 - val_mse: 0.4716\n",
      "Epoch 37/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0766 - mse: 0.0766\n",
      "Epoch 37: loss improved from 0.09540 to 0.07660, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0766 - mse: 0.0766 - val_loss: 0.1655 - val_mse: 0.1655\n",
      "Epoch 38/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0728 - mse: 0.0728\n",
      "Epoch 38: loss improved from 0.07660 to 0.07279, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0728 - mse: 0.0728 - val_loss: 0.3741 - val_mse: 0.3741\n",
      "Epoch 39/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0787 - mse: 0.0787\n",
      "Epoch 39: loss did not improve from 0.07279\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0787 - mse: 0.0787 - val_loss: 0.2359 - val_mse: 0.2359\n",
      "Epoch 40/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0619 - mse: 0.0619\n",
      "Epoch 40: loss improved from 0.07279 to 0.06189, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0619 - mse: 0.0619 - val_loss: 0.1636 - val_mse: 0.1636\n",
      "Epoch 41/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0623 - mse: 0.0623\n",
      "Epoch 41: loss did not improve from 0.06189\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.0623 - mse: 0.0623 - val_loss: 0.1633 - val_mse: 0.1633\n",
      "Epoch 42/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0628 - mse: 0.0628\n",
      "Epoch 42: loss did not improve from 0.06189\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0628 - mse: 0.0628 - val_loss: 0.1644 - val_mse: 0.1644\n",
      "Epoch 43/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0517 - mse: 0.0517\n",
      "Epoch 43: loss improved from 0.06189 to 0.05172, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0517 - mse: 0.0517 - val_loss: 0.1610 - val_mse: 0.1610\n",
      "Epoch 44/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0519 - mse: 0.0519\n",
      "Epoch 44: loss did not improve from 0.05172\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0519 - mse: 0.0519 - val_loss: 0.3988 - val_mse: 0.3988\n",
      "Epoch 45/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0442 - mse: 0.0442\n",
      "Epoch 45: loss improved from 0.05172 to 0.04422, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0442 - mse: 0.0442 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 46/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0470 - mse: 0.0470\n",
      "Epoch 46: loss did not improve from 0.04422\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0470 - mse: 0.0470 - val_loss: 0.1441 - val_mse: 0.1441\n",
      "Epoch 47/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0478 - mse: 0.0478\n",
      "Epoch 47: loss did not improve from 0.04422\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0478 - mse: 0.0478 - val_loss: 0.1445 - val_mse: 0.1445\n",
      "Epoch 48/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0548 - mse: 0.0548\n",
      "Epoch 48: loss did not improve from 0.04422\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0548 - mse: 0.0548 - val_loss: 0.1482 - val_mse: 0.1482\n",
      "Epoch 49/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0418 - mse: 0.0418\n",
      "Epoch 49: loss improved from 0.04422 to 0.04176, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.1746 - val_mse: 0.1746\n",
      "Epoch 50/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
      "Epoch 50: loss improved from 0.04176 to 0.03942, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.3659 - val_mse: 0.3659\n",
      "Epoch 51/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
      "Epoch 51: loss improved from 0.03942 to 0.03791, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.4527 - val_mse: 0.4527\n",
      "Epoch 52/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0347 - mse: 0.0347\n",
      "Epoch 52: loss improved from 0.03791 to 0.03468, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.6582 - val_mse: 0.6582\n",
      "Epoch 53/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
      "Epoch 53: loss did not improve from 0.03468\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.1324 - val_mse: 0.1324\n",
      "Epoch 54/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
      "Epoch 54: loss did not improve from 0.03468\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.2011 - val_mse: 0.2011\n",
      "Epoch 55/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
      "Epoch 55: loss did not improve from 0.03468\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.1659 - val_mse: 0.1659\n",
      "Epoch 56/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0343 - mse: 0.0343\n",
      "Epoch 56: loss improved from 0.03468 to 0.03426, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.3894 - val_mse: 0.3894\n",
      "Epoch 57/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0289 - mse: 0.0289\n",
      "Epoch 57: loss improved from 0.03426 to 0.02889, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.4719 - val_mse: 0.4719\n",
      "Epoch 58/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0278 - mse: 0.0278\n",
      "Epoch 58: loss improved from 0.02889 to 0.02784, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0935 - val_mse: 0.0935\n",
      "Epoch 59/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0264 - mse: 0.0264\n",
      "Epoch 59: loss improved from 0.02784 to 0.02640, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.1035 - val_mse: 0.1035\n",
      "Epoch 60/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0269 - mse: 0.0269\n",
      "Epoch 60: loss did not improve from 0.02640\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0974 - val_mse: 0.0974\n",
      "Epoch 61/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0251 - mse: 0.0251\n",
      "Epoch 61: loss improved from 0.02640 to 0.02509, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1003 - val_mse: 0.1003\n",
      "Epoch 62/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0238 - mse: 0.0238\n",
      "Epoch 62: loss improved from 0.02509 to 0.02378, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 30s 735ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.1250 - val_mse: 0.1250\n",
      "Epoch 63/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0253 - mse: 0.0253\n",
      "Epoch 63: loss did not improve from 0.02378\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.1340 - val_mse: 0.1340\n",
      "Epoch 64/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0216 - mse: 0.0216\n",
      "Epoch 64: loss improved from 0.02378 to 0.02158, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.1977 - val_mse: 0.1977\n",
      "Epoch 65/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0203 - mse: 0.0203\n",
      "Epoch 65: loss improved from 0.02158 to 0.02034, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.2202 - val_mse: 0.2202\n",
      "Epoch 66/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0231 - mse: 0.0231\n",
      "Epoch 66: loss did not improve from 0.02034\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.4174 - val_mse: 0.4174\n",
      "Epoch 67/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0234 - mse: 0.0234\n",
      "Epoch 67: loss did not improve from 0.02034\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0946 - val_mse: 0.0946\n",
      "Epoch 68/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0185 - mse: 0.0185\n",
      "Epoch 68: loss improved from 0.02034 to 0.01850, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.1136 - val_mse: 0.1136\n",
      "Epoch 69/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0191 - mse: 0.0191\n",
      "Epoch 69: loss did not improve from 0.01850\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.1270 - val_mse: 0.1270\n",
      "Epoch 70/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0204 - mse: 0.0204\n",
      "Epoch 70: loss did not improve from 0.01850\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.1136 - val_mse: 0.1136\n",
      "Epoch 71/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0180 - mse: 0.0180\n",
      "Epoch 71: loss improved from 0.01850 to 0.01796, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 523ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 72/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 72: loss improved from 0.01796 to 0.01753, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 28s 696ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.2680 - val_mse: 0.2680\n",
      "Epoch 73/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0183 - mse: 0.0183\n",
      "Epoch 73: loss did not improve from 0.01753\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.3483 - val_mse: 0.3483\n",
      "Epoch 74/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0156 - mse: 0.0156\n",
      "Epoch 74: loss improved from 0.01753 to 0.01565, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.1256 - val_mse: 0.1256\n",
      "Epoch 75/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0170 - mse: 0.0170\n",
      "Epoch 75: loss did not improve from 0.01565\n",
      "40/40 [==============================] - 19s 480ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.1132 - val_mse: 0.1132\n",
      "Epoch 76/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0154 - mse: 0.0154\n",
      "Epoch 76: loss improved from 0.01565 to 0.01535, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0866 - val_mse: 0.0866\n",
      "Epoch 77/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0140 - mse: 0.0140\n",
      "Epoch 77: loss improved from 0.01535 to 0.01401, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0904 - val_mse: 0.0904\n",
      "Epoch 78/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0138 - mse: 0.0138\n",
      "Epoch 78: loss improved from 0.01401 to 0.01382, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.1451 - val_mse: 0.1451\n",
      "Epoch 79/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0151 - mse: 0.0151\n",
      "Epoch 79: loss did not improve from 0.01382\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0945 - val_mse: 0.0945\n",
      "Epoch 80/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0140 - mse: 0.0140\n",
      "Epoch 80: loss did not improve from 0.01382\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.1711 - val_mse: 0.1711\n",
      "Epoch 81/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0132 - mse: 0.0132\n",
      "Epoch 81: loss improved from 0.01382 to 0.01318, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.1156 - val_mse: 0.1156\n",
      "Epoch 82/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0150 - mse: 0.0150\n",
      "Epoch 82: loss did not improve from 0.01318\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.1186 - val_mse: 0.1186\n",
      "Epoch 83/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0123 - mse: 0.0123\n",
      "Epoch 83: loss improved from 0.01318 to 0.01227, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0844 - val_mse: 0.0844\n",
      "Epoch 84/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0117 - mse: 0.0117\n",
      "Epoch 84: loss improved from 0.01227 to 0.01166, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 85/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0117 - mse: 0.0117\n",
      "Epoch 85: loss did not improve from 0.01166\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.1073 - val_mse: 0.1073\n",
      "Epoch 86/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0116 - mse: 0.0116\n",
      "Epoch 86: loss improved from 0.01166 to 0.01163, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.1754 - val_mse: 0.1754\n",
      "Epoch 87/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0120 - mse: 0.0120\n",
      "Epoch 87: loss did not improve from 0.01163\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.1451 - val_mse: 0.1451\n",
      "Epoch 88/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0121 - mse: 0.0121\n",
      "Epoch 88: loss did not improve from 0.01163\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.1467 - val_mse: 0.1467\n",
      "Epoch 89/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0112 - mse: 0.0112\n",
      "Epoch 89: loss improved from 0.01163 to 0.01122, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 20s 513ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.1255 - val_mse: 0.1255\n",
      "Epoch 90/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0119 - mse: 0.0119\n",
      "Epoch 90: loss did not improve from 0.01122\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.2262 - val_mse: 0.2262\n",
      "Epoch 91/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0098 - mse: 0.0098\n",
      "Epoch 91: loss improved from 0.01122 to 0.00979, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.1829 - val_mse: 0.1829\n",
      "Epoch 92/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0103 - mse: 0.0103\n",
      "Epoch 92: loss did not improve from 0.00979\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.1037 - val_mse: 0.1037\n",
      "Epoch 93/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0093 - mse: 0.0093\n",
      "Epoch 93: loss improved from 0.00979 to 0.00931, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.1757 - val_mse: 0.1757\n",
      "Epoch 94/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0092 - mse: 0.0092\n",
      "Epoch 94: loss improved from 0.00931 to 0.00916, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.1194 - val_mse: 0.1194\n",
      "Epoch 95/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0082 - mse: 0.0082\n",
      "Epoch 95: loss improved from 0.00916 to 0.00824, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 515ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.1235 - val_mse: 0.1235\n",
      "Epoch 96/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0081 - mse: 0.0081\n",
      "Epoch 96: loss improved from 0.00824 to 0.00805, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.1312 - val_mse: 0.1312\n",
      "Epoch 97/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0088 - mse: 0.0088\n",
      "Epoch 97: loss did not improve from 0.00805\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.1370 - val_mse: 0.1370\n",
      "Epoch 98/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0084 - mse: 0.0084\n",
      "Epoch 98: loss did not improve from 0.00805\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.1343 - val_mse: 0.1343\n",
      "Epoch 99/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0079 - mse: 0.0079\n",
      "Epoch 99: loss improved from 0.00805 to 0.00791, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 100/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0076 - mse: 0.0076\n",
      "Epoch 100: loss improved from 0.00791 to 0.00757, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "Epoch 101/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0082 - mse: 0.0082\n",
      "Epoch 101: loss did not improve from 0.00757\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.1342 - val_mse: 0.1342\n",
      "Epoch 102/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0074 - mse: 0.0074\n",
      "Epoch 102: loss improved from 0.00757 to 0.00744, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.2360 - val_mse: 0.2360\n",
      "Epoch 103/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0077 - mse: 0.0077\n",
      "Epoch 103: loss did not improve from 0.00744\n",
      "40/40 [==============================] - 20s 492ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.1553 - val_mse: 0.1553\n",
      "Epoch 104/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0073 - mse: 0.0073\n",
      "Epoch 104: loss improved from 0.00744 to 0.00726, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.1851 - val_mse: 0.1851\n",
      "Epoch 105/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0076 - mse: 0.0076\n",
      "Epoch 105: loss did not improve from 0.00726\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.2710 - val_mse: 0.2710\n",
      "Epoch 106/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0096 - mse: 0.0096\n",
      "Epoch 106: loss did not improve from 0.00726\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.1551 - val_mse: 0.1551\n",
      "Epoch 107/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0069 - mse: 0.0069\n",
      "Epoch 107: loss improved from 0.00726 to 0.00692, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.1680 - val_mse: 0.1680\n",
      "Epoch 108/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0063 - mse: 0.0063\n",
      "Epoch 108: loss improved from 0.00692 to 0.00626, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.1653 - val_mse: 0.1653\n",
      "Epoch 109/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0069 - mse: 0.0069\n",
      "Epoch 109: loss did not improve from 0.00626\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.1593 - val_mse: 0.1593\n",
      "Epoch 110/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0065 - mse: 0.0065\n",
      "Epoch 110: loss did not improve from 0.00626\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.1463 - val_mse: 0.1463\n",
      "Epoch 111/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0067 - mse: 0.0067\n",
      "Epoch 111: loss did not improve from 0.00626\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.1870 - val_mse: 0.1870\n",
      "Epoch 112/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0066 - mse: 0.0066\n",
      "Epoch 112: loss did not improve from 0.00626\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.4526 - val_mse: 0.4526\n",
      "Epoch 113/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0058 - mse: 0.0058\n",
      "Epoch 113: loss improved from 0.00626 to 0.00577, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.1353 - val_mse: 0.1353\n",
      "Epoch 114/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0064 - mse: 0.0064\n",
      "Epoch 114: loss did not improve from 0.00577\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.1121 - val_mse: 0.1121\n",
      "Epoch 115/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0069 - mse: 0.0069\n",
      "Epoch 115: loss did not improve from 0.00577\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.2136 - val_mse: 0.2136\n",
      "Epoch 116/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0066 - mse: 0.0066\n",
      "Epoch 116: loss did not improve from 0.00577\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.1618 - val_mse: 0.1618\n",
      "Epoch 117/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0067 - mse: 0.0067\n",
      "Epoch 117: loss did not improve from 0.00577\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.1682 - val_mse: 0.1682\n",
      "Epoch 118/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0055 - mse: 0.0055\n",
      "Epoch 118: loss improved from 0.00577 to 0.00550, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 524ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.1949 - val_mse: 0.1949\n",
      "Epoch 119/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0071 - mse: 0.0071\n",
      "Epoch 119: loss did not improve from 0.00550\n",
      "40/40 [==============================] - 28s 718ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.1377 - val_mse: 0.1377\n",
      "Epoch 120/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0067 - mse: 0.0067\n",
      "Epoch 120: loss did not improve from 0.00550\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.1023 - val_mse: 0.1023\n",
      "Epoch 121/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0065 - mse: 0.0065\n",
      "Epoch 121: loss did not improve from 0.00550\n",
      "40/40 [==============================] - 19s 486ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.1037 - val_mse: 0.1037\n",
      "Epoch 122/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0065 - mse: 0.0065\n",
      "Epoch 122: loss did not improve from 0.00550\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.1424 - val_mse: 0.1424\n",
      "Epoch 123/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0066 - mse: 0.0066\n",
      "Epoch 123: loss did not improve from 0.00550\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0950 - val_mse: 0.0950\n",
      "Epoch 124/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0087 - mse: 0.0087\n",
      "Epoch 124: loss did not improve from 0.00550\n",
      "40/40 [==============================] - 19s 486ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.1311 - val_mse: 0.1311\n",
      "Epoch 125/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0068 - mse: 0.0068\n",
      "Epoch 125: loss did not improve from 0.00550\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0936 - val_mse: 0.0936\n",
      "Epoch 126/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0063 - mse: 0.0063\n",
      "Epoch 126: loss did not improve from 0.00550\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.1204 - val_mse: 0.1204\n",
      "Epoch 127/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0068 - mse: 0.0068\n",
      "Epoch 127: loss did not improve from 0.00550\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.1839 - val_mse: 0.1839\n",
      "Epoch 128/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0054 - mse: 0.0054\n",
      "Epoch 128: loss improved from 0.00550 to 0.00541, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 28s 717ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 129/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0057 - mse: 0.0057\n",
      "Epoch 129: loss did not improve from 0.00541\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 130/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0054 - mse: 0.0054\n",
      "Epoch 130: loss did not improve from 0.00541\n",
      "40/40 [==============================] - 19s 486ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.1720 - val_mse: 0.1720\n",
      "Epoch 131/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0051 - mse: 0.0051\n",
      "Epoch 131: loss improved from 0.00541 to 0.00509, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.1572 - val_mse: 0.1572\n",
      "Epoch 132/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0044 - mse: 0.0044\n",
      "Epoch 132: loss improved from 0.00509 to 0.00445, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.2171 - val_mse: 0.2171\n",
      "Epoch 133/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 133: loss did not improve from 0.00445\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.2409 - val_mse: 0.2409\n",
      "Epoch 134/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0043 - mse: 0.0043\n",
      "Epoch 134: loss improved from 0.00445 to 0.00432, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.2900 - val_mse: 0.2900\n",
      "Epoch 135/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0043 - mse: 0.0043\n",
      "Epoch 135: loss improved from 0.00432 to 0.00427, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 25s 631ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.2546 - val_mse: 0.2546\n",
      "Epoch 136/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0039 - mse: 0.0039\n",
      "Epoch 136: loss improved from 0.00427 to 0.00390, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 540ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.1685 - val_mse: 0.1685\n",
      "Epoch 137/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0037 - mse: 0.0037\n",
      "Epoch 137: loss improved from 0.00390 to 0.00368, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.1734 - val_mse: 0.1734\n",
      "Epoch 138/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0036 - mse: 0.0036\n",
      "Epoch 138: loss improved from 0.00368 to 0.00361, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.1116 - val_mse: 0.1116\n",
      "Epoch 139/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 139: loss improved from 0.00361 to 0.00322, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.1342 - val_mse: 0.1342\n",
      "Epoch 140/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0038 - mse: 0.0038\n",
      "Epoch 140: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.1065 - val_mse: 0.1065\n",
      "Epoch 141/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0040 - mse: 0.0040\n",
      "Epoch 141: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 26s 651ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.1194 - val_mse: 0.1194\n",
      "Epoch 142/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0040 - mse: 0.0040\n",
      "Epoch 142: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 20s 483ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.1180 - val_mse: 0.1180\n",
      "Epoch 143/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0042 - mse: 0.0042\n",
      "Epoch 143: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.1209 - val_mse: 0.1209\n",
      "Epoch 144/300\n",
      "39/40 [============================>.] - ETA: 0s - loss: 0.0044 - mse: 0.0044\n",
      "Epoch 144: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
      "Epoch 145/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0040 - mse: 0.0040\n",
      "Epoch 145: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "Epoch 146/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0039 - mse: 0.0039\n",
      "Epoch 146: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.1291 - val_mse: 0.1291\n",
      "Epoch 147/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0042 - mse: 0.0042\n",
      "Epoch 147: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0995 - val_mse: 0.0995\n",
      "Epoch 148/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0036 - mse: 0.0036\n",
      "Epoch 148: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0929 - val_mse: 0.0929\n",
      "Epoch 149/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0035 - mse: 0.0035\n",
      "Epoch 149: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.1183 - val_mse: 0.1183\n",
      "Epoch 150/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0036 - mse: 0.0036\n",
      "Epoch 150: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.1076 - val_mse: 0.1076\n",
      "Epoch 151/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0038 - mse: 0.0038\n",
      "Epoch 151: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0826 - val_mse: 0.0826\n",
      "Epoch 152/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0045 - mse: 0.0045\n",
      "Epoch 152: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.1011 - val_mse: 0.1011\n",
      "Epoch 153/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 153: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.1178 - val_mse: 0.1178\n",
      "Epoch 154/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0051 - mse: 0.0051\n",
      "Epoch 154: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.1312 - val_mse: 0.1312\n",
      "Epoch 155/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0040 - mse: 0.0040\n",
      "Epoch 155: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.3801 - val_mse: 0.3801\n",
      "Epoch 156/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0042 - mse: 0.0042\n",
      "Epoch 156: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.1554 - val_mse: 0.1554\n",
      "Epoch 157/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0044 - mse: 0.0044\n",
      "Epoch 157: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 158/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0043 - mse: 0.0043\n",
      "Epoch 158: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 159/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0047 - mse: 0.0047\n",
      "Epoch 159: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 160/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0066 - mse: 0.0066\n",
      "Epoch 160: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 486ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.1351 - val_mse: 0.1351\n",
      "Epoch 161/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0057 - mse: 0.0057\n",
      "Epoch 161: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 492ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.1171 - val_mse: 0.1171\n",
      "Epoch 162/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0039 - mse: 0.0039\n",
      "Epoch 162: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.1277 - val_mse: 0.1277\n",
      "Epoch 163/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0035 - mse: 0.0035\n",
      "Epoch 163: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0795 - val_mse: 0.0795\n",
      "Epoch 164/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0035 - mse: 0.0035\n",
      "Epoch 164: loss did not improve from 0.00322\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.1247 - val_mse: 0.1247\n",
      "Epoch 165/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0030 - mse: 0.0030\n",
      "Epoch 165: loss improved from 0.00322 to 0.00296, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 30s 754ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1342 - val_mse: 0.1342\n",
      "Epoch 166/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0029 - mse: 0.0029\n",
      "Epoch 166: loss improved from 0.00296 to 0.00289, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 671ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.1105 - val_mse: 0.1105\n",
      "Epoch 167/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0029 - mse: 0.0029\n",
      "Epoch 167: loss improved from 0.00289 to 0.00289, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.1069 - val_mse: 0.1069\n",
      "Epoch 168/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0026 - mse: 0.0026\n",
      "Epoch 168: loss improved from 0.00289 to 0.00258, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 523ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0917 - val_mse: 0.0917\n",
      "Epoch 169/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0029 - mse: 0.0029\n",
      "Epoch 169: loss did not improve from 0.00258\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.1086 - val_mse: 0.1086\n",
      "Epoch 170/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
      "Epoch 170: loss improved from 0.00258 to 0.00250, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1075 - val_mse: 0.1075\n",
      "Epoch 171/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0028 - mse: 0.0028\n",
      "Epoch 171: loss did not improve from 0.00250\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0967 - val_mse: 0.0967\n",
      "Epoch 172/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0028 - mse: 0.0028\n",
      "Epoch 172: loss did not improve from 0.00250\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.1501 - val_mse: 0.1501\n",
      "Epoch 173/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
      "Epoch 173: loss improved from 0.00250 to 0.00247, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0933 - val_mse: 0.0933\n",
      "Epoch 174/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0024 - mse: 0.0024\n",
      "Epoch 174: loss improved from 0.00247 to 0.00240, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.1126 - val_mse: 0.1126\n",
      "Epoch 175/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 175: loss improved from 0.00240 to 0.00235, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 21s 520ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1297 - val_mse: 0.1297\n",
      "Epoch 176/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0028 - mse: 0.0028\n",
      "Epoch 176: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.1290 - val_mse: 0.1290\n",
      "Epoch 177/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 177: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.1202 - val_mse: 0.1202\n",
      "Epoch 178/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0027 - mse: 0.0027\n",
      "Epoch 178: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.1388 - val_mse: 0.1388\n",
      "Epoch 179/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0028 - mse: 0.0028\n",
      "Epoch 179: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.1121 - val_mse: 0.1121\n",
      "Epoch 180/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0030 - mse: 0.0030\n",
      "Epoch 180: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1212 - val_mse: 0.1212\n",
      "Epoch 181/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0029 - mse: 0.0029\n",
      "Epoch 181: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.1362 - val_mse: 0.1362\n",
      "Epoch 182/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 182: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0978 - val_mse: 0.0978\n",
      "Epoch 183/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0026 - mse: 0.0026\n",
      "Epoch 183: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.1054 - val_mse: 0.1054\n",
      "Epoch 184/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0029 - mse: 0.0029\n",
      "Epoch 184: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.1123 - val_mse: 0.1123\n",
      "Epoch 185/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 185: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0890 - val_mse: 0.0890\n",
      "Epoch 186/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0030 - mse: 0.0030\n",
      "Epoch 186: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1225 - val_mse: 0.1225\n",
      "Epoch 187/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0043 - mse: 0.0043\n",
      "Epoch 187: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0935 - val_mse: 0.0935\n",
      "Epoch 188/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0043 - mse: 0.0043\n",
      "Epoch 188: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.1972 - val_mse: 0.1972\n",
      "Epoch 189/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0037 - mse: 0.0037\n",
      "Epoch 189: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.3455 - val_mse: 0.3455\n",
      "Epoch 190/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 190: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.2666 - val_mse: 0.2666\n",
      "Epoch 191/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 191: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.1167 - val_mse: 0.1167\n",
      "Epoch 192/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 192: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 193/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0026 - mse: 0.0026\n",
      "Epoch 193: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.1379 - val_mse: 0.1379\n",
      "Epoch 194/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 194: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.1380 - val_mse: 0.1380\n",
      "Epoch 195/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 195: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.1468 - val_mse: 0.1468\n",
      "Epoch 196/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
      "Epoch 196: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 197/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
      "Epoch 197: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1208 - val_mse: 0.1208\n",
      "Epoch 198/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0024 - mse: 0.0024\n",
      "Epoch 198: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 36s 903ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0938 - val_mse: 0.0938\n",
      "Epoch 199/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
      "Epoch 199: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 39s 975ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1207 - val_mse: 0.1207\n",
      "Epoch 200/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 200: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.1582 - val_mse: 0.1582\n",
      "Epoch 201/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0027 - mse: 0.0027\n",
      "Epoch 201: loss did not improve from 0.00235\n",
      "40/40 [==============================] - 39s 975ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.1236 - val_mse: 0.1236\n",
      "Epoch 202/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0022 - mse: 0.0022\n",
      "Epoch 202: loss improved from 0.00235 to 0.00218, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.1284 - val_mse: 0.1284\n",
      "Epoch 203/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0020 - mse: 0.0020\n",
      "Epoch 203: loss improved from 0.00218 to 0.00204, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 204/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0024 - mse: 0.0024\n",
      "Epoch 204: loss did not improve from 0.00204\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.1066 - val_mse: 0.1066\n",
      "Epoch 205/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0019 - mse: 0.0019\n",
      "Epoch 205: loss improved from 0.00204 to 0.00195, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 206/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0020 - mse: 0.0020\n",
      "Epoch 206: loss did not improve from 0.00195\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1119 - val_mse: 0.1119\n",
      "Epoch 207/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 207: loss improved from 0.00195 to 0.00181, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.1060 - val_mse: 0.1060\n",
      "Epoch 208/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0021 - mse: 0.0021\n",
      "Epoch 208: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0915 - val_mse: 0.0915\n",
      "Epoch 209/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0027 - mse: 0.0027\n",
      "Epoch 209: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 38s 953ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.1066 - val_mse: 0.1066\n",
      "Epoch 210/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 210: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 38s 962ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 211/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 211: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.1363 - val_mse: 0.1363\n",
      "Epoch 212/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0034 - mse: 0.0034\n",
      "Epoch 212: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.1382 - val_mse: 0.1382\n",
      "Epoch 213/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0040 - mse: 0.0040\n",
      "Epoch 213: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.1143 - val_mse: 0.1143\n",
      "Epoch 214/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0036 - mse: 0.0036\n",
      "Epoch 214: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.1025 - val_mse: 0.1025\n",
      "Epoch 215/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0029 - mse: 0.0029\n",
      "Epoch 215: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0985 - val_mse: 0.0985\n",
      "Epoch 216/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0022 - mse: 0.0022\n",
      "Epoch 216: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0804 - val_mse: 0.0804\n",
      "Epoch 217/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 217: loss did not improve from 0.00181\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1189 - val_mse: 0.1189\n",
      "Epoch 218/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 218: loss improved from 0.00181 to 0.00175, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0989 - val_mse: 0.0989\n",
      "Epoch 219/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 219: loss improved from 0.00175 to 0.00151, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0857 - val_mse: 0.0857\n",
      "Epoch 220/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 220: loss improved from 0.00151 to 0.00143, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0883 - val_mse: 0.0883\n",
      "Epoch 221/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 221: loss improved from 0.00143 to 0.00136, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 40s 1s/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.2050 - val_mse: 0.2050\n",
      "Epoch 222/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 222: loss improved from 0.00136 to 0.00118, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.1381 - val_mse: 0.1381\n",
      "Epoch 223/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 223: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.1324 - val_mse: 0.1324\n",
      "Epoch 224/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 224: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.1131 - val_mse: 0.1131\n",
      "Epoch 225/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0022 - mse: 0.0022\n",
      "Epoch 225: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.1117 - val_mse: 0.1117\n",
      "Epoch 226/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0021 - mse: 0.0021\n",
      "Epoch 226: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.1159 - val_mse: 0.1159\n",
      "Epoch 227/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0026 - mse: 0.0026\n",
      "Epoch 227: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0993 - val_mse: 0.0993\n",
      "Epoch 228/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0022 - mse: 0.0022\n",
      "Epoch 228: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.1121 - val_mse: 0.1121\n",
      "Epoch 229/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0028 - mse: 0.0028\n",
      "Epoch 229: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.1009 - val_mse: 0.1009\n",
      "Epoch 230/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 230: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1165 - val_mse: 0.1165\n",
      "Epoch 231/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
      "Epoch 231: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1071 - val_mse: 0.1071\n",
      "Epoch 232/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0026 - mse: 0.0026\n",
      "Epoch 232: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.1177 - val_mse: 0.1177\n",
      "Epoch 233/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0034 - mse: 0.0034\n",
      "Epoch 233: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.1431 - val_mse: 0.1431\n",
      "Epoch 234/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0029 - mse: 0.0029\n",
      "Epoch 234: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 38s 952ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.1233 - val_mse: 0.1233\n",
      "Epoch 235/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
      "Epoch 235: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1125 - val_mse: 0.1125\n",
      "Epoch 236/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0021 - mse: 0.0021\n",
      "Epoch 236: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.1039 - val_mse: 0.1039\n",
      "Epoch 237/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0022 - mse: 0.0022\n",
      "Epoch 237: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 967ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.1020 - val_mse: 0.1020\n",
      "Epoch 238/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0024 - mse: 0.0024\n",
      "Epoch 238: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.1311 - val_mse: 0.1311\n",
      "Epoch 239/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 239: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 973ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1381 - val_mse: 0.1381\n",
      "Epoch 240/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 240: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.1336 - val_mse: 0.1336\n",
      "Epoch 241/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 241: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.2383 - val_mse: 0.2383\n",
      "Epoch 242/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0024 - mse: 0.0024\n",
      "Epoch 242: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 973ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.2219 - val_mse: 0.2219\n",
      "Epoch 243/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0020 - mse: 0.0020\n",
      "Epoch 243: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 244/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 244: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.2333 - val_mse: 0.2333\n",
      "Epoch 245/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0016 - mse: 0.0016\n",
      "Epoch 245: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.1742 - val_mse: 0.1742\n",
      "Epoch 246/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0019 - mse: 0.0019\n",
      "Epoch 246: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 967ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.1710 - val_mse: 0.1710\n",
      "Epoch 247/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0016 - mse: 0.0016\n",
      "Epoch 247: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 38s 953ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.1330 - val_mse: 0.1330\n",
      "Epoch 248/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0019 - mse: 0.0019\n",
      "Epoch 248: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.1149 - val_mse: 0.1149\n",
      "Epoch 249/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0016 - mse: 0.0016\n",
      "Epoch 249: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 974ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0999 - val_mse: 0.0999\n",
      "Epoch 250/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 250: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 973ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 251/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0019 - mse: 0.0019\n",
      "Epoch 251: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0990 - val_mse: 0.0990\n",
      "Epoch 252/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0016 - mse: 0.0016\n",
      "Epoch 252: loss did not improve from 0.00118\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.1513 - val_mse: 0.1513\n",
      "Epoch 253/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 253: loss improved from 0.00118 to 0.00117, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.1189 - val_mse: 0.1189\n",
      "Epoch 254/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 254: loss did not improve from 0.00117\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0987 - val_mse: 0.0987\n",
      "Epoch 255/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 255: loss did not improve from 0.00117\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.1087 - val_mse: 0.1087\n",
      "Epoch 256/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 256: loss did not improve from 0.00117\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 257/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 257: loss improved from 0.00117 to 0.00109, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0936 - val_mse: 0.0936\n",
      "Epoch 258/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 258: loss did not improve from 0.00109\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0843 - val_mse: 0.0843\n",
      "Epoch 259/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 259: loss improved from 0.00109 to 0.00104, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 260/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 260: loss did not improve from 0.00104\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.1082 - val_mse: 0.1082\n",
      "Epoch 261/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 261: loss did not improve from 0.00104\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.1050 - val_mse: 0.1050\n",
      "Epoch 262/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 262: loss did not improve from 0.00104\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.1011 - val_mse: 0.1011\n",
      "Epoch 263/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 263: loss did not improve from 0.00104\n",
      "40/40 [==============================] - 39s 973ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.1077 - val_mse: 0.1077\n",
      "Epoch 264/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 9.6104e-04 - mse: 9.6104e-04\n",
      "Epoch 264: loss improved from 0.00104 to 0.00096, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 9.6104e-04 - mse: 9.6104e-04 - val_loss: 0.0992 - val_mse: 0.0992\n",
      "Epoch 265/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 9.3145e-04 - mse: 9.3145e-04\n",
      "Epoch 265: loss improved from 0.00096 to 0.00093, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 9.3145e-04 - mse: 9.3145e-04 - val_loss: 0.1211 - val_mse: 0.1211\n",
      "Epoch 266/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 266: loss did not improve from 0.00093\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.1103 - val_mse: 0.1103\n",
      "Epoch 267/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 9.8245e-04 - mse: 9.8245e-04\n",
      "Epoch 267: loss did not improve from 0.00093\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 9.8245e-04 - mse: 9.8245e-04 - val_loss: 0.0890 - val_mse: 0.0890\n",
      "Epoch 268/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 8.5716e-04 - mse: 8.5716e-04\n",
      "Epoch 268: loss improved from 0.00093 to 0.00086, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 42s 1s/step - loss: 8.5716e-04 - mse: 8.5716e-04 - val_loss: 0.0964 - val_mse: 0.0964\n",
      "Epoch 269/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 8.8938e-04 - mse: 8.8938e-04\n",
      "Epoch 269: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 8.8938e-04 - mse: 8.8938e-04 - val_loss: 0.1381 - val_mse: 0.1381\n",
      "Epoch 270/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 9.0925e-04 - mse: 9.0925e-04\n",
      "Epoch 270: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 9.0925e-04 - mse: 9.0925e-04 - val_loss: 0.0928 - val_mse: 0.0928\n",
      "Epoch 271/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 271: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 38s 951ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0918 - val_mse: 0.0918\n",
      "Epoch 272/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 272: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0903 - val_mse: 0.0903\n",
      "Epoch 273/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 273: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.1016 - val_mse: 0.1016\n",
      "Epoch 274/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 274: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.1258 - val_mse: 0.1258\n",
      "Epoch 275/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 275: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.1823 - val_mse: 0.1823\n",
      "Epoch 276/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0024 - mse: 0.0024\n",
      "Epoch 276: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 990ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.2018 - val_mse: 0.2018\n",
      "Epoch 277/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0035 - mse: 0.0035\n",
      "Epoch 277: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.1746 - val_mse: 0.1746\n",
      "Epoch 278/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0063 - mse: 0.0063\n",
      "Epoch 278: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 973ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.2618 - val_mse: 0.2618\n",
      "Epoch 279/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0443 - mse: 0.0443\n",
      "Epoch 279: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0443 - mse: 0.0443 - val_loss: 0.4452 - val_mse: 0.4452\n",
      "Epoch 280/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0665 - mse: 0.0665\n",
      "Epoch 280: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0665 - mse: 0.0665 - val_loss: 0.3112 - val_mse: 0.3112\n",
      "Epoch 281/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0309 - mse: 0.0309\n",
      "Epoch 281: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 973ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.5804 - val_mse: 0.5804\n",
      "Epoch 282/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0169 - mse: 0.0169\n",
      "Epoch 282: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.2369 - val_mse: 0.2369\n",
      "Epoch 283/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0097 - mse: 0.0097\n",
      "Epoch 283: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.2004 - val_mse: 0.2004\n",
      "Epoch 284/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0064 - mse: 0.0064\n",
      "Epoch 284: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 38s 950ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.1195 - val_mse: 0.1195\n",
      "Epoch 285/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0039 - mse: 0.0039\n",
      "Epoch 285: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.1279 - val_mse: 0.1279\n",
      "Epoch 286/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0026 - mse: 0.0026\n",
      "Epoch 286: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.1358 - val_mse: 0.1358\n",
      "Epoch 287/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0023 - mse: 0.0023\n",
      "Epoch 287: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 966ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0828 - val_mse: 0.0828\n",
      "Epoch 288/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 288: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 973ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0854 - val_mse: 0.0854\n",
      "Epoch 289/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 289: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 966ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0919 - val_mse: 0.0919\n",
      "Epoch 290/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 290: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0993 - val_mse: 0.0993\n",
      "Epoch 291/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 291: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0898 - val_mse: 0.0898\n",
      "Epoch 292/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 292: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 293/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 293: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0987 - val_mse: 0.0987\n",
      "Epoch 294/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 294: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.1179 - val_mse: 0.1179\n",
      "Epoch 295/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 295: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 974ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.1134 - val_mse: 0.1134\n",
      "Epoch 296/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 9.9854e-04 - mse: 9.9854e-04\n",
      "Epoch 296: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 38s 956ms/step - loss: 9.9854e-04 - mse: 9.9854e-04 - val_loss: 0.1104 - val_mse: 0.1104\n",
      "Epoch 297/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 9.2337e-04 - mse: 9.2337e-04\n",
      "Epoch 297: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 967ms/step - loss: 9.2337e-04 - mse: 9.2337e-04 - val_loss: 0.1036 - val_mse: 0.1036\n",
      "Epoch 298/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 9.0297e-04 - mse: 9.0297e-04\n",
      "Epoch 298: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 9.0297e-04 - mse: 9.0297e-04 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 299/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 9.2748e-04 - mse: 9.2748e-04\n",
      "Epoch 299: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 9.2748e-04 - mse: 9.2748e-04 - val_loss: 0.0856 - val_mse: 0.0856\n",
      "Epoch 300/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 8.6192e-04 - mse: 8.6192e-04\n",
      "Epoch 300: loss did not improve from 0.00086\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 8.6192e-04 - mse: 8.6192e-04 - val_loss: 0.0767 - val_mse: 0.0767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4eb832e9e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_gen, batch_size=hparams['batch_size'], epochs=hparams['num_epochs'], shuffle=True, verbose=1, validation_data=val_gen, callbacks=[tensorboard_callback, model_checkpoint_callback, hparam_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec5e631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f4f409ab790>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_filepath) # model might be overfitting. train mse is lowering but val mse fluctuates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f37d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 289ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92956d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(TEST_PATH, 'r') as file:\n",
    "    y_test = np.asarray(file['specz_redshift'][:])\n",
    "    oid_test = np.asarray(file['object_id'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8cf4741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(np.ravel(pred), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e952dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_point_metrics(pd.Series(np.ravel(pred)), pd.Series(y_test), binned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63a46536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zspec_bin</th>\n",
       "      <th>count</th>\n",
       "      <th>L</th>\n",
       "      <th>bias_bw</th>\n",
       "      <th>bias_conv</th>\n",
       "      <th>scatter_bw</th>\n",
       "      <th>scatter_conv</th>\n",
       "      <th>outlier_bw</th>\n",
       "      <th>outlier_conv</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 4.0]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>0.082496</td>\n",
       "      <td>0.083326</td>\n",
       "      <td>0.088795</td>\n",
       "      <td>0.071742</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.099162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    zspec_bin  count         L   bias_bw  bias_conv  scatter_bw  scatter_conv  \\\n",
       "0  (0.0, 4.0]   2000  0.329672  0.082496   0.083326    0.088795      0.071742   \n",
       "\n",
       "   outlier_bw  outlier_conv       mse  \n",
       "0       0.146          0.24  0.099162  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1820f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pred, columns=['photoz'])\n",
    "df['specz'] = pd.Series(y_test)\n",
    "df['object_id'] = pd.Series(oid_test)\n",
    "os.makedirs(f'/data2/predictions/{model_name}', exist_ok=True)\n",
    "df.to_csv(f'/data2/predictions/{model_name}/testing_predictions.csv', index=False)\n",
    "metrics.to_csv(f'/data2/predictions/{model_name}/testing_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
