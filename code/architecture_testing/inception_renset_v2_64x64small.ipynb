{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd5cea9",
   "metadata": {},
   "source": [
    "# This notebook uses the Inception-ResNet v2 architecture. The stem has been modified to accommodate for 64x64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da1d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 23:19:40.940875: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-01 23:19:41.713429: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-01 23:19:41.713485: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-01 23:19:41.713489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import keras\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Input, concatenate, add, Activation, BatchNormalization, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from photoz_utils import *\n",
    "from DataMakerPlus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c6416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "GB_LIMIT = 17\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(GB_LIMIT*1000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dece0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (5, 64, 64)\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 300\n",
    "LEARNING_RATE = 0.00001\n",
    "Z_MAX = 4\n",
    "hparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'z_max': Z_MAX\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a546612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = f'/data/HSC/HSC_v6/step3A/64x64_training_small.hdf5'\n",
    "VAL_PATH = f'/data/HSC/HSC_v6/step3A/64x64_validation_small.hdf5'\n",
    "TEST_PATH = f'/data/HSC/HSC_v6/step3A/64x64_testing_small.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e5e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_args = {\n",
    "    'image_key': 'image',\n",
    "    'numerical_keys': None,\n",
    "    'y_key': 'specz_redshift',\n",
    "    'scaler': True,\n",
    "    'labels_encoding': False,\n",
    "    'batch_size': hparams['batch_size'],\n",
    "    'shuffle': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8e667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = HDF5DataGenerator(TRAIN_PATH, mode='train', **gen_args)\n",
    "val_gen = HDF5DataGenerator(VAL_PATH, mode='train', **gen_args)\n",
    "test_gen = HDF5DataGenerator(TEST_PATH, mode='test', **gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b1e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def calculate_loss(z_photo, z_spec):\n",
    "    \"\"\"\n",
    "    HSC METRIC. Returns an array. Loss is accuracy metric defined by HSC, meant\n",
    "    to capture the effects of bias, scatter, and outlier all in one. This has\n",
    "    uses for both point and density estimation.\n",
    "    z_photo: array\n",
    "        Photometric or predicted redshifts.\n",
    "    z_spec: array\n",
    "        Spectroscopic or actual redshifts.\n",
    "    \"\"\"\n",
    "    dz = delz(z_photo, z_spec)\n",
    "    gamma = 0.15\n",
    "    denominator = 1.0 + K.square(dz/gamma)\n",
    "    loss = 1 - 1.0 / denominator\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf11cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1,1)):\n",
    "    out = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding, data_format='channels_first')(x)\n",
    "    out = BatchNormalization(axis=1, scale=False)(out)\n",
    "    out = Activation('relu')(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e5ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_a(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 32, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 32, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 32, 3, 3)\n",
    "\n",
    "    branch3 = conv2d_bn(x, 32, 1, 1)\n",
    "    branch3 = conv2d_bn(branch3, 48, 3, 3)\n",
    "    branch3 = conv2d_bn(branch3, 64, 3, 3)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2, branch3], axis=1)\n",
    "    inc_block_out = Conv2D(384, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eac7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_a(x):\n",
    "    branch1 = conv2d_bn(x, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch2 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 256, 3, 3)\n",
    "    branch2 = conv2d_bn(branch2, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch3 = MaxPooling2D((3,3), strides=(2,2), padding='valid', data_format='channels_first')(x)\n",
    "\n",
    "    out = concatenate([branch1, branch2, branch3], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50a430e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_b(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 160, 1, 7)\n",
    "    branch2 = conv2d_bn(branch2, 192, 7, 1)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2], axis=1)\n",
    "    inc_block_out = Conv2D(1152, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b4a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_b(x):\n",
    "    branch1 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch1 = conv2d_bn(branch1, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch2 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 288, 3, 3)\n",
    "    branch2 = conv2d_bn(branch2, 320, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch3 = MaxPooling2D((3,3), strides=(2,2), padding='valid', data_format='channels_first')(x)\n",
    "\n",
    "    branch4 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch4 = conv2d_bn(branch4, 288, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    out = concatenate([branch1, branch2, branch3, branch4], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19668269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_c(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 224, 1, 3)\n",
    "    branch2 = conv2d_bn(branch2, 256, 3, 1)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2], axis=1)\n",
    "    inc_block_out = Conv2D(2144, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3383ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x):\n",
    "    out = conv2d_bn(x, 32, 3, 3)\n",
    "    out = conv2d_bn(out, 32, 3, 3)\n",
    "    out = conv2d_bn(out, 64, 3, 3)\n",
    "\n",
    "    branch1 = MaxPooling2D((3, 3), strides=(2,2), padding='same', data_format='channels_first')(out)\n",
    "    branch2 = conv2d_bn(out, 96, 3, 3, strides=(2,2))\n",
    "    out = concatenate([branch1, branch2], axis=1)\n",
    "\n",
    "    branch1 = conv2d_bn(out, 64, 1, 1)\n",
    "    branch1 = conv2d_bn(branch1, 96, 3, 3)\n",
    "    branch2 = conv2d_bn(out, 64, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 64, 7, 1)\n",
    "    branch2 = conv2d_bn(branch2, 64, 1, 7)\n",
    "    branch2 = conv2d_bn(branch2, 96, 3, 3)\n",
    "    out = concatenate([branch1, branch2], axis=1)\n",
    "\n",
    "    out = conv2d_bn(out, 384, 3, 3)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f962e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=IMAGE_SHAPE)\n",
    "x = stem(input_)\n",
    "\n",
    "x = inc_block_a(x)\n",
    "x = inc_block_a(x)\n",
    "x = reduction_block_a(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = reduction_block_b(x)\n",
    "x = inc_block_c(x)\n",
    "x = inc_block_c(x)\n",
    "x = GlobalAveragePooling2D(data_format='channels_first')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model(input_, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a32f9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 64, 64)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 64, 64)   1472        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 64, 64)  96          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 64, 64)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 64, 64)   9248        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 64, 64)  96          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 64, 64)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 64)   18496       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 64)  192         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64, 64, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, 32, 32)   0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 160, 32, 32)  0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 32, 32)   10304       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 32, 32)   28736       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 32, 32)   10304       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 32, 32)   28736       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 192, 32, 32)  0           ['activation_5[0][0]',           \n",
      "                                                                  'activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 384, 32, 32)  663936      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 384, 32, 32)  1152       ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 384, 32, 32)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 32)  96          ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 48, 32, 32)   13872       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 32)  96          ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 48, 32, 32)  144         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 48, 32, 32)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 32)   9248        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 64, 32, 32)   27712       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 32)  96          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 32)  96          ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 64, 32, 32)  192         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 64, 32, 32)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 32, 32)  0           ['activation_11[0][0]',          \n",
      "                                                                  'activation_13[0][0]',          \n",
      "                                                                  'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 384, 32, 32)  49536       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 384, 32, 32)  0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 384, 32, 32)  0           ['activation_10[0][0]',          \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 384, 32, 32)  0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 32)  96          ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 48, 32, 32)   13872       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 32)  96          ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 48, 32, 32)  144         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 48, 32, 32)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 32, 32, 32)   9248        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 64, 32, 32)   27712       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 32)  96          ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 32)  96          ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 64, 32, 32)  192         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 64, 32, 32)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 128, 32, 32)  0           ['activation_18[0][0]',          \n",
      "                                                                  'activation_20[0][0]',          \n",
      "                                                                  'activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 384, 32, 32)  49536       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 384, 32, 32)  0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 384, 32, 32)  0           ['activation_17[0][0]',          \n",
      "                                                                  'lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 384, 32, 32)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 256, 32, 32)  98560       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 256, 32, 32)  768        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 256, 32, 32)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 256, 32, 32)  590080      ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 256, 32, 32)  768        ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 256, 32, 32)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 384, 15, 15)  1327488     ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 384, 15, 15)  885120      ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 384, 15, 15)  1152       ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 384, 15, 15)  1152       ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 384, 15, 15)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 384, 15, 15)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 384, 15, 15)  0          ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 1152, 15, 15  0           ['activation_25[0][0]',          \n",
      "                                )                                 'activation_28[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 128, 15, 15)  147584      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 128, 15, 15)  384        ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 160, 15, 15)  480        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 192, 15, 15)  221376      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 192, 15, 15)  576        ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 192, 15, 15)  576        ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 384, 15, 15)  0           ['activation_29[0][0]',          \n",
      "                                                                  'activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_5[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_33[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 1152, 15, 15  0           ['concatenate_4[0][0]',          \n",
      "                                )                                 'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 1152, 15, 15  0           ['add_2[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 128, 15, 15)  384        ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 160, 15, 15)  480        ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 192, 15, 15)  576        ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 192, 15, 15)  576        ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 384, 15, 15)  0           ['activation_34[0][0]',          \n",
      "                                                                  'activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_6[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_38[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 1152, 15, 15  0           ['activation_33[0][0]',          \n",
      "                                )                                 'lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 1152, 15, 15  0           ['add_3[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 128, 15, 15)  384        ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 160, 15, 15)  480        ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 192, 15, 15)  576        ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 192, 15, 15)  576        ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 384, 15, 15)  0           ['activation_39[0][0]',          \n",
      "                                                                  'activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_43[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 1152, 15, 15  0           ['activation_38[0][0]',          \n",
      "                                )                                 'lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 1152, 15, 15  0           ['add_4[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 128, 15, 15)  384        ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 160, 15, 15)  480        ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 192, 15, 15)  576        ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 192, 15, 15)  576        ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 384, 15, 15)  0           ['activation_44[0][0]',          \n",
      "                                                                  'activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_48[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 1152, 15, 15  0           ['activation_43[0][0]',          \n",
      "                                )                                 'lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 1152, 15, 15  0           ['add_5[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 256, 15, 15)  768        ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 288, 15, 15)  663840      ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 256, 15, 15)  768        ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 288, 15, 15)  864        ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 256, 15, 15)  768        ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 288, 15, 15)  0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 384, 7, 7)    885120      ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 320, 7, 7)    829760      ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 288, 7, 7)    663840      ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 384, 7, 7)   1152        ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 320, 7, 7)   960         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 288, 7, 7)   864         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 384, 7, 7)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 320, 7, 7)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1152, 7, 7)  0           ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 288, 7, 7)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 2144, 7, 7)   0           ['activation_50[0][0]',          \n",
      "                                                                  'activation_53[0][0]',          \n",
      "                                                                  'max_pooling2d_2[0][0]',        \n",
      "                                                                  'activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 192, 7, 7)    411840      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 192, 7, 7)   576         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 224, 7, 7)    129248      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 224, 7, 7)   672         ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 224, 7, 7)    0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 192, 7, 7)    411840      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 256, 7, 7)    172288      ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 192, 7, 7)   576         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 256, 7, 7)   768         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 256, 7, 7)    0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 448, 7, 7)    0           ['activation_56[0][0]',          \n",
      "                                                                  'activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 2144, 7, 7)   962656      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 2144, 7, 7)   0           ['conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 2144, 7, 7)   0           ['concatenate_9[0][0]',          \n",
      "                                                                  'lambda_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 2144, 7, 7)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 192, 7, 7)    411840      ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 192, 7, 7)   576         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 224, 7, 7)    129248      ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 224, 7, 7)   672         ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 224, 7, 7)    0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 192, 7, 7)    411840      ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 256, 7, 7)    172288      ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 192, 7, 7)   576         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 256, 7, 7)   768         ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 256, 7, 7)    0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 448, 7, 7)    0           ['activation_61[0][0]',          \n",
      "                                                                  'activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 2144, 7, 7)   962656      ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 2144, 7, 7)   0           ['conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 2144, 7, 7)   0           ['activation_60[0][0]',          \n",
      "                                                                  'lambda_7[0][0]']               \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 2144, 7, 7)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2144)        0           ['activation_65[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2144)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2145        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,932,033\n",
      "Trainable params: 16,913,473\n",
      "Non-trainable params: 18,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e5173bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=hparams['learning_rate']), loss='mse', metrics='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e468ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'inception_resnet_2_64x64_small_v4'\n",
    "\n",
    "checkpoint_filepath = f'/data2/models/{model_name}/checkpoints/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
    "log_dir = os.path.join('/data2/logs/', model_name)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_freq='epoch',\n",
    "    save_best_only=True,\n",
    "    verbose=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "hparam_callback = hp.KerasCallback(log_dir, hparams)\n",
    "\n",
    "plateau_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb6ef640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6209 - mse: 0.6209\n",
      "Epoch 1: loss improved from inf to 0.62091, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 57s 796ms/step - loss: 0.6209 - mse: 0.6209 - val_loss: 0.6264 - val_mse: 0.6264 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5321 - mse: 0.5321\n",
      "Epoch 2: loss improved from 0.62091 to 0.53211, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 686ms/step - loss: 0.5321 - mse: 0.5321 - val_loss: 0.6235 - val_mse: 0.6235 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5342 - mse: 0.5342\n",
      "Epoch 3: loss did not improve from 0.53211\n",
      "40/40 [==============================] - 26s 656ms/step - loss: 0.5342 - mse: 0.5342 - val_loss: 0.6017 - val_mse: 0.6017 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5023 - mse: 0.5023\n",
      "Epoch 4: loss improved from 0.53211 to 0.50234, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 28s 707ms/step - loss: 0.5023 - mse: 0.5023 - val_loss: 0.4791 - val_mse: 0.4791 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4636 - mse: 0.4636\n",
      "Epoch 5: loss improved from 0.50234 to 0.46358, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 688ms/step - loss: 0.4636 - mse: 0.4636 - val_loss: 0.4362 - val_mse: 0.4362 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4995 - mse: 0.4995\n",
      "Epoch 6: loss did not improve from 0.46358\n",
      "40/40 [==============================] - 27s 669ms/step - loss: 0.4995 - mse: 0.4995 - val_loss: 0.4135 - val_mse: 0.4135 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4464 - mse: 0.4464\n",
      "Epoch 7: loss improved from 0.46358 to 0.44644, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 28s 703ms/step - loss: 0.4464 - mse: 0.4464 - val_loss: 0.4028 - val_mse: 0.4028 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4384 - mse: 0.4384\n",
      "Epoch 8: loss improved from 0.44644 to 0.43841, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 28s 697ms/step - loss: 0.4384 - mse: 0.4384 - val_loss: 0.3849 - val_mse: 0.3849 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4183 - mse: 0.4183\n",
      "Epoch 9: loss improved from 0.43841 to 0.41832, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 679ms/step - loss: 0.4183 - mse: 0.4183 - val_loss: 0.3215 - val_mse: 0.3215 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3959 - mse: 0.3959\n",
      "Epoch 10: loss improved from 0.41832 to 0.39595, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 28s 706ms/step - loss: 0.3959 - mse: 0.3959 - val_loss: 0.3534 - val_mse: 0.3534 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3972 - mse: 0.3972\n",
      "Epoch 11: loss did not improve from 0.39595\n",
      "40/40 [==============================] - 26s 662ms/step - loss: 0.3972 - mse: 0.3972 - val_loss: 0.3078 - val_mse: 0.3078 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3873 - mse: 0.3873\n",
      "Epoch 12: loss improved from 0.39595 to 0.38729, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 684ms/step - loss: 0.3873 - mse: 0.3873 - val_loss: 0.2797 - val_mse: 0.2797 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3709 - mse: 0.3709\n",
      "Epoch 13: loss improved from 0.38729 to 0.37095, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 687ms/step - loss: 0.3709 - mse: 0.3709 - val_loss: 0.3029 - val_mse: 0.3029 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3839 - mse: 0.3839\n",
      "Epoch 14: loss did not improve from 0.37095\n",
      "40/40 [==============================] - 24s 602ms/step - loss: 0.3839 - mse: 0.3839 - val_loss: 0.2280 - val_mse: 0.2280 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3633 - mse: 0.3633\n",
      "Epoch 15: loss improved from 0.37095 to 0.36325, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 23s 574ms/step - loss: 0.3633 - mse: 0.3633 - val_loss: 0.2010 - val_mse: 0.2010 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3364 - mse: 0.3364\n",
      "Epoch 16: loss improved from 0.36325 to 0.33640, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 550ms/step - loss: 0.3364 - mse: 0.3364 - val_loss: 0.1922 - val_mse: 0.1922 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3656 - mse: 0.3656\n",
      "Epoch 17: loss did not improve from 0.33640\n",
      "40/40 [==============================] - 20s 511ms/step - loss: 0.3656 - mse: 0.3656 - val_loss: 0.1919 - val_mse: 0.1919 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3565 - mse: 0.3565\n",
      "Epoch 18: loss did not improve from 0.33640\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.3565 - mse: 0.3565 - val_loss: 0.2109 - val_mse: 0.2109 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3390 - mse: 0.3390\n",
      "Epoch 19: loss did not improve from 0.33640\n",
      "40/40 [==============================] - 21s 515ms/step - loss: 0.3390 - mse: 0.3390 - val_loss: 0.1523 - val_mse: 0.1523 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3305 - mse: 0.3305\n",
      "Epoch 20: loss improved from 0.33640 to 0.33050, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 556ms/step - loss: 0.3305 - mse: 0.3305 - val_loss: 0.1781 - val_mse: 0.1781 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3338 - mse: 0.3338\n",
      "Epoch 21: loss did not improve from 0.33050\n",
      "40/40 [==============================] - 21s 528ms/step - loss: 0.3338 - mse: 0.3338 - val_loss: 0.1653 - val_mse: 0.1653 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3060 - mse: 0.3060\n",
      "Epoch 22: loss improved from 0.33050 to 0.30603, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 549ms/step - loss: 0.3060 - mse: 0.3060 - val_loss: 0.2702 - val_mse: 0.2702 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3134 - mse: 0.3134\n",
      "Epoch 23: loss did not improve from 0.30603\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.3134 - mse: 0.3134 - val_loss: 0.1315 - val_mse: 0.1315 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2974 - mse: 0.2974\n",
      "Epoch 24: loss improved from 0.30603 to 0.29738, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 551ms/step - loss: 0.2974 - mse: 0.2974 - val_loss: 0.1429 - val_mse: 0.1429 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2765 - mse: 0.2765\n",
      "Epoch 25: loss improved from 0.29738 to 0.27654, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 556ms/step - loss: 0.2765 - mse: 0.2765 - val_loss: 0.1263 - val_mse: 0.1263 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2855 - mse: 0.2855\n",
      "Epoch 26: loss did not improve from 0.27654\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.2855 - mse: 0.2855 - val_loss: 0.1239 - val_mse: 0.1239 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2785 - mse: 0.2785\n",
      "Epoch 27: loss did not improve from 0.27654\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.2785 - mse: 0.2785 - val_loss: 0.1330 - val_mse: 0.1330 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2622 - mse: 0.2622\n",
      "Epoch 28: loss improved from 0.27654 to 0.26217, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 554ms/step - loss: 0.2622 - mse: 0.2622 - val_loss: 0.1275 - val_mse: 0.1275 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2517 - mse: 0.2517\n",
      "Epoch 29: loss improved from 0.26217 to 0.25168, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 554ms/step - loss: 0.2517 - mse: 0.2517 - val_loss: 0.1582 - val_mse: 0.1582 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2631 - mse: 0.2631\n",
      "Epoch 30: loss did not improve from 0.25168\n",
      "40/40 [==============================] - 26s 660ms/step - loss: 0.2631 - mse: 0.2631 - val_loss: 0.1445 - val_mse: 0.1445 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2754 - mse: 0.2754\n",
      "Epoch 31: loss did not improve from 0.25168\n",
      "40/40 [==============================] - 25s 632ms/step - loss: 0.2754 - mse: 0.2754 - val_loss: 0.1843 - val_mse: 0.1843 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2751 - mse: 0.2751\n",
      "Epoch 32: loss did not improve from 0.25168\n",
      "40/40 [==============================] - 25s 642ms/step - loss: 0.2751 - mse: 0.2751 - val_loss: 0.1829 - val_mse: 0.1829 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2649 - mse: 0.2649\n",
      "Epoch 33: loss did not improve from 0.25168\n",
      "40/40 [==============================] - 26s 657ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.1459 - val_mse: 0.1459 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 34: loss improved from 0.25168 to 0.24677, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 676ms/step - loss: 0.2468 - mse: 0.2468 - val_loss: 0.1501 - val_mse: 0.1501 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2413 - mse: 0.2413\n",
      "Epoch 35: loss improved from 0.24677 to 0.24129, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 682ms/step - loss: 0.2413 - mse: 0.2413 - val_loss: 0.1526 - val_mse: 0.1526 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2346 - mse: 0.2346\n",
      "Epoch 36: loss improved from 0.24129 to 0.23459, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "40/40 [==============================] - 27s 682ms/step - loss: 0.2346 - mse: 0.2346 - val_loss: 0.1706 - val_mse: 0.1706 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2330 - mse: 0.2330\n",
      "Epoch 37: loss improved from 0.23459 to 0.23296, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 681ms/step - loss: 0.2330 - mse: 0.2330 - val_loss: 0.1216 - val_mse: 0.1216 - lr: 1.0000e-06\n",
      "Epoch 38/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2215 - mse: 0.2215\n",
      "Epoch 38: loss improved from 0.23296 to 0.22154, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 27s 669ms/step - loss: 0.2215 - mse: 0.2215 - val_loss: 0.1179 - val_mse: 0.1179 - lr: 1.0000e-06\n",
      "Epoch 39/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2377 - mse: 0.2377\n",
      "Epoch 39: loss did not improve from 0.22154\n",
      "40/40 [==============================] - 23s 565ms/step - loss: 0.2377 - mse: 0.2377 - val_loss: 0.1216 - val_mse: 0.1216 - lr: 1.0000e-06\n",
      "Epoch 40/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2295 - mse: 0.2295\n",
      "Epoch 40: loss did not improve from 0.22154\n",
      "40/40 [==============================] - 21s 529ms/step - loss: 0.2295 - mse: 0.2295 - val_loss: 0.1261 - val_mse: 0.1261 - lr: 1.0000e-06\n",
      "Epoch 41/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2184 - mse: 0.2184\n",
      "Epoch 41: loss improved from 0.22154 to 0.21843, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 23s 569ms/step - loss: 0.2184 - mse: 0.2184 - val_loss: 0.1218 - val_mse: 0.1218 - lr: 1.0000e-06\n",
      "Epoch 42/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2216 - mse: 0.2216\n",
      "Epoch 42: loss did not improve from 0.21843\n",
      "40/40 [==============================] - 21s 527ms/step - loss: 0.2216 - mse: 0.2216 - val_loss: 0.1201 - val_mse: 0.1201 - lr: 1.0000e-06\n",
      "Epoch 43/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2276 - mse: 0.2276\n",
      "Epoch 43: loss did not improve from 0.21843\n",
      "40/40 [==============================] - 21s 537ms/step - loss: 0.2276 - mse: 0.2276 - val_loss: 0.1260 - val_mse: 0.1260 - lr: 1.0000e-06\n",
      "Epoch 44/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2283 - mse: 0.2283\n",
      "Epoch 44: loss did not improve from 0.21843\n",
      "40/40 [==============================] - 34s 849ms/step - loss: 0.2283 - mse: 0.2283 - val_loss: 0.1285 - val_mse: 0.1285 - lr: 1.0000e-06\n",
      "Epoch 45/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2348 - mse: 0.2348\n",
      "Epoch 45: loss did not improve from 0.21843\n",
      "40/40 [==============================] - 30s 748ms/step - loss: 0.2348 - mse: 0.2348 - val_loss: 0.1205 - val_mse: 0.1205 - lr: 1.0000e-06\n",
      "Epoch 46/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2319 - mse: 0.2319\n",
      "Epoch 46: loss did not improve from 0.21843\n",
      "40/40 [==============================] - 33s 829ms/step - loss: 0.2319 - mse: 0.2319 - val_loss: 0.1203 - val_mse: 0.1203 - lr: 1.0000e-06\n",
      "Epoch 47/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2214 - mse: 0.2214\n",
      "Epoch 47: loss did not improve from 0.21843\n",
      "40/40 [==============================] - 31s 786ms/step - loss: 0.2214 - mse: 0.2214 - val_loss: 0.1275 - val_mse: 0.1275 - lr: 1.0000e-06\n",
      "Epoch 48/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2312 - mse: 0.2312\n",
      "Epoch 48: loss did not improve from 0.21843\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "40/40 [==============================] - 38s 954ms/step - loss: 0.2312 - mse: 0.2312 - val_loss: 0.1212 - val_mse: 0.1212 - lr: 1.0000e-06\n",
      "Epoch 49/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2273 - mse: 0.2273\n",
      "Epoch 49: loss did not improve from 0.21843\n",
      "40/40 [==============================] - 39s 976ms/step - loss: 0.2273 - mse: 0.2273 - val_loss: 0.1196 - val_mse: 0.1196 - lr: 1.0000e-07\n",
      "Epoch 50/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2202 - mse: 0.2202\n",
      "Epoch 50: loss did not improve from 0.21843\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2202 - mse: 0.2202 - val_loss: 0.1192 - val_mse: 0.1192 - lr: 1.0000e-07\n",
      "Epoch 51/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2182 - mse: 0.2182\n",
      "Epoch 51: loss improved from 0.21843 to 0.21823, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2182 - mse: 0.2182 - val_loss: 0.1188 - val_mse: 0.1188 - lr: 1.0000e-07\n",
      "Epoch 52/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2315 - mse: 0.2315\n",
      "Epoch 52: loss did not improve from 0.21823\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2315 - mse: 0.2315 - val_loss: 0.1182 - val_mse: 0.1182 - lr: 1.0000e-07\n",
      "Epoch 53/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2274 - mse: 0.2274\n",
      "Epoch 53: loss did not improve from 0.21823\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2274 - mse: 0.2274 - val_loss: 0.1174 - val_mse: 0.1174 - lr: 1.0000e-07\n",
      "Epoch 54/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2313 - mse: 0.2313\n",
      "Epoch 54: loss did not improve from 0.21823\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2313 - mse: 0.2313 - val_loss: 0.1171 - val_mse: 0.1171 - lr: 1.0000e-07\n",
      "Epoch 55/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2282 - mse: 0.2282\n",
      "Epoch 55: loss did not improve from 0.21823\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2282 - mse: 0.2282 - val_loss: 0.1171 - val_mse: 0.1171 - lr: 1.0000e-07\n",
      "Epoch 56/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2170 - mse: 0.2170\n",
      "Epoch 56: loss improved from 0.21823 to 0.21699, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2170 - mse: 0.2170 - val_loss: 0.1171 - val_mse: 0.1171 - lr: 1.0000e-07\n",
      "Epoch 57/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2256 - mse: 0.2256\n",
      "Epoch 57: loss did not improve from 0.21699\n",
      "40/40 [==============================] - 37s 925ms/step - loss: 0.2256 - mse: 0.2256 - val_loss: 0.1165 - val_mse: 0.1165 - lr: 1.0000e-07\n",
      "Epoch 58/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2228 - mse: 0.2228\n",
      "Epoch 58: loss did not improve from 0.21699\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2228 - mse: 0.2228 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-07\n",
      "Epoch 59/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2243 - mse: 0.2243\n",
      "Epoch 59: loss did not improve from 0.21699\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2243 - mse: 0.2243 - val_loss: 0.1172 - val_mse: 0.1172 - lr: 1.0000e-07\n",
      "Epoch 60/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2272 - mse: 0.2272\n",
      "Epoch 60: loss did not improve from 0.21699\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2272 - mse: 0.2272 - val_loss: 0.1171 - val_mse: 0.1171 - lr: 1.0000e-07\n",
      "Epoch 61/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2307 - mse: 0.2307\n",
      "Epoch 61: loss did not improve from 0.21699\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2307 - mse: 0.2307 - val_loss: 0.1173 - val_mse: 0.1173 - lr: 1.0000e-07\n",
      "Epoch 62/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2131 - mse: 0.2131\n",
      "Epoch 62: loss improved from 0.21699 to 0.21308, saving model to /data2/models/inception_resnet_2_64x64_small_v4/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2131 - mse: 0.2131 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-07\n",
      "Epoch 63/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2280 - mse: 0.2280\n",
      "Epoch 63: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2280 - mse: 0.2280 - val_loss: 0.1182 - val_mse: 0.1182 - lr: 1.0000e-07\n",
      "Epoch 64/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2313 - mse: 0.2313\n",
      "Epoch 64: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.2313 - mse: 0.2313 - val_loss: 0.1181 - val_mse: 0.1181 - lr: 1.0000e-07\n",
      "Epoch 65/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2313 - mse: 0.2313\n",
      "Epoch 65: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2313 - mse: 0.2313 - val_loss: 0.1173 - val_mse: 0.1173 - lr: 1.0000e-07\n",
      "Epoch 66/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2354 - mse: 0.2354\n",
      "Epoch 66: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2354 - mse: 0.2354 - val_loss: 0.1176 - val_mse: 0.1176 - lr: 1.0000e-07\n",
      "Epoch 67/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2264 - mse: 0.2264\n",
      "Epoch 67: loss did not improve from 0.21308\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2264 - mse: 0.2264 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-07\n",
      "Epoch 68/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2277 - mse: 0.2277\n",
      "Epoch 68: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2277 - mse: 0.2277 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-08\n",
      "Epoch 69/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2371 - mse: 0.2371\n",
      "Epoch 69: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 38s 946ms/step - loss: 0.2371 - mse: 0.2371 - val_loss: 0.1165 - val_mse: 0.1165 - lr: 1.0000e-08\n",
      "Epoch 70/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2245 - mse: 0.2245\n",
      "Epoch 70: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2245 - mse: 0.2245 - val_loss: 0.1164 - val_mse: 0.1164 - lr: 1.0000e-08\n",
      "Epoch 71/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2186 - mse: 0.2186\n",
      "Epoch 71: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2186 - mse: 0.2186 - val_loss: 0.1164 - val_mse: 0.1164 - lr: 1.0000e-08\n",
      "Epoch 72/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2222 - mse: 0.2222\n",
      "Epoch 72: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2222 - mse: 0.2222 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-08\n",
      "Epoch 73/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2225 - mse: 0.2225\n",
      "Epoch 73: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2225 - mse: 0.2225 - val_loss: 0.1165 - val_mse: 0.1165 - lr: 1.0000e-08\n",
      "Epoch 74/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2246 - mse: 0.2246\n",
      "Epoch 74: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2246 - mse: 0.2246 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-08\n",
      "Epoch 75/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2160 - mse: 0.2160\n",
      "Epoch 75: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2160 - mse: 0.2160 - val_loss: 0.1170 - val_mse: 0.1170 - lr: 1.0000e-08\n",
      "Epoch 76/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2228 - mse: 0.2228\n",
      "Epoch 76: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2228 - mse: 0.2228 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-08\n",
      "Epoch 77/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2286 - mse: 0.2286\n",
      "Epoch 77: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2286 - mse: 0.2286 - val_loss: 0.1170 - val_mse: 0.1170 - lr: 1.0000e-08\n",
      "Epoch 78/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2175 - mse: 0.2175\n",
      "Epoch 78: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2175 - mse: 0.2175 - val_loss: 0.1170 - val_mse: 0.1170 - lr: 1.0000e-08\n",
      "Epoch 79/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2172 - mse: 0.2172\n",
      "Epoch 79: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2172 - mse: 0.2172 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-08\n",
      "Epoch 80/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2311 - mse: 0.2311\n",
      "Epoch 80: loss did not improve from 0.21308\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2311 - mse: 0.2311 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-08\n",
      "Epoch 81/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2303 - mse: 0.2303\n",
      "Epoch 81: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 38s 949ms/step - loss: 0.2303 - mse: 0.2303 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-09\n",
      "Epoch 82/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2325 - mse: 0.2325\n",
      "Epoch 82: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 35s 873ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.1170 - val_mse: 0.1170 - lr: 1.0000e-09\n",
      "Epoch 83/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2258 - mse: 0.2258\n",
      "Epoch 83: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 36s 903ms/step - loss: 0.2258 - mse: 0.2258 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-09\n",
      "Epoch 84/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2333 - mse: 0.2333\n",
      "Epoch 84: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 35s 880ms/step - loss: 0.2333 - mse: 0.2333 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-09\n",
      "Epoch 85/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2197 - mse: 0.2197\n",
      "Epoch 85: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 34s 871ms/step - loss: 0.2197 - mse: 0.2197 - val_loss: 0.1171 - val_mse: 0.1171 - lr: 1.0000e-09\n",
      "Epoch 86/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2246 - mse: 0.2246\n",
      "Epoch 86: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 33s 830ms/step - loss: 0.2246 - mse: 0.2246 - val_loss: 0.1170 - val_mse: 0.1170 - lr: 1.0000e-09\n",
      "Epoch 87/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2251 - mse: 0.2251\n",
      "Epoch 87: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 33s 827ms/step - loss: 0.2251 - mse: 0.2251 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-09\n",
      "Epoch 88/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2218 - mse: 0.2218\n",
      "Epoch 88: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 30s 742ms/step - loss: 0.2218 - mse: 0.2218 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-09\n",
      "Epoch 89/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2271 - mse: 0.2271\n",
      "Epoch 89: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 33s 822ms/step - loss: 0.2271 - mse: 0.2271 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-09\n",
      "Epoch 90/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2242 - mse: 0.2242\n",
      "Epoch 90: loss did not improve from 0.21308\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "40/40 [==============================] - 28s 690ms/step - loss: 0.2242 - mse: 0.2242 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-09\n",
      "Epoch 91/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2254 - mse: 0.2254\n",
      "Epoch 91: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 30s 737ms/step - loss: 0.2254 - mse: 0.2254 - val_loss: 0.1163 - val_mse: 0.1163 - lr: 1.0000e-10\n",
      "Epoch 92/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2246 - mse: 0.2246\n",
      "Epoch 92: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 30s 742ms/step - loss: 0.2246 - mse: 0.2246 - val_loss: 0.1163 - val_mse: 0.1163 - lr: 1.0000e-10\n",
      "Epoch 93/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2195 - mse: 0.2195\n",
      "Epoch 93: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 30s 751ms/step - loss: 0.2195 - mse: 0.2195 - val_loss: 0.1165 - val_mse: 0.1165 - lr: 1.0000e-10\n",
      "Epoch 94/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2294 - mse: 0.2294\n",
      "Epoch 94: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 32s 786ms/step - loss: 0.2294 - mse: 0.2294 - val_loss: 0.1165 - val_mse: 0.1165 - lr: 1.0000e-10\n",
      "Epoch 95/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2257 - mse: 0.2257\n",
      "Epoch 95: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 30s 751ms/step - loss: 0.2257 - mse: 0.2257 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-10\n",
      "Epoch 96/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2352 - mse: 0.2352\n",
      "Epoch 96: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 30s 735ms/step - loss: 0.2352 - mse: 0.2352 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-10\n",
      "Epoch 97/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2214 - mse: 0.2214\n",
      "Epoch 97: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 29s 740ms/step - loss: 0.2214 - mse: 0.2214 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-10\n",
      "Epoch 98/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2371 - mse: 0.2371\n",
      "Epoch 98: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 30s 749ms/step - loss: 0.2371 - mse: 0.2371 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-10\n",
      "Epoch 99/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2256 - mse: 0.2256\n",
      "Epoch 99: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 28s 701ms/step - loss: 0.2256 - mse: 0.2256 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-10\n",
      "Epoch 100/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2274 - mse: 0.2274\n",
      "Epoch 100: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 29s 731ms/step - loss: 0.2274 - mse: 0.2274 - val_loss: 0.1164 - val_mse: 0.1164 - lr: 1.0000e-10\n",
      "Epoch 101/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2323 - mse: 0.2323\n",
      "Epoch 101: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 30s 736ms/step - loss: 0.2323 - mse: 0.2323 - val_loss: 0.1164 - val_mse: 0.1164 - lr: 1.0000e-10\n",
      "Epoch 102/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2266 - mse: 0.2266\n",
      "Epoch 102: loss did not improve from 0.21308\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "40/40 [==============================] - 33s 833ms/step - loss: 0.2266 - mse: 0.2266 - val_loss: 0.1164 - val_mse: 0.1164 - lr: 1.0000e-10\n",
      "Epoch 103/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2240 - mse: 0.2240\n",
      "Epoch 103: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 36s 908ms/step - loss: 0.2240 - mse: 0.2240 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-11\n",
      "Epoch 104/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2291 - mse: 0.2291\n",
      "Epoch 104: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 35s 879ms/step - loss: 0.2291 - mse: 0.2291 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-11\n",
      "Epoch 105/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2286 - mse: 0.2286\n",
      "Epoch 105: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 38s 959ms/step - loss: 0.2286 - mse: 0.2286 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-11\n",
      "Epoch 106/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2259 - mse: 0.2259\n",
      "Epoch 106: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2259 - mse: 0.2259 - val_loss: 0.1171 - val_mse: 0.1171 - lr: 1.0000e-11\n",
      "Epoch 107/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2265 - mse: 0.2265\n",
      "Epoch 107: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2265 - mse: 0.2265 - val_loss: 0.1171 - val_mse: 0.1171 - lr: 1.0000e-11\n",
      "Epoch 108/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2307 - mse: 0.2307\n",
      "Epoch 108: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2307 - mse: 0.2307 - val_loss: 0.1171 - val_mse: 0.1171 - lr: 1.0000e-11\n",
      "Epoch 109/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2247 - mse: 0.2247\n",
      "Epoch 109: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2247 - mse: 0.2247 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-11\n",
      "Epoch 110/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2242 - mse: 0.2242\n",
      "Epoch 110: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2242 - mse: 0.2242 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-11\n",
      "Epoch 111/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2183 - mse: 0.2183\n",
      "Epoch 111: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2183 - mse: 0.2183 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-11\n",
      "Epoch 112/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2331 - mse: 0.2331\n",
      "Epoch 112: loss did not improve from 0.21308\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2331 - mse: 0.2331 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-11\n",
      "Epoch 113/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2266 - mse: 0.2266\n",
      "Epoch 113: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2266 - mse: 0.2266 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-12\n",
      "Epoch 114/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2226 - mse: 0.2226\n",
      "Epoch 114: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 38s 963ms/step - loss: 0.2226 - mse: 0.2226 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-12\n",
      "Epoch 115/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2272 - mse: 0.2272\n",
      "Epoch 115: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 38s 956ms/step - loss: 0.2272 - mse: 0.2272 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-12\n",
      "Epoch 116/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2274 - mse: 0.2274\n",
      "Epoch 116: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2274 - mse: 0.2274 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-12\n",
      "Epoch 117/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2235 - mse: 0.2235\n",
      "Epoch 117: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2235 - mse: 0.2235 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-12\n",
      "Epoch 118/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2276 - mse: 0.2276\n",
      "Epoch 118: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2276 - mse: 0.2276 - val_loss: 0.1165 - val_mse: 0.1165 - lr: 1.0000e-12\n",
      "Epoch 119/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2335 - mse: 0.2335\n",
      "Epoch 119: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2335 - mse: 0.2335 - val_loss: 0.1164 - val_mse: 0.1164 - lr: 1.0000e-12\n",
      "Epoch 120/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2288 - mse: 0.2288\n",
      "Epoch 120: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2288 - mse: 0.2288 - val_loss: 0.1164 - val_mse: 0.1164 - lr: 1.0000e-12\n",
      "Epoch 121/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2342 - mse: 0.2342\n",
      "Epoch 121: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2342 - mse: 0.2342 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-12\n",
      "Epoch 122/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2207 - mse: 0.2207\n",
      "Epoch 122: loss did not improve from 0.21308\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2207 - mse: 0.2207 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-12\n",
      "Epoch 123/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2187 - mse: 0.2187\n",
      "Epoch 123: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2187 - mse: 0.2187 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-13\n",
      "Epoch 124/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2267 - mse: 0.2267\n",
      "Epoch 124: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2267 - mse: 0.2267 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-13\n",
      "Epoch 125/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2318 - mse: 0.2318\n",
      "Epoch 125: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2318 - mse: 0.2318 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-13\n",
      "Epoch 126/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2258 - mse: 0.2258\n",
      "Epoch 126: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 966ms/step - loss: 0.2258 - mse: 0.2258 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-13\n",
      "Epoch 127/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2250 - mse: 0.2250\n",
      "Epoch 127: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 38s 946ms/step - loss: 0.2250 - mse: 0.2250 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-13\n",
      "Epoch 128/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2267 - mse: 0.2267\n",
      "Epoch 128: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2267 - mse: 0.2267 - val_loss: 0.1170 - val_mse: 0.1170 - lr: 1.0000e-13\n",
      "Epoch 129/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2271 - mse: 0.2271\n",
      "Epoch 129: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2271 - mse: 0.2271 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-13\n",
      "Epoch 130/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2313 - mse: 0.2313\n",
      "Epoch 130: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2313 - mse: 0.2313 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-13\n",
      "Epoch 131/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2308 - mse: 0.2308\n",
      "Epoch 131: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2308 - mse: 0.2308 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-13\n",
      "Epoch 132/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2330 - mse: 0.2330\n",
      "Epoch 132: loss did not improve from 0.21308\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2330 - mse: 0.2330 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-13\n",
      "Epoch 133/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2260 - mse: 0.2260\n",
      "Epoch 133: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2260 - mse: 0.2260 - val_loss: 0.1169 - val_mse: 0.1169 - lr: 1.0000e-14\n",
      "Epoch 134/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2287 - mse: 0.2287\n",
      "Epoch 134: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2287 - mse: 0.2287 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-14\n",
      "Epoch 135/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2267 - mse: 0.2267\n",
      "Epoch 135: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 973ms/step - loss: 0.2267 - mse: 0.2267 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-14\n",
      "Epoch 136/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2287 - mse: 0.2287\n",
      "Epoch 136: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2287 - mse: 0.2287 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-14\n",
      "Epoch 137/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2284 - mse: 0.2284\n",
      "Epoch 137: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 970ms/step - loss: 0.2284 - mse: 0.2284 - val_loss: 0.1168 - val_mse: 0.1168 - lr: 1.0000e-14\n",
      "Epoch 138/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2329 - mse: 0.2329\n",
      "Epoch 138: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2329 - mse: 0.2329 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-14\n",
      "Epoch 139/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2132 - mse: 0.2132\n",
      "Epoch 139: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 38s 945ms/step - loss: 0.2132 - mse: 0.2132 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-14\n",
      "Epoch 140/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2280 - mse: 0.2280\n",
      "Epoch 140: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 969ms/step - loss: 0.2280 - mse: 0.2280 - val_loss: 0.1166 - val_mse: 0.1166 - lr: 1.0000e-14\n",
      "Epoch 141/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2215 - mse: 0.2215\n",
      "Epoch 141: loss did not improve from 0.21308\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2215 - mse: 0.2215 - val_loss: 0.1170 - val_mse: 0.1170 - lr: 1.0000e-14\n",
      "Epoch 142/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2271 - mse: 0.2271\n",
      "Epoch 142: loss did not improve from 0.21308\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "40/40 [==============================] - 39s 968ms/step - loss: 0.2271 - mse: 0.2271 - val_loss: 0.1167 - val_mse: 0.1167 - lr: 1.0000e-14\n",
      "Epoch 143/300\n",
      " 1/40 [..............................] - ETA: 35s - loss: 0.1418 - mse: 0.1418"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_gen, batch_size=hparams['batch_size'], epochs=hparams['num_epochs'], shuffle=True, verbose=1, validation_data=val_gen, callbacks=[tensorboard_callback, model_checkpoint_callback, hparam_callback, plateau_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec5e631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fba805c6f20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_filepath) # model might be overfitting. train mse is lowering but val mse fluctuates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f37d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 237ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92956d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(TEST_PATH, 'r') as file:\n",
    "    y_test = np.asarray(file['specz_redshift'][:])\n",
    "    oid_test = np.asarray(file['object_id'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8cf4741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(np.ravel(pred), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e952dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_point_metrics(pd.Series(np.ravel(pred)), pd.Series(y_test), binned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63a46536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zspec_bin</th>\n",
       "      <th>count</th>\n",
       "      <th>L</th>\n",
       "      <th>bias_bw</th>\n",
       "      <th>bias_conv</th>\n",
       "      <th>scatter_bw</th>\n",
       "      <th>scatter_conv</th>\n",
       "      <th>outlier_bw</th>\n",
       "      <th>outlier_conv</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 4.0]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.323019</td>\n",
       "      <td>0.05357</td>\n",
       "      <td>0.051052</td>\n",
       "      <td>0.133371</td>\n",
       "      <td>0.104865</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>0.2895</td>\n",
       "      <td>0.123146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    zspec_bin  count         L  bias_bw  bias_conv  scatter_bw  scatter_conv  \\\n",
       "0  (0.0, 4.0]   2000  0.323019  0.05357   0.051052    0.133371      0.104865   \n",
       "\n",
       "   outlier_bw  outlier_conv       mse  \n",
       "0      0.1175        0.2895  0.123146  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1820f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pred, columns=['photoz'])\n",
    "df['specz'] = pd.Series(y_test)\n",
    "df['object_id'] = pd.Series(oid_test)\n",
    "os.makedirs(f'/data2/predictions/{model_name}', exist_ok=True)\n",
    "df.to_csv(f'/data2/predictions/{model_name}/testing_predictions.csv', index=False)\n",
    "metrics.to_csv(f'/data2/predictions/{model_name}/testing_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
