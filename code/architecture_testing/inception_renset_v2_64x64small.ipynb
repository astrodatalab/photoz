{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd5cea9",
   "metadata": {},
   "source": [
    "# This notebook uses the Inception-ResNet v2 architecture. The stem has been modified to accommodate for 64x64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da1d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 16:03:40.995969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-01 16:03:41.568632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-01 16:03:41.568691: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-01 16:03:41.568696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import keras\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Input, concatenate, add, Activation, BatchNormalization, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from photoz_utils import *\n",
    "from DataMakerPlus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c6416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "GB_LIMIT = 17\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(GB_LIMIT*1000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dece0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (5, 64, 64)\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 300\n",
    "LEARNING_RATE = 0.00001\n",
    "Z_MAX = 4\n",
    "hparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'z_max': Z_MAX\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a546612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = f'/data/HSC/HSC_v6/step3A/64x64_training_small.hdf5'\n",
    "VAL_PATH = f'/data/HSC/HSC_v6/step3A/64x64_validation_small.hdf5'\n",
    "TEST_PATH = f'/data/HSC/HSC_v6/step3A/64x64_testing_small.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e5e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_args = {\n",
    "    'image_key': 'image',\n",
    "    'numerical_keys': None,\n",
    "    'y_key': 'specz_redshift',\n",
    "    'scaler': True,\n",
    "    'labels_encoding': False,\n",
    "    'batch_size': hparams['batch_size'],\n",
    "    'shuffle': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8e667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = HDF5DataGenerator(TRAIN_PATH, mode='train', **gen_args)\n",
    "val_gen = HDF5DataGenerator(VAL_PATH, mode='train', **gen_args)\n",
    "test_gen = HDF5DataGenerator(TEST_PATH, mode='test', **gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b1e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def calculate_loss(z_photo, z_spec):\n",
    "    \"\"\"\n",
    "    HSC METRIC. Returns an array. Loss is accuracy metric defined by HSC, meant\n",
    "    to capture the effects of bias, scatter, and outlier all in one. This has\n",
    "    uses for both point and density estimation.\n",
    "    z_photo: array\n",
    "        Photometric or predicted redshifts.\n",
    "    z_spec: array\n",
    "        Spectroscopic or actual redshifts.\n",
    "    \"\"\"\n",
    "    dz = delz(z_photo, z_spec)\n",
    "    gamma = 0.15\n",
    "    denominator = 1.0 + K.square(dz/gamma)\n",
    "    loss = 1 - 1.0 / denominator\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf11cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1,1)):\n",
    "    out = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding, data_format='channels_first')(x)\n",
    "    out = BatchNormalization(axis=1, scale=False)(out)\n",
    "    out = Activation('relu')(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e5ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_a(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 32, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 32, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 32, 3, 3)\n",
    "\n",
    "    branch3 = conv2d_bn(x, 32, 1, 1)\n",
    "    branch3 = conv2d_bn(branch3, 48, 3, 3)\n",
    "    branch3 = conv2d_bn(branch3, 64, 3, 3)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2, branch3], axis=1)\n",
    "    inc_block_out = Conv2D(384, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eac7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_a(x):\n",
    "    branch1 = conv2d_bn(x, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch2 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 256, 3, 3)\n",
    "    branch2 = conv2d_bn(branch2, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch3 = MaxPooling2D((3,3), strides=(2,2), padding='valid', data_format='channels_first')(x)\n",
    "\n",
    "    out = concatenate([branch1, branch2, branch3], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50a430e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_b(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 160, 1, 7)\n",
    "    branch2 = conv2d_bn(branch2, 192, 7, 1)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2], axis=1)\n",
    "    inc_block_out = Conv2D(1152, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b4a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_block_b(x):\n",
    "    branch1 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch1 = conv2d_bn(branch1, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch2 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 288, 3, 3)\n",
    "    branch2 = conv2d_bn(branch2, 320, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch3 = MaxPooling2D((3,3), strides=(2,2), padding='valid', data_format='channels_first')(x)\n",
    "\n",
    "    branch4 = conv2d_bn(x, 256, 1, 1)\n",
    "    branch4 = conv2d_bn(branch4, 288, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    out = concatenate([branch1, branch2, branch3, branch4], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19668269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_block_c(x):\n",
    "    original_in = x\n",
    "\n",
    "    branch1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch2 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 224, 1, 3)\n",
    "    branch2 = conv2d_bn(branch2, 256, 3, 1)\n",
    "\n",
    "    inc_block_out = concatenate([branch1, branch2], axis=1)\n",
    "    inc_block_out = Conv2D(2144, (1,1), strides=(1,1), padding='same', data_format='channels_first')(inc_block_out)\n",
    "    inc_block_out = Lambda(lambda x: x * 0.1)(inc_block_out)\n",
    "    out = add([original_in, inc_block_out])\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3383ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x):\n",
    "    out = conv2d_bn(x, 32, 3, 3)\n",
    "    out = conv2d_bn(out, 32, 3, 3)\n",
    "    out = conv2d_bn(out, 64, 3, 3)\n",
    "\n",
    "    branch1 = MaxPooling2D((3, 3), strides=(2,2), padding='same', data_format='channels_first')(out)\n",
    "    branch2 = conv2d_bn(out, 96, 3, 3, strides=(2,2))\n",
    "    out = concatenate([branch1, branch2], axis=1)\n",
    "\n",
    "    branch1 = conv2d_bn(out, 64, 1, 1)\n",
    "    branch1 = conv2d_bn(branch1, 96, 3, 3)\n",
    "    branch2 = conv2d_bn(out, 64, 1, 1)\n",
    "    branch2 = conv2d_bn(branch2, 64, 7, 1)\n",
    "    branch2 = conv2d_bn(branch2, 64, 1, 7)\n",
    "    branch2 = conv2d_bn(branch2, 96, 3, 3)\n",
    "    out = concatenate([branch1, branch2], axis=1)\n",
    "\n",
    "    out = conv2d_bn(out, 384, 3, 3)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f962e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=IMAGE_SHAPE)\n",
    "x = stem(input_)\n",
    "\n",
    "x = inc_block_a(x)\n",
    "x = inc_block_a(x)\n",
    "x = reduction_block_a(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = inc_block_b(x)\n",
    "x = reduction_block_b(x)\n",
    "x = inc_block_c(x)\n",
    "x = inc_block_c(x)\n",
    "x = GlobalAveragePooling2D(data_format='channels_first')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model(input_, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a32f9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 64, 64)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 64, 64)   1472        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 64, 64)  96          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 64, 64)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 64, 64)   9248        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 64, 64)  96          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 64, 64)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 64)   18496       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 64)  192         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64, 64, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, 32, 32)   0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 160, 32, 32)  0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 32, 32)   10304       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 32, 32)   28736       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 32, 32)   10304       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 32, 32)   28736       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64, 32, 32)  192         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 64, 32, 32)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 96, 32, 32)   55392       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 96, 32, 32)  288         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 96, 32, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 192, 32, 32)  0           ['activation_5[0][0]',           \n",
      "                                                                  'activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 384, 32, 32)  663936      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 384, 32, 32)  1152       ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 384, 32, 32)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 32)  96          ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 48, 32, 32)   13872       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 32)  96          ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 48, 32, 32)  144         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 48, 32, 32)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 32)   9248        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 64, 32, 32)   27712       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 32)  96          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 32)  96          ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 64, 32, 32)  192         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 64, 32, 32)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 32, 32)  0           ['activation_11[0][0]',          \n",
      "                                                                  'activation_13[0][0]',          \n",
      "                                                                  'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 384, 32, 32)  49536       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 384, 32, 32)  0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 384, 32, 32)  0           ['activation_10[0][0]',          \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 384, 32, 32)  0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 32)  96          ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 48, 32, 32)   13872       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 32)  96          ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 48, 32, 32)  144         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 48, 32, 32)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 32, 32)   12320       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 32, 32, 32)   9248        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 64, 32, 32)   27712       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 32)  96          ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 32)  96          ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 64, 32, 32)  192         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 64, 32, 32)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 128, 32, 32)  0           ['activation_18[0][0]',          \n",
      "                                                                  'activation_20[0][0]',          \n",
      "                                                                  'activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 384, 32, 32)  49536       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 384, 32, 32)  0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 384, 32, 32)  0           ['activation_17[0][0]',          \n",
      "                                                                  'lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 384, 32, 32)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 256, 32, 32)  98560       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 256, 32, 32)  768        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 256, 32, 32)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 256, 32, 32)  590080      ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 256, 32, 32)  768        ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 256, 32, 32)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 384, 15, 15)  1327488     ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 384, 15, 15)  885120      ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 384, 15, 15)  1152       ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 384, 15, 15)  1152       ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 384, 15, 15)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 384, 15, 15)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 384, 15, 15)  0          ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 1152, 15, 15  0           ['activation_25[0][0]',          \n",
      "                                )                                 'activation_28[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 128, 15, 15)  147584      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 128, 15, 15)  384        ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 160, 15, 15)  480        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 192, 15, 15)  221376      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 192, 15, 15)  576        ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 192, 15, 15)  576        ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 384, 15, 15)  0           ['activation_29[0][0]',          \n",
      "                                                                  'activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_5[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_33[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 1152, 15, 15  0           ['concatenate_4[0][0]',          \n",
      "                                )                                 'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 1152, 15, 15  0           ['add_2[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 128, 15, 15)  384        ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 160, 15, 15)  480        ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 192, 15, 15)  576        ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 192, 15, 15)  576        ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 384, 15, 15)  0           ['activation_34[0][0]',          \n",
      "                                                                  'activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_6[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_38[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 1152, 15, 15  0           ['activation_33[0][0]',          \n",
      "                                )                                 'lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 1152, 15, 15  0           ['add_3[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 128, 15, 15)  384        ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 160, 15, 15)  480        ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 192, 15, 15)  576        ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 192, 15, 15)  576        ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 384, 15, 15)  0           ['activation_39[0][0]',          \n",
      "                                                                  'activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_43[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 1152, 15, 15  0           ['activation_38[0][0]',          \n",
      "                                )                                 'lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 1152, 15, 15  0           ['add_4[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 128, 15, 15)  147584      ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 128, 15, 15)  384        ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 128, 15, 15)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 160, 15, 15)  143520      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 160, 15, 15)  480        ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 160, 15, 15)  0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 192, 15, 15)  221376      ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 192, 15, 15)  215232      ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 192, 15, 15)  576        ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 192, 15, 15)  576        ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 192, 15, 15)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 384, 15, 15)  0           ['activation_44[0][0]',          \n",
      "                                                                  'activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 1152, 15, 15  443520      ['concatenate_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 1152, 15, 15  0           ['conv2d_48[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 1152, 15, 15  0           ['activation_43[0][0]',          \n",
      "                                )                                 'lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 1152, 15, 15  0           ['add_5[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 256, 15, 15)  768        ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 288, 15, 15)  663840      ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 256, 15, 15)  295168      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 256, 15, 15)  768        ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 288, 15, 15)  864        ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 256, 15, 15)  768        ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 288, 15, 15)  0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 256, 15, 15)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 384, 7, 7)    885120      ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 320, 7, 7)    829760      ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 288, 7, 7)    663840      ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 384, 7, 7)   1152        ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 320, 7, 7)   960         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 288, 7, 7)   864         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 384, 7, 7)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 320, 7, 7)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1152, 7, 7)  0           ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 288, 7, 7)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 2144, 7, 7)   0           ['activation_50[0][0]',          \n",
      "                                                                  'activation_53[0][0]',          \n",
      "                                                                  'max_pooling2d_2[0][0]',        \n",
      "                                                                  'activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 192, 7, 7)    411840      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 192, 7, 7)   576         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 224, 7, 7)    129248      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 224, 7, 7)   672         ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 224, 7, 7)    0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 192, 7, 7)    411840      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 256, 7, 7)    172288      ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 192, 7, 7)   576         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 256, 7, 7)   768         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 256, 7, 7)    0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 448, 7, 7)    0           ['activation_56[0][0]',          \n",
      "                                                                  'activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 2144, 7, 7)   962656      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 2144, 7, 7)   0           ['conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 2144, 7, 7)   0           ['concatenate_9[0][0]',          \n",
      "                                                                  'lambda_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 2144, 7, 7)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 192, 7, 7)    411840      ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 192, 7, 7)   576         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 224, 7, 7)    129248      ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 224, 7, 7)   672         ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 224, 7, 7)    0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 192, 7, 7)    411840      ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 256, 7, 7)    172288      ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 192, 7, 7)   576         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 256, 7, 7)   768         ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 192, 7, 7)    0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 256, 7, 7)    0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 448, 7, 7)    0           ['activation_61[0][0]',          \n",
      "                                                                  'activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 2144, 7, 7)   962656      ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 2144, 7, 7)   0           ['conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 2144, 7, 7)   0           ['activation_60[0][0]',          \n",
      "                                                                  'lambda_7[0][0]']               \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 2144, 7, 7)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2144)        0           ['activation_65[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2144)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2145        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,932,033\n",
      "Trainable params: 16,913,473\n",
      "Non-trainable params: 18,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e5173bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=hparams['learning_rate']), loss='mse', metrics='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e468ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'inception_resnet_2_64x64_small_v2'\n",
    "\n",
    "checkpoint_filepath = f'/data2/models/{model_name}/checkpoints/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
    "log_dir = os.path.join('/data2/logs/', model_name)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_freq='epoch',\n",
    "    save_best_only=True,\n",
    "    verbose=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "hparam_callback = hp.KerasCallback(log_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb6ef640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6673 - mse: 0.6673\n",
      "Epoch 1: loss improved from inf to 0.66733, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 91s 1s/step - loss: 0.6673 - mse: 0.6673 - val_loss: 0.6018 - val_mse: 0.6018\n",
      "Epoch 2/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5343 - mse: 0.5343\n",
      "Epoch 2: loss improved from 0.66733 to 0.53433, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.5343 - mse: 0.5343 - val_loss: 0.5948 - val_mse: 0.5948\n",
      "Epoch 3/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5524 - mse: 0.5524\n",
      "Epoch 3: loss did not improve from 0.53433\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.5524 - mse: 0.5524 - val_loss: 0.6173 - val_mse: 0.6173\n",
      "Epoch 4/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4977 - mse: 0.4977\n",
      "Epoch 4: loss improved from 0.53433 to 0.49770, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.4977 - mse: 0.4977 - val_loss: 0.6510 - val_mse: 0.6510\n",
      "Epoch 5/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5114 - mse: 0.5114\n",
      "Epoch 5: loss did not improve from 0.49770\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.5114 - mse: 0.5114 - val_loss: 0.6756 - val_mse: 0.6756\n",
      "Epoch 6/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4948 - mse: 0.4948\n",
      "Epoch 6: loss improved from 0.49770 to 0.49478, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.4948 - mse: 0.4948 - val_loss: 0.7938 - val_mse: 0.7938\n",
      "Epoch 7/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4441 - mse: 0.4441\n",
      "Epoch 7: loss improved from 0.49478 to 0.44405, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.4441 - mse: 0.4441 - val_loss: 0.7741 - val_mse: 0.7741\n",
      "Epoch 8/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4664 - mse: 0.4664\n",
      "Epoch 8: loss did not improve from 0.44405\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.4664 - mse: 0.4664 - val_loss: 0.6860 - val_mse: 0.6860\n",
      "Epoch 9/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4251 - mse: 0.4251\n",
      "Epoch 9: loss improved from 0.44405 to 0.42510, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.4251 - mse: 0.4251 - val_loss: 0.6998 - val_mse: 0.6998\n",
      "Epoch 10/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4164 - mse: 0.4164\n",
      "Epoch 10: loss improved from 0.42510 to 0.41635, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.4164 - mse: 0.4164 - val_loss: 0.6101 - val_mse: 0.6101\n",
      "Epoch 11/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4017 - mse: 0.4017\n",
      "Epoch 11: loss improved from 0.41635 to 0.40170, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.4017 - mse: 0.4017 - val_loss: 0.5049 - val_mse: 0.5049\n",
      "Epoch 12/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3670 - mse: 0.3670\n",
      "Epoch 12: loss improved from 0.40170 to 0.36700, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.3670 - mse: 0.3670 - val_loss: 0.4144 - val_mse: 0.4144\n",
      "Epoch 13/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4002 - mse: 0.4002\n",
      "Epoch 13: loss did not improve from 0.36700\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.4002 - mse: 0.4002 - val_loss: 0.4459 - val_mse: 0.4459\n",
      "Epoch 14/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3658 - mse: 0.3658\n",
      "Epoch 14: loss improved from 0.36700 to 0.36584, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.3658 - mse: 0.3658 - val_loss: 0.4131 - val_mse: 0.4131\n",
      "Epoch 15/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3907 - mse: 0.3907\n",
      "Epoch 15: loss did not improve from 0.36584\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.3907 - mse: 0.3907 - val_loss: 0.3679 - val_mse: 0.3679\n",
      "Epoch 16/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3810 - mse: 0.3810\n",
      "Epoch 16: loss did not improve from 0.36584\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.3810 - mse: 0.3810 - val_loss: 0.2816 - val_mse: 0.2816\n",
      "Epoch 17/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3402 - mse: 0.3402\n",
      "Epoch 17: loss improved from 0.36584 to 0.34016, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.3402 - mse: 0.3402 - val_loss: 0.2267 - val_mse: 0.2267\n",
      "Epoch 18/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3539 - mse: 0.3539\n",
      "Epoch 18: loss did not improve from 0.34016\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.3539 - mse: 0.3539 - val_loss: 0.2558 - val_mse: 0.2558\n",
      "Epoch 19/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3297 - mse: 0.3297\n",
      "Epoch 19: loss improved from 0.34016 to 0.32969, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 43s 1s/step - loss: 0.3297 - mse: 0.3297 - val_loss: 0.1464 - val_mse: 0.1464\n",
      "Epoch 20/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3145 - mse: 0.3145\n",
      "Epoch 20: loss improved from 0.32969 to 0.31450, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.3145 - mse: 0.3145 - val_loss: 0.1501 - val_mse: 0.1501\n",
      "Epoch 21/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3162 - mse: 0.3162\n",
      "Epoch 21: loss did not improve from 0.31450\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.3162 - mse: 0.3162 - val_loss: 0.1739 - val_mse: 0.1739\n",
      "Epoch 22/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2942 - mse: 0.2942\n",
      "Epoch 22: loss improved from 0.31450 to 0.29425, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2942 - mse: 0.2942 - val_loss: 0.1360 - val_mse: 0.1360\n",
      "Epoch 23/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3131 - mse: 0.3131\n",
      "Epoch 23: loss did not improve from 0.29425\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.3131 - mse: 0.3131 - val_loss: 0.1256 - val_mse: 0.1256\n",
      "Epoch 24/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3082 - mse: 0.3082\n",
      "Epoch 24: loss did not improve from 0.29425\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.3082 - mse: 0.3082 - val_loss: 0.2599 - val_mse: 0.2599\n",
      "Epoch 25/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3103 - mse: 0.3103\n",
      "Epoch 25: loss did not improve from 0.29425\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.3103 - mse: 0.3103 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 26/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2832 - mse: 0.2832\n",
      "Epoch 26: loss improved from 0.29425 to 0.28324, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2832 - mse: 0.2832 - val_loss: 0.1415 - val_mse: 0.1415\n",
      "Epoch 27/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2982 - mse: 0.2982\n",
      "Epoch 27: loss did not improve from 0.28324\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2982 - mse: 0.2982 - val_loss: 0.1926 - val_mse: 0.1926\n",
      "Epoch 28/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2781 - mse: 0.2781\n",
      "Epoch 28: loss improved from 0.28324 to 0.27814, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2781 - mse: 0.2781 - val_loss: 0.1284 - val_mse: 0.1284\n",
      "Epoch 29/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2941 - mse: 0.2941\n",
      "Epoch 29: loss did not improve from 0.27814\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2941 - mse: 0.2941 - val_loss: 0.1635 - val_mse: 0.1635\n",
      "Epoch 30/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2626 - mse: 0.2626\n",
      "Epoch 30: loss improved from 0.27814 to 0.26256, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2626 - mse: 0.2626 - val_loss: 0.1293 - val_mse: 0.1293\n",
      "Epoch 31/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2703 - mse: 0.2703\n",
      "Epoch 31: loss did not improve from 0.26256\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2703 - mse: 0.2703 - val_loss: 0.1168 - val_mse: 0.1168\n",
      "Epoch 32/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2776 - mse: 0.2776\n",
      "Epoch 32: loss did not improve from 0.26256\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2776 - mse: 0.2776 - val_loss: 0.1798 - val_mse: 0.1798\n",
      "Epoch 33/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2858 - mse: 0.2858\n",
      "Epoch 33: loss did not improve from 0.26256\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.2858 - mse: 0.2858 - val_loss: 0.1784 - val_mse: 0.1784\n",
      "Epoch 34/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2702 - mse: 0.2702\n",
      "Epoch 34: loss did not improve from 0.26256\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2702 - mse: 0.2702 - val_loss: 0.1170 - val_mse: 0.1170\n",
      "Epoch 35/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2559 - mse: 0.2559\n",
      "Epoch 35: loss improved from 0.26256 to 0.25587, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2559 - mse: 0.2559 - val_loss: 0.1184 - val_mse: 0.1184\n",
      "Epoch 36/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2454 - mse: 0.2454\n",
      "Epoch 36: loss improved from 0.25587 to 0.24541, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2454 - mse: 0.2454 - val_loss: 0.1201 - val_mse: 0.1201\n",
      "Epoch 37/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2510 - mse: 0.2510\n",
      "Epoch 37: loss did not improve from 0.24541\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.2510 - mse: 0.2510 - val_loss: 0.1139 - val_mse: 0.1139\n",
      "Epoch 38/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2484 - mse: 0.2484\n",
      "Epoch 38: loss did not improve from 0.24541\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.2484 - mse: 0.2484 - val_loss: 0.1215 - val_mse: 0.1215\n",
      "Epoch 39/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2495 - mse: 0.2495\n",
      "Epoch 39: loss did not improve from 0.24541\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2495 - mse: 0.2495 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 40/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2367 - mse: 0.2367\n",
      "Epoch 40: loss improved from 0.24541 to 0.23669, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2367 - mse: 0.2367 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 41/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2268 - mse: 0.2268\n",
      "Epoch 41: loss improved from 0.23669 to 0.22684, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2268 - mse: 0.2268 - val_loss: 0.1093 - val_mse: 0.1093\n",
      "Epoch 42/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2262 - mse: 0.2262\n",
      "Epoch 42: loss improved from 0.22684 to 0.22616, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2262 - mse: 0.2262 - val_loss: 0.1560 - val_mse: 0.1560\n",
      "Epoch 43/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2141 - mse: 0.2141\n",
      "Epoch 43: loss improved from 0.22616 to 0.21414, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2141 - mse: 0.2141 - val_loss: 0.1215 - val_mse: 0.1215\n",
      "Epoch 44/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2118 - mse: 0.2118\n",
      "Epoch 44: loss improved from 0.21414 to 0.21178, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2118 - mse: 0.2118 - val_loss: 0.1563 - val_mse: 0.1563\n",
      "Epoch 45/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2194 - mse: 0.2194\n",
      "Epoch 45: loss did not improve from 0.21178\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2194 - mse: 0.2194 - val_loss: 0.1431 - val_mse: 0.1431\n",
      "Epoch 46/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2412 - mse: 0.2412\n",
      "Epoch 46: loss did not improve from 0.21178\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2412 - mse: 0.2412 - val_loss: 0.1293 - val_mse: 0.1293\n",
      "Epoch 47/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2073 - mse: 0.2073\n",
      "Epoch 47: loss improved from 0.21178 to 0.20731, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2073 - mse: 0.2073 - val_loss: 0.1246 - val_mse: 0.1246\n",
      "Epoch 48/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2073 - mse: 0.2073\n",
      "Epoch 48: loss improved from 0.20731 to 0.20729, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2073 - mse: 0.2073 - val_loss: 0.1304 - val_mse: 0.1304\n",
      "Epoch 49/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2059 - mse: 0.2059\n",
      "Epoch 49: loss improved from 0.20729 to 0.20589, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.2059 - mse: 0.2059 - val_loss: 0.1803 - val_mse: 0.1803\n",
      "Epoch 50/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2112 - mse: 0.2112\n",
      "Epoch 50: loss did not improve from 0.20589\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2112 - mse: 0.2112 - val_loss: 0.1108 - val_mse: 0.1108\n",
      "Epoch 51/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2112 - mse: 0.2112\n",
      "Epoch 51: loss did not improve from 0.20589\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2112 - mse: 0.2112 - val_loss: 0.1401 - val_mse: 0.1401\n",
      "Epoch 52/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1948 - mse: 0.1948\n",
      "Epoch 52: loss improved from 0.20589 to 0.19477, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1948 - mse: 0.1948 - val_loss: 0.1286 - val_mse: 0.1286\n",
      "Epoch 53/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2069 - mse: 0.2069\n",
      "Epoch 53: loss did not improve from 0.19477\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2069 - mse: 0.2069 - val_loss: 0.1208 - val_mse: 0.1208\n",
      "Epoch 54/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1899 - mse: 0.1899\n",
      "Epoch 54: loss improved from 0.19477 to 0.18986, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1899 - mse: 0.1899 - val_loss: 0.1040 - val_mse: 0.1040\n",
      "Epoch 55/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1875 - mse: 0.1875\n",
      "Epoch 55: loss improved from 0.18986 to 0.18751, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1875 - mse: 0.1875 - val_loss: 0.1001 - val_mse: 0.1001\n",
      "Epoch 56/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1986 - mse: 0.1986\n",
      "Epoch 56: loss did not improve from 0.18751\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1986 - mse: 0.1986 - val_loss: 0.1183 - val_mse: 0.1183\n",
      "Epoch 57/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1889 - mse: 0.1889\n",
      "Epoch 57: loss did not improve from 0.18751\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1889 - mse: 0.1889 - val_loss: 0.1194 - val_mse: 0.1194\n",
      "Epoch 58/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1890 - mse: 0.1890\n",
      "Epoch 58: loss did not improve from 0.18751\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1890 - mse: 0.1890 - val_loss: 0.1236 - val_mse: 0.1236\n",
      "Epoch 59/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1803 - mse: 0.1803\n",
      "Epoch 59: loss improved from 0.18751 to 0.18033, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1803 - mse: 0.1803 - val_loss: 0.1039 - val_mse: 0.1039\n",
      "Epoch 60/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1878 - mse: 0.1878\n",
      "Epoch 60: loss did not improve from 0.18033\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.1013 - val_mse: 0.1013\n",
      "Epoch 61/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1763 - mse: 0.1763\n",
      "Epoch 61: loss improved from 0.18033 to 0.17634, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1763 - mse: 0.1763 - val_loss: 0.1189 - val_mse: 0.1189\n",
      "Epoch 62/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1789 - mse: 0.1789\n",
      "Epoch 62: loss did not improve from 0.17634\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.1789 - mse: 0.1789 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 63/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1761 - mse: 0.1761\n",
      "Epoch 63: loss improved from 0.17634 to 0.17612, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1761 - mse: 0.1761 - val_loss: 0.1196 - val_mse: 0.1196\n",
      "Epoch 64/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1761 - mse: 0.1761\n",
      "Epoch 64: loss improved from 0.17612 to 0.17611, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 43s 1s/step - loss: 0.1761 - mse: 0.1761 - val_loss: 0.1168 - val_mse: 0.1168\n",
      "Epoch 65/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1735 - mse: 0.1735\n",
      "Epoch 65: loss improved from 0.17611 to 0.17354, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1735 - mse: 0.1735 - val_loss: 0.1047 - val_mse: 0.1047\n",
      "Epoch 66/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1686 - mse: 0.1686\n",
      "Epoch 66: loss improved from 0.17354 to 0.16860, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1686 - mse: 0.1686 - val_loss: 0.1180 - val_mse: 0.1180\n",
      "Epoch 67/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1719 - mse: 0.1719\n",
      "Epoch 67: loss did not improve from 0.16860\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.1719 - mse: 0.1719 - val_loss: 0.1204 - val_mse: 0.1204\n",
      "Epoch 68/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1605 - mse: 0.1605\n",
      "Epoch 68: loss improved from 0.16860 to 0.16050, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1605 - mse: 0.1605 - val_loss: 0.0987 - val_mse: 0.0987\n",
      "Epoch 69/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1611 - mse: 0.1611\n",
      "Epoch 69: loss did not improve from 0.16050\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1611 - mse: 0.1611 - val_loss: 0.1318 - val_mse: 0.1318\n",
      "Epoch 70/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1614 - mse: 0.1614\n",
      "Epoch 70: loss did not improve from 0.16050\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1614 - mse: 0.1614 - val_loss: 0.1506 - val_mse: 0.1506\n",
      "Epoch 71/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1577 - mse: 0.1577\n",
      "Epoch 71: loss improved from 0.16050 to 0.15772, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1577 - mse: 0.1577 - val_loss: 0.1114 - val_mse: 0.1114\n",
      "Epoch 72/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1565 - mse: 0.1565\n",
      "Epoch 72: loss improved from 0.15772 to 0.15652, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1565 - mse: 0.1565 - val_loss: 0.1544 - val_mse: 0.1544\n",
      "Epoch 73/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1590 - mse: 0.1590\n",
      "Epoch 73: loss did not improve from 0.15652\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1590 - mse: 0.1590 - val_loss: 0.0974 - val_mse: 0.0974\n",
      "Epoch 74/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1502 - mse: 0.1502\n",
      "Epoch 74: loss improved from 0.15652 to 0.15021, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1502 - mse: 0.1502 - val_loss: 0.0983 - val_mse: 0.0983\n",
      "Epoch 75/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1454 - mse: 0.1454\n",
      "Epoch 75: loss improved from 0.15021 to 0.14544, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1454 - mse: 0.1454 - val_loss: 0.1129 - val_mse: 0.1129\n",
      "Epoch 76/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1424 - mse: 0.1424\n",
      "Epoch 76: loss improved from 0.14544 to 0.14240, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1424 - mse: 0.1424 - val_loss: 0.1234 - val_mse: 0.1234\n",
      "Epoch 77/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1437 - mse: 0.1437\n",
      "Epoch 77: loss did not improve from 0.14240\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1437 - mse: 0.1437 - val_loss: 0.1327 - val_mse: 0.1327\n",
      "Epoch 78/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1441 - mse: 0.1441\n",
      "Epoch 78: loss did not improve from 0.14240\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1441 - mse: 0.1441 - val_loss: 0.1153 - val_mse: 0.1153\n",
      "Epoch 79/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1395 - mse: 0.1395\n",
      "Epoch 79: loss improved from 0.14240 to 0.13953, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1395 - mse: 0.1395 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "Epoch 80/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1347 - mse: 0.1347\n",
      "Epoch 80: loss improved from 0.13953 to 0.13472, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1347 - mse: 0.1347 - val_loss: 0.1230 - val_mse: 0.1230\n",
      "Epoch 81/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1332 - mse: 0.1332\n",
      "Epoch 81: loss improved from 0.13472 to 0.13318, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1332 - mse: 0.1332 - val_loss: 0.1102 - val_mse: 0.1102\n",
      "Epoch 82/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1370 - mse: 0.1370\n",
      "Epoch 82: loss did not improve from 0.13318\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.1370 - mse: 0.1370 - val_loss: 0.0965 - val_mse: 0.0965\n",
      "Epoch 83/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1355 - mse: 0.1355\n",
      "Epoch 83: loss did not improve from 0.13318\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1355 - mse: 0.1355 - val_loss: 0.1187 - val_mse: 0.1187\n",
      "Epoch 84/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1366 - mse: 0.1366\n",
      "Epoch 84: loss did not improve from 0.13318\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1366 - mse: 0.1366 - val_loss: 0.1172 - val_mse: 0.1172\n",
      "Epoch 85/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1170 - mse: 0.1170\n",
      "Epoch 85: loss improved from 0.13318 to 0.11703, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.1170 - mse: 0.1170 - val_loss: 0.1174 - val_mse: 0.1174\n",
      "Epoch 86/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1298 - mse: 0.1298\n",
      "Epoch 86: loss did not improve from 0.11703\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.1298 - mse: 0.1298 - val_loss: 0.1026 - val_mse: 0.1026\n",
      "Epoch 87/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1128 - mse: 0.1128\n",
      "Epoch 87: loss improved from 0.11703 to 0.11276, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 29s 729ms/step - loss: 0.1128 - mse: 0.1128 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "Epoch 88/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1056 - mse: 0.1056\n",
      "Epoch 88: loss improved from 0.11276 to 0.10564, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 554ms/step - loss: 0.1056 - mse: 0.1056 - val_loss: 0.1176 - val_mse: 0.1176\n",
      "Epoch 89/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1063 - mse: 0.1063\n",
      "Epoch 89: loss did not improve from 0.10564\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.1063 - mse: 0.1063 - val_loss: 0.0867 - val_mse: 0.0867\n",
      "Epoch 90/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1103 - mse: 0.1103\n",
      "Epoch 90: loss did not improve from 0.10564\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.1103 - mse: 0.1103 - val_loss: 0.1703 - val_mse: 0.1703\n",
      "Epoch 91/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0983 - mse: 0.0983\n",
      "Epoch 91: loss improved from 0.10564 to 0.09827, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 550ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0904 - val_mse: 0.0904\n",
      "Epoch 92/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1038 - mse: 0.1038\n",
      "Epoch 92: loss did not improve from 0.09827\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.1038 - mse: 0.1038 - val_loss: 0.1006 - val_mse: 0.1006\n",
      "Epoch 93/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1029 - mse: 0.1029\n",
      "Epoch 93: loss did not improve from 0.09827\n",
      "40/40 [==============================] - 21s 515ms/step - loss: 0.1029 - mse: 0.1029 - val_loss: 0.0951 - val_mse: 0.0951\n",
      "Epoch 94/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0913 - mse: 0.0913\n",
      "Epoch 94: loss improved from 0.09827 to 0.09131, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 555ms/step - loss: 0.0913 - mse: 0.0913 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 95/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0862 - mse: 0.0862\n",
      "Epoch 95: loss improved from 0.09131 to 0.08619, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 565ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.0995 - val_mse: 0.0995\n",
      "Epoch 96/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0849 - mse: 0.0849\n",
      "Epoch 96: loss improved from 0.08619 to 0.08493, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 553ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.1167 - val_mse: 0.1167\n",
      "Epoch 97/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0933 - mse: 0.0933\n",
      "Epoch 97: loss did not improve from 0.08493\n",
      "40/40 [==============================] - 21s 521ms/step - loss: 0.0933 - mse: 0.0933 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 98/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0873 - mse: 0.0873\n",
      "Epoch 98: loss did not improve from 0.08493\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.1059 - val_mse: 0.1059\n",
      "Epoch 99/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0770 - mse: 0.0770\n",
      "Epoch 99: loss improved from 0.08493 to 0.07697, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 553ms/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.1086 - val_mse: 0.1086\n",
      "Epoch 100/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0800 - mse: 0.0800\n",
      "Epoch 100: loss did not improve from 0.07697\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0800 - mse: 0.0800 - val_loss: 0.1079 - val_mse: 0.1079\n",
      "Epoch 101/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0884 - mse: 0.0884\n",
      "Epoch 101: loss did not improve from 0.07697\n",
      "40/40 [==============================] - 21s 515ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1359 - val_mse: 0.1359\n",
      "Epoch 102/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0787 - mse: 0.0787\n",
      "Epoch 102: loss did not improve from 0.07697\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0787 - mse: 0.0787 - val_loss: 0.0972 - val_mse: 0.0972\n",
      "Epoch 103/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0741 - mse: 0.0741\n",
      "Epoch 103: loss improved from 0.07697 to 0.07414, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 557ms/step - loss: 0.0741 - mse: 0.0741 - val_loss: 0.0933 - val_mse: 0.0933\n",
      "Epoch 104/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0791 - mse: 0.0791\n",
      "Epoch 104: loss did not improve from 0.07414\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0791 - mse: 0.0791 - val_loss: 0.1019 - val_mse: 0.1019\n",
      "Epoch 105/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0763 - mse: 0.0763\n",
      "Epoch 105: loss did not improve from 0.07414\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0763 - mse: 0.0763 - val_loss: 0.0900 - val_mse: 0.0900\n",
      "Epoch 106/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0733 - mse: 0.0733\n",
      "Epoch 106: loss improved from 0.07414 to 0.07331, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 552ms/step - loss: 0.0733 - mse: 0.0733 - val_loss: 0.0944 - val_mse: 0.0944\n",
      "Epoch 107/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0730 - mse: 0.0730\n",
      "Epoch 107: loss improved from 0.07331 to 0.07304, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 554ms/step - loss: 0.0730 - mse: 0.0730 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 108/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0767 - mse: 0.0767\n",
      "Epoch 108: loss did not improve from 0.07304\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0767 - mse: 0.0767 - val_loss: 0.1343 - val_mse: 0.1343\n",
      "Epoch 109/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0854 - mse: 0.0854\n",
      "Epoch 109: loss did not improve from 0.07304\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0854 - mse: 0.0854 - val_loss: 0.1042 - val_mse: 0.1042\n",
      "Epoch 110/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0713 - mse: 0.0713\n",
      "Epoch 110: loss improved from 0.07304 to 0.07132, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 552ms/step - loss: 0.0713 - mse: 0.0713 - val_loss: 0.1162 - val_mse: 0.1162\n",
      "Epoch 111/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0703 - mse: 0.0703\n",
      "Epoch 111: loss improved from 0.07132 to 0.07028, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 554ms/step - loss: 0.0703 - mse: 0.0703 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 112/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0685 - mse: 0.0685\n",
      "Epoch 112: loss improved from 0.07028 to 0.06848, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 552ms/step - loss: 0.0685 - mse: 0.0685 - val_loss: 0.1727 - val_mse: 0.1727\n",
      "Epoch 113/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0676 - mse: 0.0676\n",
      "Epoch 113: loss improved from 0.06848 to 0.06757, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 548ms/step - loss: 0.0676 - mse: 0.0676 - val_loss: 0.1571 - val_mse: 0.1571\n",
      "Epoch 114/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0693 - mse: 0.0693\n",
      "Epoch 114: loss did not improve from 0.06757\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0693 - mse: 0.0693 - val_loss: 0.1121 - val_mse: 0.1121\n",
      "Epoch 115/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0683 - mse: 0.0683\n",
      "Epoch 115: loss did not improve from 0.06757\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0683 - mse: 0.0683 - val_loss: 0.0982 - val_mse: 0.0982\n",
      "Epoch 116/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0688 - mse: 0.0688\n",
      "Epoch 116: loss did not improve from 0.06757\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0688 - mse: 0.0688 - val_loss: 0.1057 - val_mse: 0.1057\n",
      "Epoch 117/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0638 - mse: 0.0638\n",
      "Epoch 117: loss improved from 0.06757 to 0.06380, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 552ms/step - loss: 0.0638 - mse: 0.0638 - val_loss: 0.1137 - val_mse: 0.1137\n",
      "Epoch 118/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0614 - mse: 0.0614\n",
      "Epoch 118: loss improved from 0.06380 to 0.06139, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 551ms/step - loss: 0.0614 - mse: 0.0614 - val_loss: 0.1172 - val_mse: 0.1172\n",
      "Epoch 119/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0594 - mse: 0.0594\n",
      "Epoch 119: loss improved from 0.06139 to 0.05944, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 554ms/step - loss: 0.0594 - mse: 0.0594 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 120/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0615 - mse: 0.0615\n",
      "Epoch 120: loss did not improve from 0.05944\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0615 - mse: 0.0615 - val_loss: 0.0944 - val_mse: 0.0944\n",
      "Epoch 121/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0615 - mse: 0.0615\n",
      "Epoch 121: loss did not improve from 0.05944\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0615 - mse: 0.0615 - val_loss: 0.1259 - val_mse: 0.1259\n",
      "Epoch 122/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0578 - mse: 0.0578\n",
      "Epoch 122: loss improved from 0.05944 to 0.05781, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 546ms/step - loss: 0.0578 - mse: 0.0578 - val_loss: 0.0896 - val_mse: 0.0896\n",
      "Epoch 123/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0545 - mse: 0.0545\n",
      "Epoch 123: loss improved from 0.05781 to 0.05448, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 549ms/step - loss: 0.0545 - mse: 0.0545 - val_loss: 0.1055 - val_mse: 0.1055\n",
      "Epoch 124/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0559 - mse: 0.0559\n",
      "Epoch 124: loss did not improve from 0.05448\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0559 - mse: 0.0559 - val_loss: 0.1062 - val_mse: 0.1062\n",
      "Epoch 125/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0584 - mse: 0.0584\n",
      "Epoch 125: loss did not improve from 0.05448\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0584 - mse: 0.0584 - val_loss: 0.0946 - val_mse: 0.0946\n",
      "Epoch 126/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0591 - mse: 0.0591\n",
      "Epoch 126: loss did not improve from 0.05448\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0591 - mse: 0.0591 - val_loss: 0.0929 - val_mse: 0.0929\n",
      "Epoch 127/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0642 - mse: 0.0642\n",
      "Epoch 127: loss did not improve from 0.05448\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0642 - mse: 0.0642 - val_loss: 0.1011 - val_mse: 0.1011\n",
      "Epoch 128/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0568 - mse: 0.0568\n",
      "Epoch 128: loss did not improve from 0.05448\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0568 - mse: 0.0568 - val_loss: 0.0884 - val_mse: 0.0884\n",
      "Epoch 129/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0527 - mse: 0.0527\n",
      "Epoch 129: loss improved from 0.05448 to 0.05274, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 551ms/step - loss: 0.0527 - mse: 0.0527 - val_loss: 0.0886 - val_mse: 0.0886\n",
      "Epoch 130/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0522 - mse: 0.0522\n",
      "Epoch 130: loss improved from 0.05274 to 0.05222, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 555ms/step - loss: 0.0522 - mse: 0.0522 - val_loss: 0.0989 - val_mse: 0.0989\n",
      "Epoch 131/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0527 - mse: 0.0527\n",
      "Epoch 131: loss did not improve from 0.05222\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0527 - mse: 0.0527 - val_loss: 0.0990 - val_mse: 0.0990\n",
      "Epoch 132/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0485 - mse: 0.0485\n",
      "Epoch 132: loss improved from 0.05222 to 0.04854, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 553ms/step - loss: 0.0485 - mse: 0.0485 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 133/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 133: loss did not improve from 0.04854\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0914 - val_mse: 0.0914\n",
      "Epoch 134/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0497 - mse: 0.0497\n",
      "Epoch 134: loss did not improve from 0.04854\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 135/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0515 - mse: 0.0515\n",
      "Epoch 135: loss did not improve from 0.04854\n",
      "40/40 [==============================] - 21s 515ms/step - loss: 0.0515 - mse: 0.0515 - val_loss: 0.1245 - val_mse: 0.1245\n",
      "Epoch 136/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0507 - mse: 0.0507\n",
      "Epoch 136: loss did not improve from 0.04854\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.1020 - val_mse: 0.1020\n",
      "Epoch 137/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0492 - mse: 0.0492\n",
      "Epoch 137: loss did not improve from 0.04854\n",
      "40/40 [==============================] - 21s 515ms/step - loss: 0.0492 - mse: 0.0492 - val_loss: 0.2031 - val_mse: 0.2031\n",
      "Epoch 138/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 138: loss did not improve from 0.04854\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0537 - mse: 0.0537 - val_loss: 0.1175 - val_mse: 0.1175\n",
      "Epoch 139/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0474 - mse: 0.0474\n",
      "Epoch 139: loss improved from 0.04854 to 0.04740, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 552ms/step - loss: 0.0474 - mse: 0.0474 - val_loss: 0.0907 - val_mse: 0.0907\n",
      "Epoch 140/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0491 - mse: 0.0491\n",
      "Epoch 140: loss did not improve from 0.04740\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 0.1043 - val_mse: 0.1043\n",
      "Epoch 141/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0480 - mse: 0.0480\n",
      "Epoch 141: loss did not improve from 0.04740\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0480 - mse: 0.0480 - val_loss: 0.0928 - val_mse: 0.0928\n",
      "Epoch 142/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0486 - mse: 0.0486\n",
      "Epoch 142: loss did not improve from 0.04740\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0486 - mse: 0.0486 - val_loss: 0.0878 - val_mse: 0.0878\n",
      "Epoch 143/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0458 - mse: 0.0458\n",
      "Epoch 143: loss improved from 0.04740 to 0.04580, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 554ms/step - loss: 0.0458 - mse: 0.0458 - val_loss: 0.0915 - val_mse: 0.0915\n",
      "Epoch 144/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0480 - mse: 0.0480\n",
      "Epoch 144: loss did not improve from 0.04580\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0480 - mse: 0.0480 - val_loss: 0.1006 - val_mse: 0.1006\n",
      "Epoch 145/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0467 - mse: 0.0467\n",
      "Epoch 145: loss did not improve from 0.04580\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0467 - mse: 0.0467 - val_loss: 0.1199 - val_mse: 0.1199\n",
      "Epoch 146/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0482 - mse: 0.0482\n",
      "Epoch 146: loss did not improve from 0.04580\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0482 - mse: 0.0482 - val_loss: 0.0934 - val_mse: 0.0934\n",
      "Epoch 147/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0446 - mse: 0.0446\n",
      "Epoch 147: loss improved from 0.04580 to 0.04455, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 552ms/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.0944 - val_mse: 0.0944\n",
      "Epoch 148/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 148: loss did not improve from 0.04455\n",
      "40/40 [==============================] - 21s 522ms/step - loss: 0.0459 - mse: 0.0459 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 149/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0462 - mse: 0.0462\n",
      "Epoch 149: loss did not improve from 0.04455\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0462 - mse: 0.0462 - val_loss: 0.0998 - val_mse: 0.0998\n",
      "Epoch 150/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 150: loss improved from 0.04455 to 0.04282, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 557ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.1139 - val_mse: 0.1139\n",
      "Epoch 151/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0483 - mse: 0.0483\n",
      "Epoch 151: loss did not improve from 0.04282\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0483 - mse: 0.0483 - val_loss: 0.0960 - val_mse: 0.0960\n",
      "Epoch 152/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
      "Epoch 152: loss improved from 0.04282 to 0.04067, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 555ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0999 - val_mse: 0.0999\n",
      "Epoch 153/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0415 - mse: 0.0415\n",
      "Epoch 153: loss did not improve from 0.04067\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.1055 - val_mse: 0.1055\n",
      "Epoch 154/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0414\n",
      "Epoch 154: loss did not improve from 0.04067\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0912 - val_mse: 0.0912\n",
      "Epoch 155/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
      "Epoch 155: loss did not improve from 0.04067\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 156/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
      "Epoch 156: loss improved from 0.04067 to 0.04003, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 555ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 157/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
      "Epoch 157: loss improved from 0.04003 to 0.03873, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 553ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0896 - val_mse: 0.0896\n",
      "Epoch 158/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
      "Epoch 158: loss improved from 0.03873 to 0.03868, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 546ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0949 - val_mse: 0.0949\n",
      "Epoch 159/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
      "Epoch 159: loss did not improve from 0.03868\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0912 - val_mse: 0.0912\n",
      "Epoch 160/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
      "Epoch 160: loss improved from 0.03868 to 0.03814, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 553ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 161/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
      "Epoch 161: loss improved from 0.03814 to 0.03706, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 551ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0874 - val_mse: 0.0874\n",
      "Epoch 162/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
      "Epoch 162: loss improved from 0.03706 to 0.03686, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 551ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0949 - val_mse: 0.0949\n",
      "Epoch 163/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
      "Epoch 163: loss did not improve from 0.03686\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.2436 - val_mse: 0.2436\n",
      "Epoch 164/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
      "Epoch 164: loss improved from 0.03686 to 0.03674, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 553ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.1078 - val_mse: 0.1078\n",
      "Epoch 165/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
      "Epoch 165: loss did not improve from 0.03674\n",
      "40/40 [==============================] - 21s 519ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0879 - val_mse: 0.0879\n",
      "Epoch 166/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
      "Epoch 166: loss did not improve from 0.03674\n",
      "40/40 [==============================] - 21s 527ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.1537 - val_mse: 0.1537\n",
      "Epoch 167/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
      "Epoch 167: loss did not improve from 0.03674\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0992 - val_mse: 0.0992\n",
      "Epoch 168/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0342 - mse: 0.0342\n",
      "Epoch 168: loss improved from 0.03674 to 0.03419, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 559ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 169/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
      "Epoch 169: loss did not improve from 0.03419\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0980 - val_mse: 0.0980\n",
      "Epoch 170/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0348 - mse: 0.0348\n",
      "Epoch 170: loss did not improve from 0.03419\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.1407 - val_mse: 0.1407\n",
      "Epoch 171/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
      "Epoch 171: loss did not improve from 0.03419\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0894 - val_mse: 0.0894\n",
      "Epoch 172/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0352 - mse: 0.0352\n",
      "Epoch 172: loss did not improve from 0.03419\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.0918 - val_mse: 0.0918\n",
      "Epoch 173/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0331 - mse: 0.0331\n",
      "Epoch 173: loss improved from 0.03419 to 0.03312, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 554ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.0974 - val_mse: 0.0974\n",
      "Epoch 174/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0335 - mse: 0.0335\n",
      "Epoch 174: loss did not improve from 0.03312\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 175/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0355 - mse: 0.0355\n",
      "Epoch 175: loss did not improve from 0.03312\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "Epoch 176/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0341 - mse: 0.0341\n",
      "Epoch 176: loss did not improve from 0.03312\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.1020 - val_mse: 0.1020\n",
      "Epoch 177/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0340 - mse: 0.0340\n",
      "Epoch 177: loss did not improve from 0.03312\n",
      "40/40 [==============================] - 21s 518ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.0885 - val_mse: 0.0885\n",
      "Epoch 178/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0329 - mse: 0.0329\n",
      "Epoch 178: loss improved from 0.03312 to 0.03291, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 555ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0901 - val_mse: 0.0901\n",
      "Epoch 179/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0302 - mse: 0.0302\n",
      "Epoch 179: loss improved from 0.03291 to 0.03017, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 553ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0877 - val_mse: 0.0877\n",
      "Epoch 180/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0331 - mse: 0.0331\n",
      "Epoch 180: loss did not improve from 0.03017\n",
      "40/40 [==============================] - 21s 517ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.1041 - val_mse: 0.1041\n",
      "Epoch 181/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0331 - mse: 0.0331\n",
      "Epoch 181: loss did not improve from 0.03017\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.0883 - val_mse: 0.0883\n",
      "Epoch 182/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0329 - mse: 0.0329\n",
      "Epoch 182: loss did not improve from 0.03017\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0950 - val_mse: 0.0950\n",
      "Epoch 183/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0300 - mse: 0.0300\n",
      "Epoch 183: loss improved from 0.03017 to 0.03002, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 22s 563ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0886 - val_mse: 0.0886\n",
      "Epoch 184/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0301 - mse: 0.0301\n",
      "Epoch 184: loss did not improve from 0.03002\n",
      "40/40 [==============================] - 21s 516ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0871 - val_mse: 0.0871\n",
      "Epoch 185/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0300 - mse: 0.0300\n",
      "Epoch 185: loss did not improve from 0.03002\n",
      "40/40 [==============================] - 25s 641ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0855 - val_mse: 0.0855\n",
      "Epoch 186/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0286 - mse: 0.0286\n",
      "Epoch 186: loss improved from 0.03002 to 0.02859, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.1061 - val_mse: 0.1061\n",
      "Epoch 187/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0287 - mse: 0.0287\n",
      "Epoch 187: loss did not improve from 0.02859\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0964 - val_mse: 0.0964\n",
      "Epoch 188/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0283 - mse: 0.0283\n",
      "Epoch 188: loss improved from 0.02859 to 0.02828, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 189/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0273 - mse: 0.0273\n",
      "Epoch 189: loss improved from 0.02828 to 0.02734, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0995 - val_mse: 0.0995\n",
      "Epoch 190/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0272 - mse: 0.0272\n",
      "Epoch 190: loss improved from 0.02734 to 0.02725, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0915 - val_mse: 0.0915\n",
      "Epoch 191/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0273 - mse: 0.0273\n",
      "Epoch 191: loss did not improve from 0.02725\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 192/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0262 - mse: 0.0262\n",
      "Epoch 192: loss improved from 0.02725 to 0.02621, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1434 - val_mse: 0.1434\n",
      "Epoch 193/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0274 - mse: 0.0274\n",
      "Epoch 193: loss did not improve from 0.02621\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1034 - val_mse: 0.1034\n",
      "Epoch 194/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0274 - mse: 0.0274\n",
      "Epoch 194: loss did not improve from 0.02621\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1128 - val_mse: 0.1128\n",
      "Epoch 195/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0267 - mse: 0.0267\n",
      "Epoch 195: loss did not improve from 0.02621\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0923 - val_mse: 0.0923\n",
      "Epoch 196/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0257 - mse: 0.0257\n",
      "Epoch 196: loss improved from 0.02621 to 0.02567, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 43s 1s/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 197/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0250 - mse: 0.0250\n",
      "Epoch 197: loss improved from 0.02567 to 0.02504, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.1058 - val_mse: 0.1058\n",
      "Epoch 198/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0253 - mse: 0.0253\n",
      "Epoch 198: loss did not improve from 0.02504\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0943 - val_mse: 0.0943\n",
      "Epoch 199/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0242 - mse: 0.0242\n",
      "Epoch 199: loss improved from 0.02504 to 0.02424, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 43s 1s/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0912 - val_mse: 0.0912\n",
      "Epoch 200/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0263 - mse: 0.0263\n",
      "Epoch 200: loss did not improve from 0.02424\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 201/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0251 - mse: 0.0251\n",
      "Epoch 201: loss did not improve from 0.02424\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0951 - val_mse: 0.0951\n",
      "Epoch 202/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0261 - mse: 0.0261\n",
      "Epoch 202: loss did not improve from 0.02424\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0973 - val_mse: 0.0973\n",
      "Epoch 203/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0253 - mse: 0.0253\n",
      "Epoch 203: loss did not improve from 0.02424\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0980 - val_mse: 0.0980\n",
      "Epoch 204/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0230 - mse: 0.0230\n",
      "Epoch 204: loss improved from 0.02424 to 0.02302, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0946 - val_mse: 0.0946\n",
      "Epoch 205/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0241 - mse: 0.0241\n",
      "Epoch 205: loss did not improve from 0.02302\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0917 - val_mse: 0.0917\n",
      "Epoch 206/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0241 - mse: 0.0241\n",
      "Epoch 206: loss did not improve from 0.02302\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0979 - val_mse: 0.0979\n",
      "Epoch 207/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0239 - mse: 0.0239\n",
      "Epoch 207: loss did not improve from 0.02302\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.0874 - val_mse: 0.0874\n",
      "Epoch 208/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0239 - mse: 0.0239\n",
      "Epoch 208: loss did not improve from 0.02302\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.0913 - val_mse: 0.0913\n",
      "Epoch 209/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0237 - mse: 0.0237\n",
      "Epoch 209: loss did not improve from 0.02302\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0888 - val_mse: 0.0888\n",
      "Epoch 210/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0249 - mse: 0.0249\n",
      "Epoch 210: loss did not improve from 0.02302\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0988 - val_mse: 0.0988\n",
      "Epoch 211/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0237 - mse: 0.0237\n",
      "Epoch 211: loss did not improve from 0.02302\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0946 - val_mse: 0.0946\n",
      "Epoch 212/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0242 - mse: 0.0242\n",
      "Epoch 212: loss did not improve from 0.02302\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0949 - val_mse: 0.0949\n",
      "Epoch 213/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0219 - mse: 0.0219\n",
      "Epoch 213: loss improved from 0.02302 to 0.02194, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 214/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0238 - mse: 0.0238\n",
      "Epoch 214: loss did not improve from 0.02194\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.0912 - val_mse: 0.0912\n",
      "Epoch 215/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0243 - mse: 0.0243\n",
      "Epoch 215: loss did not improve from 0.02194\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0893 - val_mse: 0.0893\n",
      "Epoch 216/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0235 - mse: 0.0235\n",
      "Epoch 216: loss did not improve from 0.02194\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.1040 - val_mse: 0.1040\n",
      "Epoch 217/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0202 - mse: 0.0202\n",
      "Epoch 217: loss improved from 0.02194 to 0.02022, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0857 - val_mse: 0.0857\n",
      "Epoch 218/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0207 - mse: 0.0207\n",
      "Epoch 218: loss did not improve from 0.02022\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0968 - val_mse: 0.0968\n",
      "Epoch 219/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0201 - mse: 0.0201\n",
      "Epoch 219: loss improved from 0.02022 to 0.02006, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 43s 1s/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "Epoch 220/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0202 - mse: 0.0202\n",
      "Epoch 220: loss did not improve from 0.02006\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0904 - val_mse: 0.0904\n",
      "Epoch 221/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0191 - mse: 0.0191\n",
      "Epoch 221: loss improved from 0.02006 to 0.01915, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 222/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0200 - mse: 0.0200\n",
      "Epoch 222: loss did not improve from 0.01915\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0999 - val_mse: 0.0999\n",
      "Epoch 223/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 223: loss did not improve from 0.01915\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 224/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0200 - mse: 0.0200\n",
      "Epoch 224: loss did not improve from 0.01915\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 225/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 225: loss did not improve from 0.01915\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.1088 - val_mse: 0.1088\n",
      "Epoch 226/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0204 - mse: 0.0204\n",
      "Epoch 226: loss did not improve from 0.01915\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0881 - val_mse: 0.0881\n",
      "Epoch 227/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0200 - mse: 0.0200\n",
      "Epoch 227: loss did not improve from 0.01915\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0931 - val_mse: 0.0931\n",
      "Epoch 228/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0188 - mse: 0.0188\n",
      "Epoch 228: loss improved from 0.01915 to 0.01878, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0885 - val_mse: 0.0885\n",
      "Epoch 229/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0182 - mse: 0.0182\n",
      "Epoch 229: loss improved from 0.01878 to 0.01820, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0867 - val_mse: 0.0867\n",
      "Epoch 230/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0180 - mse: 0.0180\n",
      "Epoch 230: loss improved from 0.01820 to 0.01804, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0916 - val_mse: 0.0916\n",
      "Epoch 231/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 231: loss did not improve from 0.01804\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0904 - val_mse: 0.0904\n",
      "Epoch 232/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 232: loss improved from 0.01804 to 0.01793, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 43s 1s/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.1024 - val_mse: 0.1024\n",
      "Epoch 233/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0187 - mse: 0.0187\n",
      "Epoch 233: loss did not improve from 0.01793\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0875 - val_mse: 0.0875\n",
      "Epoch 234/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0189 - mse: 0.0189\n",
      "Epoch 234: loss did not improve from 0.01793\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0932 - val_mse: 0.0932\n",
      "Epoch 235/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 235: loss improved from 0.01793 to 0.01788, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.1044 - val_mse: 0.1044\n",
      "Epoch 236/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 236: loss improved from 0.01788 to 0.01726, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.1111 - val_mse: 0.1111\n",
      "Epoch 237/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0188 - mse: 0.0188\n",
      "Epoch 237: loss did not improve from 0.01726\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 238/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0165 - mse: 0.0165\n",
      "Epoch 238: loss improved from 0.01726 to 0.01646, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0943 - val_mse: 0.0943\n",
      "Epoch 239/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0160 - mse: 0.0160\n",
      "Epoch 239: loss improved from 0.01646 to 0.01596, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0928 - val_mse: 0.0928\n",
      "Epoch 240/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0183 - mse: 0.0183\n",
      "Epoch 240: loss did not improve from 0.01596\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0899 - val_mse: 0.0899\n",
      "Epoch 241/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0185 - mse: 0.0185\n",
      "Epoch 241: loss did not improve from 0.01596\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0863 - val_mse: 0.0863\n",
      "Epoch 242/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0181 - mse: 0.0181\n",
      "Epoch 242: loss did not improve from 0.01596\n",
      "40/40 [==============================] - 40s 1s/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0971 - val_mse: 0.0971\n",
      "Epoch 243/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 243: loss did not improve from 0.01596\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.1102 - val_mse: 0.1102\n",
      "Epoch 244/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0159 - mse: 0.0159\n",
      "Epoch 244: loss improved from 0.01596 to 0.01587, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0971 - val_mse: 0.0971\n",
      "Epoch 245/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0155 - mse: 0.0155\n",
      "Epoch 245: loss improved from 0.01587 to 0.01555, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.1001 - val_mse: 0.1001\n",
      "Epoch 246/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0153 - mse: 0.0153\n",
      "Epoch 246: loss improved from 0.01555 to 0.01534, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0901 - val_mse: 0.0901\n",
      "Epoch 247/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0158 - mse: 0.0158\n",
      "Epoch 247: loss did not improve from 0.01534\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0887 - val_mse: 0.0887\n",
      "Epoch 248/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0151 - mse: 0.0151\n",
      "Epoch 248: loss improved from 0.01534 to 0.01513, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0931 - val_mse: 0.0931\n",
      "Epoch 249/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0162 - mse: 0.0162\n",
      "Epoch 249: loss did not improve from 0.01513\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 250/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0157 - mse: 0.0157\n",
      "Epoch 250: loss did not improve from 0.01513\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0865 - val_mse: 0.0865\n",
      "Epoch 251/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0157 - mse: 0.0157\n",
      "Epoch 251: loss did not improve from 0.01513\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.1000 - val_mse: 0.1000\n",
      "Epoch 252/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0163 - mse: 0.0163\n",
      "Epoch 252: loss did not improve from 0.01513\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0969 - val_mse: 0.0969\n",
      "Epoch 253/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0151 - mse: 0.0151\n",
      "Epoch 253: loss did not improve from 0.01513\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0891 - val_mse: 0.0891\n",
      "Epoch 254/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0141 - mse: 0.0141\n",
      "Epoch 254: loss improved from 0.01513 to 0.01405, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0883 - val_mse: 0.0883\n",
      "Epoch 255/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0143 - mse: 0.0143\n",
      "Epoch 255: loss did not improve from 0.01405\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0872 - val_mse: 0.0872\n",
      "Epoch 256/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0139 - mse: 0.0139\n",
      "Epoch 256: loss improved from 0.01405 to 0.01386, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0900 - val_mse: 0.0900\n",
      "Epoch 257/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0141 - mse: 0.0141\n",
      "Epoch 257: loss did not improve from 0.01386\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0875 - val_mse: 0.0875\n",
      "Epoch 258/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0168 - mse: 0.0168\n",
      "Epoch 258: loss did not improve from 0.01386\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0870 - val_mse: 0.0870\n",
      "Epoch 259/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0153 - mse: 0.0153\n",
      "Epoch 259: loss did not improve from 0.01386\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.1073 - val_mse: 0.1073\n",
      "Epoch 260/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0142 - mse: 0.0142\n",
      "Epoch 260: loss did not improve from 0.01386\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 261/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0129 - mse: 0.0129\n",
      "Epoch 261: loss improved from 0.01386 to 0.01290, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0841 - val_mse: 0.0841\n",
      "Epoch 262/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0131 - mse: 0.0131\n",
      "Epoch 262: loss did not improve from 0.01290\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 263/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0130 - mse: 0.0130\n",
      "Epoch 263: loss did not improve from 0.01290\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0866 - val_mse: 0.0866\n",
      "Epoch 264/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0125 - mse: 0.0125\n",
      "Epoch 264: loss improved from 0.01290 to 0.01245, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "Epoch 265/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0127 - mse: 0.0127\n",
      "Epoch 265: loss did not improve from 0.01245\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0992 - val_mse: 0.0992\n",
      "Epoch 266/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0126 - mse: 0.0126\n",
      "Epoch 266: loss did not improve from 0.01245\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.1092 - val_mse: 0.1092\n",
      "Epoch 267/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0128 - mse: 0.0128\n",
      "Epoch 267: loss did not improve from 0.01245\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 268/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0133 - mse: 0.0133\n",
      "Epoch 268: loss did not improve from 0.01245\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0886 - val_mse: 0.0886\n",
      "Epoch 269/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0115 - mse: 0.0115\n",
      "Epoch 269: loss improved from 0.01245 to 0.01153, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 43s 1s/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.1072 - val_mse: 0.1072\n",
      "Epoch 270/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0123 - mse: 0.0123\n",
      "Epoch 270: loss did not improve from 0.01153\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0911 - val_mse: 0.0911\n",
      "Epoch 271/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0124 - mse: 0.0124\n",
      "Epoch 271: loss did not improve from 0.01153\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0912 - val_mse: 0.0912\n",
      "Epoch 272/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0135 - mse: 0.0135\n",
      "Epoch 272: loss did not improve from 0.01153\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.1009 - val_mse: 0.1009\n",
      "Epoch 273/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0125 - mse: 0.0125\n",
      "Epoch 273: loss did not improve from 0.01153\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.1049 - val_mse: 0.1049\n",
      "Epoch 274/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0124 - mse: 0.0124\n",
      "Epoch 274: loss did not improve from 0.01153\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0913 - val_mse: 0.0913\n",
      "Epoch 275/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0119 - mse: 0.0119\n",
      "Epoch 275: loss did not improve from 0.01153\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0970 - val_mse: 0.0970\n",
      "Epoch 276/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0120 - mse: 0.0120\n",
      "Epoch 276: loss did not improve from 0.01153\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0902 - val_mse: 0.0902\n",
      "Epoch 277/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0115 - mse: 0.0115\n",
      "Epoch 277: loss improved from 0.01153 to 0.01152, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 43s 1s/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.1012 - val_mse: 0.1012\n",
      "Epoch 278/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0111 - mse: 0.0111\n",
      "Epoch 278: loss improved from 0.01152 to 0.01110, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0870 - val_mse: 0.0870\n",
      "Epoch 279/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0103 - mse: 0.0103\n",
      "Epoch 279: loss improved from 0.01110 to 0.01032, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0874 - val_mse: 0.0874\n",
      "Epoch 280/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0112 - mse: 0.0112\n",
      "Epoch 280: loss did not improve from 0.01032\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0879 - val_mse: 0.0879\n",
      "Epoch 281/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0112 - mse: 0.0112\n",
      "Epoch 281: loss did not improve from 0.01032\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0918 - val_mse: 0.0918\n",
      "Epoch 282/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0111 - mse: 0.0111\n",
      "Epoch 282: loss did not improve from 0.01032\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0898 - val_mse: 0.0898\n",
      "Epoch 283/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0109 - mse: 0.0109\n",
      "Epoch 283: loss did not improve from 0.01032\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0884 - val_mse: 0.0884\n",
      "Epoch 284/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0103 - mse: 0.0103\n",
      "Epoch 284: loss improved from 0.01032 to 0.01030, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0858 - val_mse: 0.0858\n",
      "Epoch 285/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0109 - mse: 0.0109\n",
      "Epoch 285: loss did not improve from 0.01030\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0868 - val_mse: 0.0868\n",
      "Epoch 286/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0110 - mse: 0.0110\n",
      "Epoch 286: loss did not improve from 0.01030\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0866 - val_mse: 0.0866\n",
      "Epoch 287/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0103 - mse: 0.0103\n",
      "Epoch 287: loss did not improve from 0.01030\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0943 - val_mse: 0.0943\n",
      "Epoch 288/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0108 - mse: 0.0108\n",
      "Epoch 288: loss did not improve from 0.01030\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0822 - val_mse: 0.0822\n",
      "Epoch 289/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0102 - mse: 0.0102\n",
      "Epoch 289: loss improved from 0.01030 to 0.01024, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0825 - val_mse: 0.0825\n",
      "Epoch 290/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0112 - mse: 0.0112\n",
      "Epoch 290: loss did not improve from 0.01024\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0999 - val_mse: 0.0999\n",
      "Epoch 291/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0098 - mse: 0.0098\n",
      "Epoch 291: loss improved from 0.01024 to 0.00978, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0876 - val_mse: 0.0876\n",
      "Epoch 292/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0095 - mse: 0.0095\n",
      "Epoch 292: loss improved from 0.00978 to 0.00946, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "Epoch 293/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0089 - mse: 0.0089\n",
      "Epoch 293: loss improved from 0.00946 to 0.00892, saving model to /data2/models/inception_resnet_2_64x64_small_v2/checkpoints/cp.ckpt\n",
      "40/40 [==============================] - 44s 1s/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0861 - val_mse: 0.0861\n",
      "Epoch 294/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0097 - mse: 0.0097\n",
      "Epoch 294: loss did not improve from 0.00892\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0899 - val_mse: 0.0899\n",
      "Epoch 295/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0094 - mse: 0.0094\n",
      "Epoch 295: loss did not improve from 0.00892\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0895 - val_mse: 0.0895\n",
      "Epoch 296/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0111 - mse: 0.0111\n",
      "Epoch 296: loss did not improve from 0.00892\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0933 - val_mse: 0.0933\n",
      "Epoch 297/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0112 - mse: 0.0112\n",
      "Epoch 297: loss did not improve from 0.00892\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0860 - val_mse: 0.0860\n",
      "Epoch 298/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0100 - mse: 0.0100\n",
      "Epoch 298: loss did not improve from 0.00892\n",
      "40/40 [==============================] - 42s 1s/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0852 - val_mse: 0.0852\n",
      "Epoch 299/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0097 - mse: 0.0097\n",
      "Epoch 299: loss did not improve from 0.00892\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0914 - val_mse: 0.0914\n",
      "Epoch 300/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0091 - mse: 0.0091\n",
      "Epoch 300: loss did not improve from 0.00892\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0828 - val_mse: 0.0828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d61bbaf20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_gen, batch_size=hparams['batch_size'], epochs=hparams['num_epochs'], shuffle=True, verbose=1, validation_data=val_gen, callbacks=[tensorboard_callback, model_checkpoint_callback, hparam_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec5e631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f4d50b0fee0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_filepath) # model might be overfitting. train mse is lowering but val mse fluctuates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f37d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 292ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92956d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(TEST_PATH, 'r') as file:\n",
    "    y_test = np.asarray(file['specz_redshift'][:])\n",
    "    oid_test = np.asarray(file['object_id'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8cf4741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(np.ravel(pred), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e952dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_point_metrics(pd.Series(np.ravel(pred)), pd.Series(y_test), binned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63a46536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zspec_bin</th>\n",
       "      <th>count</th>\n",
       "      <th>L</th>\n",
       "      <th>bias_bw</th>\n",
       "      <th>bias_conv</th>\n",
       "      <th>scatter_bw</th>\n",
       "      <th>scatter_conv</th>\n",
       "      <th>outlier_bw</th>\n",
       "      <th>outlier_conv</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 4.0]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.256779</td>\n",
       "      <td>0.047844</td>\n",
       "      <td>0.049495</td>\n",
       "      <td>0.088617</td>\n",
       "      <td>0.070514</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.1795</td>\n",
       "      <td>0.092983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    zspec_bin  count         L   bias_bw  bias_conv  scatter_bw  scatter_conv  \\\n",
       "0  (0.0, 4.0]   2000  0.256779  0.047844   0.049495    0.088617      0.070514   \n",
       "\n",
       "   outlier_bw  outlier_conv       mse  \n",
       "0      0.1315        0.1795  0.092983  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1820f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pred, columns=['photoz'])\n",
    "df['specz'] = pd.Series(y_test)\n",
    "df['object_id'] = pd.Series(oid_test)\n",
    "os.makedirs(f'/data2/predictions/{model_name}', exist_ok=True)\n",
    "df.to_csv(f'/data2/predictions/{model_name}/testing_predictions.csv', index=False)\n",
    "metrics.to_csv(f'/data2/predictions/{model_name}/testing_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
