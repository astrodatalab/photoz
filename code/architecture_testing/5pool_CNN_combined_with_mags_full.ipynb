{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9195a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import keras\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from photoz_utils import *\n",
    "from DataMaker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0276b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (5, 127, 127)\n",
    "GB_LIMIT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399daa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(GB_LIMIT*1000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c193e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DENSE_UNITS = 200\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 1000\n",
    "LEARNING_RATE = 0.0001\n",
    "Z_MAX = 4\n",
    "hparams = {\n",
    "    'num_dense_units': NUM_DENSE_UNITS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'z_max': Z_MAX\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5fe813",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = f'/data/HSC/HSC_v6/step2A/127x127/five_band_image127x127_with_metadata_corrected.hdf5'\n",
    "VAL_PATH = f'/data/HSC/HSC_v6/step2A/127x127/five_band_image127x127_with_metadata_corrected_validation.hdf5'\n",
    "TEST_PATH = f'/data/HSC/HSC_v6/step2A/127x127/five_band_image127x127_with_metadata_corrected_testing.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['object_id', 'specz_redshift', 'g_cmodel_mag', 'r_cmodel_mag', 'i_cmodel_mag', 'z_cmodel_mag', 'y_cmodel_mag']\n",
    "with h5py.File(TRAIN_PATH, 'r') as hf:\n",
    "    train_df = pd.DataFrame()\n",
    "    for name in column_names:\n",
    "        train_df[name] = hf[name][:]\n",
    "    train_images = np.asarray(hf['image'][:])\n",
    "    \n",
    "with h5py.File(VAL_PATH, 'r') as hf:\n",
    "    val_df = pd.DataFrame()\n",
    "    for name in column_names:\n",
    "        val_df[name] = hf[name][:]\n",
    "    val_images = np.asarray(hf['image'][:])\n",
    "    \n",
    "with h5py.File(TEST_PATH, 'r') as hf:\n",
    "    test_df = pd.DataFrame()\n",
    "    for name in column_names:\n",
    "        test_df[name] = hf[name][:]\n",
    "    test_images = np.asarray(hf['image'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = ['g_cmodel_mag', 'r_cmodel_mag', 'i_cmodel_mag', 'z_cmodel_mag', 'y_cmodel_mag']\n",
    "X_train, X_val, X_test = train_df[mags], val_df[mags], test_df[mags]\n",
    "y_train, y_val, y_test = train_df['specz_redshift'], val_df['specz_redshift'], test_df['specz_redshift']\n",
    "OID_train, OID_val, OID_test = train_df['object_id'], val_df['object_id'], test_df['object_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now, only run the NN architecture\n",
    "# add callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def calculate_loss(z_photo, z_spec):\n",
    "    \"\"\"\n",
    "    HSC METRIC. Returns an array. Loss is accuracy metric defined by HSC, meant\n",
    "    to capture the effects of bias, scatter, and outlier all in one. This has\n",
    "    uses for both point and density estimation.\n",
    "    z_photo: array\n",
    "        Photometric or predicted redshifts.\n",
    "    z_spec: array\n",
    "        Spectroscopic or actual redshifts.\n",
    "    \"\"\"\n",
    "    dz = delz(z_photo, z_spec)\n",
    "    gamma = 0.15\n",
    "    denominator = 1.0 + K.square(dz/gamma)\n",
    "    loss = 1 - 1.0 / denominator\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b768da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nn = Input(shape=X_train.shape[1:])\n",
    "hidden1 = Dense(hparams['num_dense_units'], activation=\"relu\")(input_nn)\n",
    "hidden2 = Dense(hparams['num_dense_units'], activation=\"relu\")(hidden1)\n",
    "hidden3 = Dense(hparams['num_dense_units'], activation=\"relu\")(hidden2)\n",
    "hidden4 = Dense(hparams['num_dense_units'], activation=\"relu\")(hidden3)\n",
    "\n",
    "input_cnn = Input(shape=(5,127,127))\n",
    "conv1 = Conv2D(32, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(input_cnn)\n",
    "pool1 = MaxPooling2D(pool_size = (2,2), data_format='channels_first')(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=(2,2), activation='tanh', padding='same', data_format='channels_first')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size = (2,2), data_format='channels_first')(conv2)\n",
    "conv3 = Conv2D(128, kernel_size=(2,2), activation='tanh', padding='same', data_format='channels_first')(pool2)\n",
    "pool3 = MaxPooling2D(pool_size = (2,2), data_format='channels_first')(conv3)\n",
    "conv4 = Conv2D(256, kernel_size=(2,2), activation='tanh', padding='same', data_format='channels_first')(pool3)\n",
    "pool4 = MaxPooling2D(pool_size = (2,2), data_format='channels_first')(conv4)\n",
    "conv5 = Conv2D(256, kernel_size=(2,2), activation='tanh', padding='same', data_format='channels_first')(pool4)\n",
    "pool5 = MaxPooling2D(pool_size = (2,2), data_format='channels_first')(conv5)\n",
    "conv6 = Conv2D(512, kernel_size=(3,3),activation='relu', padding='same', data_format='channels_first')(pool5)\n",
    "conv7 = Conv2D(512, kernel_size=(2,2),activation='relu', padding='same', data_format='channels_first')(conv6)\n",
    "flatten = Flatten()(conv7)\n",
    "dense1 = Dense(512, activation='tanh')(flatten)\n",
    "dense2 = Dense(128, activation='tanh')(dense1)\n",
    "dense3 = Dense(32, activation='tanh')(dense2)\n",
    "\n",
    "concat = Concatenate()([hidden4, dense3])\n",
    "distribution_params = Dense(units=2)(concat)\n",
    "output = tfp.layers.IndependentNormal(1)(distribution_params)\n",
    "model = Model(inputs=[input_nn, input_cnn], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b40cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f180343",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=hparams['learning_rate']), loss=calculate_loss, metrics='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83073425",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HSC_v6_NN_neurips_combined_with_5pool_v3'\n",
    "\n",
    "checkpoint_filepath = os.path.join('/models/', model_name)+'/'+model_name\n",
    "log_dir = os.path.join('/logs/', model_name)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='max',\n",
    "    save_freq='epoch',\n",
    "    save_best_only=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "hparam_callback = hp.KerasCallback(log_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d84ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=[X_train, train_images], y=y_train, batch_size=hparams['batch_size'], epochs=hparams['num_epochs'], shuffle=True, verbose=1, validation_data=([X_val, val_images], y_val), callbacks=[tensorboard_callback, model_checkpoint_callback, hparam_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce077753",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_model = model([X_test, test_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# photoz = []\n",
    "# for i in range(0,len(X_test)):\n",
    "#     pred = model([X_test[i], test_images[i]])\n",
    "#     photoz.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_arr = evaluated_model.mean().numpy()\n",
    "std_arr = evaluated_model.stddev().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e76080",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.ravel(mean_arr) # to make ndim = 1\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_point_metrics(pd.Series(y_pred), pd.Series(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e872876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred, columns=['photoz'])\n",
    "df['photz_err'] = np.ravel(std_arr)\n",
    "df['specz'] = y_test\n",
    "df['object_id'] = OID_test\n",
    "if os.path.exists(f'/predictions/{model_name}') == False:\n",
    "    os.makedirs(f'/predictions/{model_name}')\n",
    "    \n",
    "df.to_csv(f'/predictions/{model_name}/testing_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69959e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'predictions/{model_name}') ==  False:\n",
    "    os.makedirs(f'predictions/{model_name}')\n",
    "    \n",
    "metrics.to_csv(f'/predictions/{model_name}/testing_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d522aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 batch_size = 256\n",
    "# v2 batch_size = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
