{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "import random\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from IPython.display import clear_output\n",
    "from scipy.interpolate import interp1d\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_max = 4\n",
    "\n",
    "train_array = pd.read_csv('/mnt/data/HSC/3_model_comparison/training_v1.csv')\n",
    "train_array.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "train_array = np.asarray(train_array)\n",
    "\n",
    "test_array = pd.read_csv('/mnt/data/HSC/3_model_comparison/testing_v1.csv')\n",
    "test_array.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "test_array = np.asarray(train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "OID_test = test_array[:,0]\n",
    "X_test = test_array[:,[12,13,14,15,16]]\n",
    "y_test = test_array[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_array = [4, 3, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnetwork = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total = list(train_array[:,[12,13,14,15,16]])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(2, nnetwork + 2):\n",
    "    training_array_bin = pd.read_csv('/mnt/data/HSC/3_model_comparison/training_set_'+str(sample_array)+'#'+str(i)+'.csv')\n",
    "    training_array_bin.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    training_array_bin = np.asarray(training_array_bin)\n",
    "    X_bin = list(training_array_bin[:,[12,13,14,15,16]])\n",
    "    X_bin = scaler.transform(X_bin)\n",
    "    y_bin = list(training_array_bin[:,2])\n",
    "    X.append(X_bin)\n",
    "    y.append(y_bin)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nensemble = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162070af",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for i in range(0,nensemble):\n",
    "    input_ = tf.keras.layers.Input(shape=X[0].shape[1:])\n",
    "    hidden1 = tf.keras.layers.Dense(200, activation=\"tanh\")(input_)\n",
    "    hidden2 = tf.keras.layers.Dense(200, activation=\"tanh\")(hidden1)\n",
    "    hidden3 = tf.keras.layers.Dense(200, activation=\"tanh\")(hidden2)\n",
    "    hidden4 = tf.keras.layers.Dense(200, activation=\"relu\")(hidden3)\n",
    "    concat = tf.keras.layers.Concatenate()([input_, hidden4])\n",
    "    output = tf.keras.layers.Dense(1)(concat)\n",
    "    model = tf.keras.Model(inputs=[input_], outputs=[output])\n",
    "    model.compile(optimizer='Adam', loss=\"mse\",metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, nensemble):\n",
    "    models[i].fit(X[i],y[i],epochs=50,shuffle = True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "for i in range(0, nensemble):\n",
    "    y_predict_single = models[i].predict(X_test)\n",
    "    y_predict.append(np.transpose(y_predict_single)[0])\n",
    "\n",
    "y_predict = np.transpose(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4708c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_array = []\n",
    "std_array = []\n",
    "for i in range(0,len(X_test)):\n",
    "    mean = np.mean(y_predict[i])\n",
    "    std = np.std(y_predict[i])\n",
    "    mean_array.append(mean)\n",
    "    std_array.append(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972dba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "xy = np.asarray(np.vstack([y_test,mean_array])).astype('float32')\n",
    "z = gaussian_kde(xy)(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 0\n",
    "overlap_array = []\n",
    "for i in range(0,len(X_test)):\n",
    "    if np.abs(y_test[i]-mean_array[i])<=std_array[i]:\n",
    "        overlap += 1\n",
    "        overlap_array.append(1)\n",
    "    else:\n",
    "        overlap_array.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445168a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array = np.transpose(np.vstack((y_test, mean_array, std_array, overlap_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889581b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_combined_array = sorted(combined_array, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e306b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = int(z_max*10)\n",
    "splitted_sorted_combined_array = np.array_split(sorted_combined_array,bins)\n",
    "coverage = []\n",
    "for i in range(0,bins):\n",
    "    bins_count = 0\n",
    "    for j in range(0,len(splitted_sorted_combined_array[i])):\n",
    "        if splitted_sorted_combined_array[i][j][3] == 1:\n",
    "            bins_count += 1\n",
    "    coverage.append(bins_count/len(splitted_sorted_combined_array[i])/0.6827)\n",
    "x_array = np.arange(0,z_max,z_max/bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6eb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Evan's code, didn't completely understand this part\n",
    "def posterior_mean_field(kernel_size: int, bias_size: int, dtype: any) -> tf.keras.Model:\n",
    "    \"\"\"Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\"\"\"\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype, initializer=lambda shape, dtype: random_gaussian_initializer(shape, dtype), trainable=True),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :n],\n",
    "                       scale= + 10e-4*tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "def prior_trainable(kernel_size: int, bias_size: int, dtype: any) -> tf.keras.Model:\n",
    "    \"\"\"Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\"\"\"\n",
    "    n = kernel_size + bias_size\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),  # Returns a trainable variable of shape n, regardless of input\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t, scale=1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "def random_gaussian_initializer(shape, dtype):\n",
    "    n = int(shape / 2)\n",
    "    loc_norm = tf.random_normal_initializer(mean=0., stddev=0.1)\n",
    "    loc = tf.Variable(\n",
    "        initial_value=loc_norm(shape=(n,), dtype=dtype)\n",
    "    )\n",
    "    scale_norm = tf.random_normal_initializer(mean=-3., stddev=0.1)\n",
    "    scale = tf.Variable(\n",
    "        initial_value=scale_norm(shape=(n,), dtype=dtype)\n",
    "    )\n",
    "    return tf.concat([loc, scale], 0)\n",
    "\n",
    "def negative_loglikelihood(targets, estimated_distribution):\n",
    "    return -estimated_distribution.log_prob(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781138dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variational = tf.keras.layers.Input(shape=X[nnetwork - 1].shape[1:])\n",
    "hidden1_variational = tfp.layers.DenseVariational(200, activation='tanh', input_shape=(5,),\n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1 / len(y[nnetwork - 1]))(input_variational)\n",
    "hidden2_variational = tfp.layers.DenseVariational(200, activation='tanh', input_shape=(5,),\n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1 / len(y[nnetwork - 1]))(hidden1_variational)\n",
    "hidden3_variational = tfp.layers.DenseVariational(200, activation='tanh', input_shape=(5,),\n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1 / len(y[nnetwork - 1]))(hidden2_variational)\n",
    "hidden4_variational = tfp.layers.DenseVariational(200, activation='relu', input_shape=(5,),\n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1 / len(y[nnetwork - 1]))(hidden3_variational)   \n",
    "\n",
    "concat_variational = tf.keras.layers.Concatenate()([input_variational, hidden4_variational])\n",
    "\n",
    "distribution_params_variational = tf.keras.layers.Dense(units=2)(concat_variational)\n",
    "\n",
    "output_variational = tfp.layers.IndependentNormal(1)(distribution_params_variational)\n",
    "\n",
    "model_variational = tf.keras.Model(inputs=[input_variational], outputs=[output_variational])\n",
    "\n",
    "model_variational.compile(optimizer='adam', loss=negative_loglikelihood,metrics=[keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variational.fit(X[nnetwork - 1],y[nnetwork - 1], epochs=50, shuffle = True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_variational = model_variational(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77565955",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_variational_mean = np.asarray(y_predict_variational.mean())[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_variational_std = np.asarray(y_predict_variational.stddev())[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_variational = 0\n",
    "overlap_array_variational = []\n",
    "for i in range(0,len(X_test)):\n",
    "    if np.abs(y_test[i]-y_predict_variational_mean[i])<=y_predict_variational_std[i]:\n",
    "        overlap_variational += 1\n",
    "        overlap_array_variational.append(1)\n",
    "    else:\n",
    "        overlap_array_variational.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array_variational = np.transpose(np.vstack((y_test,y_predict_variational_mean,y_predict_variational_std,overlap_array_variational)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600afabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_combined_array_variational = sorted(combined_array_variational, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b576c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_variational = int(z_max*10)\n",
    "splitted_sorted_combined_array_variational = np.array_split(sorted_combined_array_variational,bins)\n",
    "coverage_variational = []\n",
    "for i in range(0,bins_variational):\n",
    "    bins_count_variational = 0\n",
    "    for j in range(0,len(splitted_sorted_combined_array_variational[i])):\n",
    "        if splitted_sorted_combined_array_variational[i][j][3] == 1:\n",
    "            bins_count_variational += 1\n",
    "    coverage_variational.append(bins_count_variational/len(splitted_sorted_combined_array_variational[i])/0.6827)\n",
    "x_array = np.arange(0,z_max,z_max/bins_variational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccb5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(true, predicted):\n",
    "    return (predicted - true)/(1 + true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_array = []\n",
    "for i in range(0,len(y_test)):\n",
    "    bias_array.append(bias(y_test[i],mean_array[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array_scatter = np.transpose(np.vstack((y_test,bias_array)))\n",
    "sorted_combined_array_scatter = sorted(combined_array_scatter, key=lambda x: x[0])\n",
    "bins_scatter = int(z_max*10)\n",
    "splitted_sorted_combined_array_scatter = np.array_split(sorted_combined_array_scatter,bins_scatter)\n",
    "scatter_array = []\n",
    "for i in range(0,bins_scatter):\n",
    "    scatter = np.std(splitted_sorted_combined_array_scatter[i])\n",
    "    scatter_array.append(scatter)\n",
    "x_array_scatter = np.arange(0,z_max,z_max/bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d553c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array_outlier = np.transpose(np.vstack((y_test,bias_array)))\n",
    "sorted_combined_array_outlier = sorted(combined_array_outlier, key=lambda x: x[0])\n",
    "bins_outlier = int(z_max*10)\n",
    "splitted_sorted_combined_array_outlier = np.array_split(sorted_combined_array_outlier,bins_outlier)\n",
    "outlier_array = []\n",
    "for i in range(0,bins_scatter):\n",
    "    outlier_count = 0\n",
    "    for j in range(0,len(splitted_sorted_combined_array_outlier[i])):\n",
    "        if np.abs(splitted_sorted_combined_array_outlier[i][j][1]) >= 1/4:\n",
    "            outlier_count += 1\n",
    "    outlier_array.append(outlier_count/len(X_test))\n",
    "x_array_outlier = np.arange(0,z_max,z_max/bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2edf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_array_variational = []\n",
    "for i in range(0,len(y_test)):\n",
    "    bias_array_variational.append(bias(y_test[i],y_predict_variational_mean[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_array_string = \"\"\n",
    "for i in sample_array:\n",
    "    sample_array_string += str(i)\n",
    "    sample_array_string += \"_\"\n",
    "\n",
    "ensemble_prediction = np.transpose(np.vstack((OID_test, y_test, mean_array, std_array)))\n",
    "bayesian_prediction = np.transpose(np.vstack((OID_test, y_test, y_predict_variational_mean, y_predict_variational_std)))\n",
    "\n",
    "df = pd.DataFrame(ensemble_prediction, columns=['object_id', 'specz_redshift', 'predicted_redshift',\n",
    "       'uncertainty'])\n",
    "df.to_csv('/mnt/data/HSC/3_model_comparison/ensemble_predictions'+str(sample_array)+'_v1.csv')\n",
    "\n",
    "df = pd.DataFrame(bayesian_prediction, columns=['object_id', 'specz_redshift', 'predicted_redshift',\n",
    "       'uncertainty'])\n",
    "df.to_csv('/mnt/data/HSC/3_model_comparison/bayesian_predictions'+str(sample_array)+'_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ccca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "fig, ax = plt.subplots()\n",
    "scatter_plot = ax.scatter(y_test, mean_array, c = z, s = 1, edgecolor = None)\n",
    "plt.colorbar(scatter_plot, label = 'Density')\n",
    "plt.title('Prediction, Ensemble')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('photo z')\n",
    "plt.plot([0, z_max], [0, z_max],color = 'black')\n",
    "plt.xlim([0, z_max])\n",
    "plt.ylim([0, z_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_variational = np.asarray(np.vstack([y_test,y_predict_variational_mean])).astype('float32')\n",
    "z_variational = gaussian_kde(xy_variational)(xy_variational)\n",
    "fig, ax = plt.subplots()\n",
    "scatter_plot = ax.scatter(y_test, y_predict_variational_mean, c = z_variational, s = 1, edgecolor = None)\n",
    "plt.colorbar(scatter_plot, label = 'Density')\n",
    "plt.title('Prediction, Bayesian')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('photo z')\n",
    "plt.plot([0, z_max], [0, z_max],color = 'black')\n",
    "plt.xlim([0, z_max])\n",
    "plt.ylim([0, z_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70168bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "error_plot = ax.errorbar(y_test, mean_array, yerr = std_array, fmt=\"o\" ,markersize=1)\n",
    "plt.title('Prediction with error, Ensemble')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('photo z')\n",
    "plt.plot([0, z_max], [0, z_max],color = 'black')\n",
    "plt.xlim([0, z_max])\n",
    "plt.ylim([0, z_max])\n",
    "print(\"Coverage: \"+str(overlap/len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "error_plot = ax.errorbar(y_test, y_predict_variational_mean, yerr = y_predict_variational_std, fmt=\"o\", markersize=1)\n",
    "plt.title('Prediction with error, Bayesian')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('photo z')\n",
    "plt.plot([0, z_max], [0, z_max],color = 'black')\n",
    "plt.xlim([0, z_max])\n",
    "plt.ylim([0, z_max])\n",
    "print(\"Coverage: \"+str(overlap_variational/len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_array, coverage, c = 'red')\n",
    "plt.title('Coverage, Ensemble')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('coverage')\n",
    "plt.ylim([0,1.5])\n",
    "print(\"Coverage: \"+str(overlap/len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_array, coverage_variational, c = 'red')\n",
    "plt.title('Coverage, Bayesian')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('coverage')\n",
    "plt.ylim([0,1.5])\n",
    "print(\"Coverage: \"+str(overlap_variational/len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_bias = np.asarray(np.vstack([y_test,bias_array])).astype('float32')\n",
    "z_bias = gaussian_kde(xy_bias)(xy_bias)\n",
    "fig, ax = plt.subplots()\n",
    "scatter_plot = ax.scatter(y_test, bias_array, c = z_bias, s = 1, edgecolor = None, label = 'Data')\n",
    "plt.colorbar(scatter_plot, label = 'Density')\n",
    "plt.title('Bias Plot, Ensemble')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('bias')\n",
    "plt.plot([0, z_max], [0, 0], color = 'black')\n",
    "plt.xlim([0, z_max])\n",
    "plt.ylim([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_bias_variational = np.asarray(np.vstack([y_test,bias_array_variational])).astype('float32')\n",
    "z_bias_variational = gaussian_kde(xy_bias_variational)(xy_bias_variational)\n",
    "fig, ax = plt.subplots()\n",
    "scatter_plot = ax.scatter(y_test, bias_array_variational, c = z_bias_variational, s = 1, edgecolor = None, label = 'Data')\n",
    "plt.colorbar(scatter_plot, label = 'Density')\n",
    "plt.title('Bias Plot, Bayesian')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('bias')\n",
    "plt.plot([0, z_max], [0, 0], color = 'black')\n",
    "plt.xlim([0, z_max])\n",
    "plt.ylim([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbee5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array_scatter_variational = np.transpose(np.vstack((y_test,bias_array_variational)))\n",
    "sorted_combined_array_scatter_variational = sorted(combined_array_scatter_variational, key=lambda x: x[0])\n",
    "bins_scatter_variational = int(z_max*10)\n",
    "splitted_sorted_combined_array_scatter_variational = np.array_split(sorted_combined_array_scatter_variational,bins_scatter_variational)\n",
    "scatter_array_variational = []\n",
    "for i in range(0,bins_scatter_variational):\n",
    "    scatter_variational = np.std(splitted_sorted_combined_array_scatter_variational[i])\n",
    "    scatter_array_variational.append(scatter_variational)\n",
    "x_array_scatter = np.arange(0,z_max,z_max/bins)\n",
    "plt.plot(x_array_scatter, scatter_array_variational, c = 'red', label = 'Bayesian')\n",
    "plt.plot(x_array_scatter, scatter_array, c = 'blue', label = 'Ensemble')\n",
    "plt.title('Scatter Ensemble vs. Bayesian')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('scatter')\n",
    "plt.xlim([0,z_max])\n",
    "plt.ylim([0,2])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array_outlier_variational = np.transpose(np.vstack((y_test,bias_array_variational)))\n",
    "sorted_combined_array_outlier_variational = sorted(combined_array_outlier_variational, key=lambda x: x[0])\n",
    "bins_outlier_variational = int(z_max*10)\n",
    "splitted_sorted_combined_array_outlier_variational = np.array_split(sorted_combined_array_outlier_variational,bins_outlier_variational)\n",
    "outlier_array_variational = []\n",
    "for i in range(0,bins_scatter_variational):\n",
    "    outlier_count_variational = 0\n",
    "    for j in range(0,len(splitted_sorted_combined_array_outlier_variational[i])):\n",
    "        if np.abs(splitted_sorted_combined_array_outlier_variational[i][j][1]) >= 1/4:\n",
    "            outlier_count_variational += 1\n",
    "    outlier_array_variational.append(outlier_count_variational/len(X_test))\n",
    "x_array_outlier = np.arange(0,z_max,z_max/bins)\n",
    "plt.plot(x_array_outlier, outlier_array_variational, c = 'red', label = 'Bayesian')\n",
    "plt.plot(x_array_outlier, outlier_array, c = 'blue', label = 'Ensemble')\n",
    "plt.title('Outlier Rate Ensemble vs. Bayesian')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('outlier rate')\n",
    "plt.xlim([0,z_max])\n",
    "plt.ylim([0,1/30])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dce8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
