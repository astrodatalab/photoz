{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e38f09b",
   "metadata": {},
   "source": [
    "# VAE - Faces\n",
    "\n",
    "2022-07-15\n",
    "\n",
    "Zooey Nguyen\n",
    "\n",
    "Autoencoder with variational layer.\n",
    "\n",
    "- v4: add MaxPooling2D\n",
    "- v5: UpSampling2D+Conv2D to replace Conv2DTranspose with corresponding input decoder changes\n",
    "- v6: jk going back but adding strides to all convolutions\n",
    "- v7: increase latent dimensions to 32\n",
    "- v8: try pooling and upsampling for dimension change again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfb8f1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6698743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad0ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/models/UTKFace_VAE_v8/model\n",
      "/models/UTKFace_VAE_v8/checkpoints\n",
      "/logs/UTKFace_VAE_v8\n",
      "/predictions/UTKFace_VAE_v8\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"UTKFace\"\n",
    "MODEL_TYPE = \"VAE\"\n",
    "MODEL_VERSION = \"v8\"\n",
    "\n",
    "model_id = '_'.join([DATASET_NAME, MODEL_TYPE, MODEL_VERSION])\n",
    "dir_model = os.path.join('/models', model_id, 'model')\n",
    "dir_checkpoints = os.path.join('/models', model_id, 'checkpoints')\n",
    "dir_logs = os.path.join('/logs', model_id)\n",
    "dir_predictions = os.path.join('/predictions', model_id)\n",
    "\n",
    "print(dir_model)\n",
    "print(dir_checkpoints)\n",
    "print(dir_logs)\n",
    "print(dir_predictions)\n",
    "\n",
    "os.makedirs(dir_model, exist_ok=True)\n",
    "os.makedirs(dir_checkpoints, exist_ok=True)\n",
    "os.makedirs(dir_logs, exist_ok=True)\n",
    "os.makedirs(dir_predictions, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1111d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 32\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 40\n",
    "GB_LIMIT = 5\n",
    "CHECKPOINTS_TO_SAVE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce73c2d",
   "metadata": {},
   "source": [
    "## Allocate GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b81cd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=GB_LIMIT*1000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a7595d",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Create a shuffled and batched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb94919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import basename\n",
    "from os import listdir\n",
    "import h5py\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "DATA_PATH = \"/data/UTKFace/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da444131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23708 files belonging to 1 classes.\n",
      "Found 23708 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "single_image_gen = image_dataset_from_directory(DATA_PATH,\n",
    "                                         labels=None,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=True\n",
    "                                        )\n",
    "\n",
    "image_gen = image_dataset_from_directory(DATA_PATH,\n",
    "                                         labels=None,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True\n",
    "                                        )\n",
    "\n",
    "image_gen = image_gen.map(lambda x: x/255)\n",
    "single_image_gen = single_image_gen.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20510a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, 256, 256, 3), (None, 256, 256, 3)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen = tf.data.Dataset.zip((image_gen, image_gen))\n",
    "data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccd0b3",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543a1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "logs_callback = TensorBoard(log_dir=dir_logs)\n",
    "weights_callback = ModelCheckpoint(filepath=os.path.join(dir_checkpoints, 'weights_epoch{epoch}.hdf5'),\n",
    "                                   save_freq=int(EPOCHS/CHECKPOINTS_TO_SAVE),\n",
    "                                   save_weights_only=True\n",
    "                                  )\n",
    "LR_callback = ReduceLROnPlateau()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4a907",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e255cf2",
   "metadata": {},
   "source": [
    "## Variational autoencoder model\n",
    "\n",
    "https://keras.io/examples/generative/vae/\n",
    "\n",
    "https://agustinus.kristia.de/techblog/2016/12/17/conditional-vae/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0345b",
   "metadata": {},
   "source": [
    "### Define VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "469b5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Sampling(Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e2b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.initializers import Zeros\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        images = Input(shape=IMAGE_SHAPE)\n",
    "        x = Conv2D(32, kernel_size=3, activation='relu')(images)\n",
    "        x = MaxPooling2D()(x)\n",
    "        x = Conv2D(32, kernel_size=3, activation='relu')(x)\n",
    "        x = MaxPooling2D()(x)\n",
    "        x = Flatten()(x)\n",
    "        mean = Dense(LATENT_DIM, activation='relu')(x)\n",
    "        logvar = Dense(LATENT_DIM, activation='relu', kernel_initializer='zeros')(x)\n",
    "        samples = Sampling()([mean, logvar])\n",
    "        self.encoder = Model(images, samples)\n",
    "\n",
    "        latents = Input(shape=(LATENT_DIM,))\n",
    "        x = Dense(65*65*3)(latents) # don't know why it requires 65 and not 64 (256/4) for this... but ok.\n",
    "        x = Reshape(target_shape=(65,65,3))(x)\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(32, kernel_size=3, activation='relu')(x)\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(32, kernel_size=3, activation='relu')(x)\n",
    "        x = Conv2DTranspose(3, kernel_size=3, activation='relu')(x)\n",
    "        outputs = Reshape(target_shape=IMAGE_SHAPE)(x)\n",
    "        self.decoder = Model(latents, outputs)\n",
    "\n",
    " \n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567a496",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5a121e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "384f653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e9374",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7484564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_gen,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[logs_callback, weights_callback],\n",
    "                    verbose=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9619148",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_path = os.path.join(dir_predictions, \"train_preds.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972146f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_big_number = 100\n",
    "with h5py.File(train_preds_path, 'w') as f:\n",
    "    f.create_dataset('images', (some_big_number,) + IMAGE_SHAPE, maxshape=((None,) + IMAGE_SHAPE))\n",
    "    f.create_dataset('preds', (some_big_number,) + IMAGE_SHAPE, maxshape=((None,) + IMAGE_SHAPE))\n",
    "    f.create_dataset('metrics', (some_big_number,1) , maxshape=(None, 1))\n",
    "    for idx, image in enumerate(iter(single_image_gen)):\n",
    "        if idx == some_big_number:\n",
    "            break\n",
    "        prediction = model.predict(image, verbose=0)\n",
    "        evaluation = model.evaluate(image, verbose=0)\n",
    "        f['images'][idx] = image\n",
    "        f['preds'][idx] = prediction\n",
    "        f['metrics'][idx] = evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55856ed8",
   "metadata": {},
   "source": [
    "## Check out some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_path = os.path.join(dir_predictions, \"train_preds.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 10\n",
    "with h5py.File(train_preds_path, 'r') as f:\n",
    "    for i in range(num_images):\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].imshow(f['images'][i])\n",
    "        ax[1].imshow(f['preds'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88eaeac",
   "metadata": {},
   "source": [
    "##### Evaluate model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
