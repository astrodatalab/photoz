{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import data set\n",
    "import numpy as np\n",
    "mnist = tf.keras.datasets.mnist\n",
    "import random\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import data set\n",
    "import numpy as np\n",
    "mnist = tf.keras.datasets.mnist\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "tfd = tfp.distributions\n",
    "from photoz_utils import *\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import pandas as pd\n",
    "#import data set\n",
    "import numpy as np\n",
    "mnist = tf.keras.datasets.mnist\n",
    "import random\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "#import photoz data:\n",
    "tfd = tfp.distributions\n",
    "#from google.colab import files\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#photozdata = pd.read_csv('~/Downloads/forced_forced2_spec_z_matched_online.csv')\n",
    "\n",
    "#photozdata = pd.read_csv('~/Downloads/forced_forced2_spec_z_matched_online.csv')\n",
    "#photozdata = pd.read_csv('~/Downloads/trimmed_forced_forced2_spec_z_matched_online.csv')\n",
    "photozdata = pd.read_csv('/mnt/data/HSC/HSC_v6/HSC_v6.csv')\n",
    "#photozdata = pd.read_csv('/data/HSC/HSC_IMAGES_FIXED/HSC_photozdata_full_header_trimmed.csv')\n",
    "\n",
    "print(len(photozdata))\n",
    "filt = (photozdata['specz_redshift'] < 4)\\\n",
    "& (photozdata['specz_redshift'] > 0.01)\\\n",
    "& (photozdata['specz_redshift_err'] > 0) \\\n",
    "& (photozdata['specz_redshift_err'] < 1)\\\n",
    "&(photozdata[\"specz_redshift_err\"]<0.005*(1+photozdata[\"specz_redshift\"]))\\\n",
    "&(photozdata['g_cmodel_mag'] >0)\\\n",
    "&(photozdata['r_cmodel_mag'] >0)\\\n",
    "&(photozdata['i_cmodel_mag'] >0)\\\n",
    "&(photozdata['z_cmodel_mag'] >0)\\\n",
    "&(photozdata['y_cmodel_mag'] >0)\\\n",
    "&(photozdata['g_cmodel_mag'] < 50)\\\n",
    "&(photozdata['r_cmodel_mag'] < 50)\\\n",
    "&(photozdata['i_cmodel_mag'] < 50)\\\n",
    "&(photozdata['z_cmodel_mag'] < 50)\\\n",
    "&(photozdata['y_cmodel_mag'] < 50)\\\n",
    "&(photozdata['specz_flag_homogeneous'] == True)\n",
    "\n",
    "photozdata.drop_duplicates(subset=['object_id'])\n",
    "print(len(photozdata))\n",
    "photozdata_subset = photozdata[filt]\n",
    "print(len(photozdata_subset))\n",
    "\n",
    "photozdata = photozdata_subset\n",
    "spectro_z = np.asarray(photozdata[\"specz_redshift\"])\n",
    "col1 = np.asarray(photozdata[\"g_cmodel_mag\"])\n",
    "col2 = np.asarray(photozdata[\"r_cmodel_mag\"])\n",
    "col3 = np.asarray(photozdata[\"i_cmodel_mag\"])\n",
    "col4 =np.asarray(photozdata[\"z_cmodel_mag\"])\n",
    "col5 = np.asarray(photozdata[\"y_cmodel_mag\"])\n",
    "specz_err = np.asarray(photozdata[\"specz_redshift_err\"])\n",
    "gmag_err = np.asarray(photozdata[\"g_cmodel_magsigma\"])\n",
    "ymag_err = np.asarray(photozdata[\"y_cmodel_magsigma\"])\n",
    "\n",
    "photodata = {'col1':col1,\n",
    "             'col2':col2,\n",
    "             'col3':col3,\n",
    "             'col4':col4,\n",
    "             'col5':col5,\n",
    "             'specz_err': specz_err,\n",
    "             'gmag_err':gmag_err,\n",
    "             'ymag_err':ymag_err,\n",
    "             'specz':spectro_z\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(photodata)\n",
    "photodata = df\n",
    "\n",
    "photodata.replace(-99., np.nan, inplace=True)\n",
    "photodata.replace(-99.9, np.nan, inplace=True)\n",
    "photodata.replace(np.inf, np.nan, inplace=True)\n",
    "photodata= photodata.dropna(how='any')\n",
    "specz_err = photodata['specz_err']\n",
    "spectro_z = photodata['specz']\n",
    "gmag_err = photodata['gmag_err']\n",
    "ymag_err = photodata['ymag_err']\n",
    "photodata = photodata.drop(\"specz\", axis=1)\n",
    "photodata = photodata.drop(\"specz_err\", axis =1)\n",
    "photodata = photodata.drop(\"gmag_err\", axis =1)\n",
    "photodata = photodata.drop(\"ymag_err\", axis =1)\n",
    "\n",
    "print(len(photodata))\n",
    "\n",
    "#normalize\n",
    "photodata = min_max_scaler.fit_transform(photodata)\n",
    "\n",
    "indices = np.arange(len(spectro_z))\n",
    "\n",
    "x_train ,x_test,y_train,y_test, idx_train,idx_test = train_test_split(photodata,spectro_z, indices, test_size=0.20)\n",
    "\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def calculate_loss(z_photo, z_spec):\n",
    "    \"\"\"\n",
    "    HSC METRIC. Returns an array. Loss is accuracy metric defined by HSC, meant\n",
    "    to capture the effects of bias, scatter, and outlier all in one. This has\n",
    "    uses for both point and density estimation.\n",
    "    z_photo: array\n",
    "        Photometric or predicted redshifts.\n",
    "    z_spec: array\n",
    "        Spectroscopic or actual redshifts.\n",
    "    \"\"\"\n",
    "    dz = delz(z_photo, z_spec)\n",
    "    gamma = 0.15\n",
    "    denominator = 1.0 + K.square(dz/gamma)\n",
    "    L = 1 - 1.0 / denominator\n",
    "    return L\n",
    "\n",
    "\n",
    "def calculate_conv_outlier_rate2(z_photo,z_spec):\n",
    "\n",
    "# This function calculate the conventional outlier rate. \n",
    "\n",
    "    outliers = []\n",
    "    outliers_bayesian = []\n",
    "    outlier_index_bayesian = []\n",
    "    outlier_index = []\n",
    "    for i in range(0,len(z_spec)):\n",
    "\n",
    "\n",
    "        #outliers.append((abs(photoz[i] - y_test_original[i]))/(1+y_test_original[i]))\n",
    "        outliers.append((abs(z_photo[i] - z_spec[i]))/(1+z_spec[i]))\n",
    "        if outliers[i] > 0.15:\n",
    "            outlier_index.append(i)\n",
    "    \n",
    "        outliers_bayesian.append((abs(z_photo[i] - z_spec[i]))/(1+z_spec[i]))\n",
    "        if outliers_bayesian[i] > 0.15:\n",
    "            outlier_index_bayesian.append(i)\n",
    "            \n",
    "    outlier_rate_conv = len(outlier_index_bayesian)/len(z_spec) \n",
    "    \n",
    "    return outlier_rate_conv\n",
    "\n",
    "\n",
    "def calculate_bayesian_outlier_rate2(z_photo,z_spec,zpdf_std):\n",
    "\n",
    "# This function calculate the conventional outlier rate. \n",
    "\n",
    "    outliers = []\n",
    "    outliers_bayesian = []\n",
    "    outlier_index_bayesian = []\n",
    "    for i in range(0,len(z_spec)):\n",
    "\n",
    "\n",
    "        outliers_bayesian.append((abs(z_photo[i] - z_spec[i])-zpdf_std[i][0])/(1+z_spec[i]))\n",
    "        if outliers_bayesian[i] > 0.15:\n",
    "            outlier_index_bayesian.append(i)\n",
    "            \n",
    "    bayesian_outlier_rate = len(outlier_index_bayesian)/len(z_spec) \n",
    "    \n",
    "    return bayesian_outlier_rate\n",
    "#bin the redshift:\n",
    "#bin_size = 0.05\n",
    "#bin_size = 0.12\n",
    "\n",
    "#spectro_z = round(spectro_z/bin_size)\n",
    "\n",
    "# training_size = 1000\n",
    "# data = photozdata_total\n",
    "# random.shuffle(data)\n",
    "#\n",
    "# training_set = data[:training_size]\n",
    "# test_data = data[training_size:]\n",
    "\n",
    "indices = np.arange(len(spectro_z))\n",
    "\n",
    "x_train ,x_test,y_train,y_test, idx_train,idx_test = train_test_split(photodata,spectro_z, indices, test_size=0.20)\n",
    "\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5) # \n",
    "\n",
    "    \n",
    "#save train,test,and,evaluation sets\n",
    "    \n",
    "y_train = np.array(y_train)\n",
    "#y_train = np.round(y_train/bin_size)\n",
    "y_test = np.array(y_test)\n",
    "y_test_original = y_test\n",
    "#y_test = np.round(y_test/bin_size)\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "\n",
    "np.save('x_train.npy', x_train) # save\n",
    "np.save('y_train.npy', y_train) # save\n",
    "\n",
    "np.save('x_test.npy', x_test) # save\n",
    "np.save('y_test.npy', y_test) # save\n",
    "\n",
    "np.save('x_validation.npy', x_val) # save\n",
    "np.save('y_validation.npy', y_val) # save\n",
    "\n",
    "\n",
    "\n",
    "#plot N(z) of training and test sets to total dataset to confirm they have the same approximate distribution\n",
    "\n",
    "\n",
    "\n",
    "# #build the model\n",
    "\n",
    "def posterior_mean_field(kernel_size: int, bias_size: int, dtype: any) -> tf.keras.Model:\n",
    "    \"\"\"Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\"\"\"\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype, initializer=lambda shape, dtype: random_gaussian_initializer(shape, dtype), trainable=True),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :n],\n",
    "                       scale= + 10e-4*tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# def posterior_mean_field(kernel_size, bias_size, dtype=None):\n",
    "#     n = kernel_size + bias_size\n",
    "#     posterior_model = keras.Sequential(\n",
    "#         [\n",
    "#             tfp.layers.VariableLayer(\n",
    "#                 tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
    "#             ),\n",
    "#             tfp.layers.MultivariateNormalTriL(n),\n",
    "#         ]\n",
    "#     )\n",
    "#     return posterior_model\n",
    "\n",
    "def prior_trainable(kernel_size: int, bias_size: int, dtype: any) -> tf.keras.Model:\n",
    "    \"\"\"Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\"\"\"\n",
    "    n = kernel_size + bias_size\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),  # Returns a trainable variable of shape n, regardless of input\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t, scale=1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# def prior_trainable(kernel_size, bias_size, dtype=None):\n",
    "#     n = kernel_size + bias_size\n",
    "#     prior_model = keras.Sequential(\n",
    "#         [\n",
    "#             tfp.layers.DistributionLambda(\n",
    "#                 lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "#                     loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
    "#                 )\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "#     return prior_model\n",
    "\n",
    "\n",
    "def random_gaussian_initializer(shape, dtype):\n",
    "    n = int(shape / 2)\n",
    "    loc_norm = tf.random_normal_initializer(mean=0., stddev=0.1)\n",
    "    loc = tf.Variable(\n",
    "        initial_value=loc_norm(shape=(n,), dtype=dtype)\n",
    "    )\n",
    "    scale_norm = tf.random_normal_initializer(mean=-3., stddev=0.1)\n",
    "    scale = tf.Variable(\n",
    "        initial_value=scale_norm(shape=(n,), dtype=dtype)\n",
    "    )\n",
    "    return tf.concat([loc, scale], 0)\n",
    "\n",
    "def negative_loglikelihood(targets, estimated_distribution):\n",
    "    return -estimated_distribution.log_prob(targets)\n",
    "\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([20,40,60,80,100]))\n",
    "HP_NUM_EPOCHS = hp.HParam('num_epochs', hp.Discrete([500,1000]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'RMSprop']))\n",
    "\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([40,60,80,120,160]))\n",
    "HP_NUM_EPOCHS = hp.HParam('num_epochs', hp.Discrete([500,750,1000,1250,1500]))\n",
    "HP_NUM_EPOCHS = hp.HParam('num_epochs', hp.Discrete([1000]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([200]))\n",
    "\n",
    "METRIC_ACCURACY = keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "# with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "#   hp.hparams_config(\n",
    "#     hparams=[HP_NUM_UNITS, HP_NUM_EPOCHS, HP_OPTIMIZER],\n",
    "#     metrics=[hp.Metric('accuracy', display_name='RMS error')],\n",
    "#   )\n",
    "session_num = 0\n",
    "coverage_list_mean = []\n",
    "coverage_list_std = []\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  \n",
    "  for num_epochs in (HP_NUM_EPOCHS.domain.values):\n",
    "    \n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "      hparams = {\n",
    "          HP_NUM_UNITS: num_units,\n",
    "          HP_NUM_EPOCHS: num_epochs,\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "      }\n",
    "      input_ = tf.keras.layers.Input(shape=x_train.shape[1:])\n",
    "      hidden1 = tfp.layers.DenseVariational(hparams[HP_NUM_UNITS], activation='relu', input_shape=(5,),\n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1 / len(y_train))(input_)\n",
    "      hidden2 = tfp.layers.DenseVariational(hparams[HP_NUM_UNITS], activation='relu', input_shape=(5,),\n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1 / len(y_train))(hidden1)\n",
    "      hidden3 = tfp.layers.DenseVariational(hparams[HP_NUM_UNITS], activation='relu', input_shape=(5,),\n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1 / len(y_train))(hidden2)\n",
    "      hidden4 = tfp.layers.DenseVariational(hparams[HP_NUM_UNITS], activation='relu', input_shape=(5,),\n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1 / len(y_train))(hidden3)   \n",
    "      hidden5 = tfp.layers.DenseVariational(hparams[HP_NUM_UNITS], activation='relu', input_shape=(5,),\n",
    "                                      make_posterior_fn=posterior_mean_field,\n",
    "                                      make_prior_fn=prior_trainable,\n",
    "                                      kl_weight=1 / len(y_train))(hidden4) \n",
    "\n",
    "      concat = tf.keras.layers.Concatenate()([input_, hidden4])\n",
    "      #output = tf.keras.layers.Dense(1)(concat)\n",
    "      distribution_params = tf.keras.layers.Dense(units=2)(concat)\n",
    "      output = tfp.layers.IndependentNormal(1)(distribution_params)\n",
    "      model = tf.keras.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "      #note sure what these inputs mean. Find out.\n",
    "      #adam utilizes adaptive momentum - variant of stochasticc gradient descent.\n",
    "      #loss is the loss function\n",
    "\n",
    "\n",
    "      #note sure what these inputs mean. Find out.\n",
    "      #adam utilizes adaptive momentum - variant of stochasticc gradient descent.\n",
    "      #loss is the loss function\n",
    "\n",
    "      #model.compile(optimizer='Adam', loss=calculate_loss,metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "      #model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "      #model.compile(optimizer='adam', loss='mean_absolute_error',metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "\n",
    "      model.compile(optimizer=hparams[HP_OPTIMIZER], loss=negative_loglikelihood,metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "      #model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001), loss=negative_loglikelihood,metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "      print(\"Start training the model...\")\n",
    "      _, rmse = model.evaluate(x_train,y_train, verbose=1)\n",
    "      print(f\"Train RMSE: {round(rmse, 5)}\")\n",
    "\n",
    "      print(\"Evaluating model performance...\")\n",
    "      _, rmse = model.evaluate(x_test, y_test,verbose=1)\n",
    "      print(f\"Test RMSE: {round(rmse, 5)}\")\n",
    "\n",
    "      model.summary()\n",
    "\n",
    "      #for tensorboard\n",
    "      log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "      #history = model.fit(x_train,y_train,epochs=hparams[HP_NUM_EPOCHS],shuffle = True,verbose=1,validation_data=(x_test,y_test), callbacks=[tensorboard_callback])\n",
    "      history = model.fit(x_train,y_train,epochs=hparams[HP_NUM_EPOCHS],batch_size = 2000,shuffle = True,verbose=1,validation_data=(x_test,y_test), callbacks=[\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1),  # log metrics\n",
    "        hp.KerasCallback(log_dir, hparams)])\n",
    "\n",
    "      run_name = \"run-%d\" % session_num\n",
    "      print('--- Starting trial: %s' % run_name)\n",
    "      print({h.name: hparams[h] for h in hparams})\n",
    "      session_num += 1\n",
    "\n",
    "    #calculate the % of predictions are are within 1 sigma of the mean:\n",
    "\n",
    "      coverage_1sigma_list = []\n",
    "      outlier_list1 = []\n",
    "      outlier_list2 = []\n",
    "      outlier_list3 = []\n",
    "      bayesian_outlier_list = []\n",
    "      mean_rms_i_list = []\n",
    "      std_rms_i_list = []\n",
    "      prediction_mean_list = []\n",
    "      prediction_stdv_list = []\n",
    "      delta_z_list = []\n",
    "      delta_z_list2 = []\n",
    "      summed_rms_list = []\n",
    "      loop_size = 2\n",
    "\n",
    "      coverage_list_per_gal= []\n",
    "        \n",
    "\n",
    "      for_coverage_list_per_gal = [0] * len(y_test)\n",
    "\n",
    "        \n",
    "      for j in range(0,loop_size):\n",
    "\n",
    "        #if the mean of a model distribution is within 1 sigma, record 1 for that index\n",
    "        \n",
    "          \n",
    "        evaluated_model = model(x_test)\n",
    "\n",
    "        prediction_mean = (evaluated_model.mean()).numpy().tolist()\n",
    "\n",
    "        prediction_stdv = (evaluated_model.stddev()).numpy()\n",
    "        #prediction_stdv = np.std((model(x_test)*bin_size))\n",
    "        # The 95% CI is computed as mean Â± (1.96 * stdv)\n",
    "        upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()\n",
    "        lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()\n",
    "        prediction_stdv = prediction_stdv.tolist()\n",
    "        #delta_z_list.append(np.array(y_test) - np.array(prediction_mean))\n",
    "        prediction_mean_list.append(prediction_mean)\n",
    "        prediction_stdv_list.append(prediction_stdv)\n",
    "        predictions = model.predict(x_test)\n",
    "        outlier_list1.append(calculate_conv_outlier_rate2(prediction_mean,y_test))\n",
    "\n",
    "        num_within = 0\n",
    "        for idx in range(len(y_test)):\n",
    "            #print(\n",
    "                #f\"Prediction mean: {round(prediction_mean[idx][0], 2)}, \"\n",
    "                #f\"stddev: {round(prediction_stdv[idx][0], 2)}, \"\n",
    "                #f\"95% CI: [{round(upper[idx][0], 2)} - {round(lower[idx][0], 2)}]\"\n",
    "                #f\" - Actual: {y_test[idx]*bin_size}\"\n",
    "            #)\n",
    "            if abs(prediction_mean[idx][0] - y_test[idx]) < prediction_stdv[idx][0]:\n",
    "              for_coverage_list_per_gal[idx] = for_coverage_list_per_gal[idx] + 1\n",
    "              num_within = num_within+1\n",
    "\n",
    "        coverage_1sigma_list.append(round(num_within/len(y_test)*100,3))\n",
    "        #coverage_list_per_gal.append(for_coverage_list_per_gal)\n",
    "        \n",
    "        ####calculate individual galaxy RMS#####\n",
    "        ####calculate individual galaxy RMS#####\n",
    "        predictions = model.predict(x_test)\n",
    "        photoz = []\n",
    "        outlier_list2.append(calculate_conv_outlier_rate2(predictions,y_test))\n",
    "        outlier_list3.append(calculate_bayesian_outlier_rate2(predictions,y_test,prediction_stdv))\n",
    "        delta_list_temp = []\n",
    "        for i in range(0,len(y_test)):\n",
    "            photoz.append(predictions[i])\n",
    "            delta_list_temp.append(y_test[i]-predictions[i])\n",
    "       \n",
    "        delta_z_list2.append(delta_list_temp)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train','Test'],loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        #delta_z_list.append(np.array(y_test) - np.array(photoz))\n",
    "        photoz_2 = predictions\n",
    "        RMS_i = []\n",
    "        for i in range(0,len(y_test)):\n",
    "          rms = ((photoz[i]-y_test_original[i])/(1+y_test_original[i]))**2\n",
    "          RMS_i.append(rms)\n",
    "\n",
    "        mean_rms_i = np.mean(RMS_i)\n",
    "        mean_rms_i_list.append(mean_rms_i)\n",
    "        std_rms_i = np.std(RMS_i)\n",
    "        std_rms_i_list.append(std_rms_i)\n",
    "        #print(\"mean individual galaxy RMS\", mean_rms_i)\n",
    "        #print(\"stdev individual galaxy RMS\", std_rms_i)\n",
    "\n",
    "        ####calculate summed RMS\n",
    "        rms_sum = 0\n",
    "        for i in range(0,len(y_test)):\n",
    "          rms_sum = rms_sum + RMS_i[i] \n",
    "\n",
    "          rms_sum = np.sqrt(rms_sum/len(y_test))\n",
    "          \n",
    "        #print(\"Summed RMS: \", rms_sum)\n",
    "        summed_rms_list.append(rms_sum)\n",
    "\n",
    "\n",
    "\n",
    "      mean_summed_rms_list = np.mean(summed_rms_list)\n",
    "      std_summed_rms_list = np.std(summed_rms_list)\n",
    "\n",
    "      mean_of_rms_i_means_list = np.mean(mean_rms_i_list)\n",
    "      std_of_rms_i_means_list = np.std(mean_rms_i_list)\n",
    "\n",
    "      std_of_std_of_rms_i_list = np.std(std_rms_i_list)\n",
    "\n",
    "\n",
    "      print(\"mean_summed_rms_list\",mean_summed_rms_list)\n",
    "      print(\"std_summed_rms_list\",std_summed_rms_list)\n",
    "      print(\"mean_of_rms_i_means_list\",mean_of_rms_i_means_list)\n",
    "      print(\"std_of_rms_i_means_list\",std_of_rms_i_means_list)\n",
    "      print(\"std_of_std_of_rms_i_list\",std_of_std_of_rms_i_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      print(\"% of spec-z within 1 stdev of the prediction mean :\", round(num_within/len(y_test) * 100.0,2))\n",
    "\n",
    "      mean_coverage_1sigma_list = np.mean(coverage_1sigma_list)\n",
    "      std_coverage_1sigma_list = np.std(coverage_1sigma_list)\n",
    "\n",
    "      print(\"mean 1 sigma coverage with 100 trials : \", mean_coverage_1sigma_list)\n",
    "      print(\"stdev of 1 sigma coverage with 100 trials: \", std_coverage_1sigma_list)\n",
    "\n",
    "      coverage_list_mean.append(mean_coverage_1sigma_list)\n",
    "      coverage_list_std.append(std_coverage_1sigma_list)\n",
    "\n",
    "\n",
    "\n",
    "#now calc coverage per gal\n",
    "final_per_gal_coverage = []\n",
    "for i in range(0,len(y_test)):\n",
    "  final_per_gal_coverage.append(100*for_coverage_list_per_gal[i]/loop_size)\n",
    "\n",
    "coverage_percent_diff = []\n",
    "for i in range(0,len(coverage_list_mean)):\n",
    "  coverage_percent_diff.append(-(68-coverage_list_mean[i])/68)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "np.argmax(predictions[0])\n",
    "photoz = []\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    photoz.append(predictions[i])\n",
    "\n",
    "plt.scatter(y_test_original,photoz)\n",
    "#plt.title('Photo-z determination')\n",
    "plt.ylabel('spectro-z')\n",
    "plt.xlabel('photo-z')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "num_correct = 0\n",
    "outliers = []\n",
    "outliers_bayesian = []\n",
    "outlier_index = []\n",
    "outlier_index_bayesian = []\n",
    "cat_outlier_index_bayesian = []\n",
    "cat_outlier_index = []\n",
    "#y_test = y_test * bin_size\n",
    "for i in range(0,len(y_test)):\n",
    "\n",
    "    if abs(photoz[i] - (y_test[i])) < 0.0001:\n",
    "\n",
    "        num_correct = num_correct + 1\n",
    "        \n",
    "    outliers.append((abs(photoz[i] - y_test_original[i]))/(1+y_test_original[i]))\n",
    "    outliers_bayesian.append((abs(photoz[i] - y_test_original[i]) - prediction_stdv[i][0])/(1+y_test_original[i]))\n",
    "\n",
    "    if outliers[i] > 0.15:\n",
    "            outlier_index.append(i)\n",
    "\n",
    "\n",
    "    if outliers[i] > 1:\n",
    "            cat_outlier_index.append(i)\n",
    "            \n",
    "    if outliers_bayesian[i] > 0.15:\n",
    "            outlier_index_bayesian.append(i)\n",
    "\n",
    "\n",
    "    if outliers_bayesian[i] > 1:\n",
    "            cat_outlier_index_bayesian.append(i)\n",
    "\n",
    "\n",
    "\n",
    "print(\"% correct: \", 100.0*num_correct/len(y_test))\n",
    "print(\"number of outliers: \", len(outlier_index), \" out of \", len(y_test))\n",
    "print(\"% of outliers: \", 100.0*len(outlier_index)/len(y_test))\n",
    "print(\"number of catastrophic outliers: \", len(cat_outlier_index), \" out of \", len(y_test))\n",
    "print(\"% of catastrophic outliers: \", len(cat_outlier_index)/len(y_test))\n",
    "\n",
    "print(\"number of bayesian outliers: \", len(outlier_index_bayesian), \" out of \", len(y_test))\n",
    "print(\"% of bayesian outliers: \", 100.0*len(outlier_index_bayesian)/len(y_test))\n",
    "print(\"number of bayesian catastrophic outliers: \", len(cat_outlier_index_bayesian), \" out of \", len(y_test))\n",
    "print(\"% of bayesian catastrophic outliers: \", len(cat_outlier_index_bayesian)/len(y_test))\n",
    "\n",
    "# #calculate RMS error:\n",
    "# RMS_error = np.sqrt(np.sum(((abs(photoz - y_test_original)/(1+y_test_original))**2))/len(y_test))\n",
    "# squares = [x*x for x in outliers]\n",
    "# RMS_error_2 = np.sqrt(np.sum(squares)/len(y_test))\n",
    "# print(\"RMS error: \", RMS_error_2)\n",
    "\n",
    "# #calculating RMSE another way:\n",
    "# RMSE = mean_squared_error(y_test_original,photoz, squared = False)\n",
    "# print(\"RMS error = \", RMSE)\n",
    "\n",
    "####calculate individual galaxy RMS#####\n",
    "RMS_i = []\n",
    "for i in range(0,len(y_test)):\n",
    "  rms = abs((photoz[i]-y_test_original[i])/(1+y_test_original[i]))\n",
    "  RMS_i.append(rms)\n",
    "\n",
    "mean_rms_i = np.mean(RMS_i)\n",
    "std_rms_i = np.std(RMS_i)\n",
    "print(\"mean individual galaxy RMS\", mean_rms_i)\n",
    "print(\"stdev individual galaxy RMS\", std_rms_i)\n",
    "\n",
    "####calculate summed RMS\n",
    "rms_sum = 0\n",
    "for i in range(0,len(y_test)):\n",
    " rms_sum = rms_sum + RMS_i[i] * RMS_i[i]\n",
    "\n",
    "rms_sum = np.sqrt(rms_sum/len(y_test))\n",
    "\n",
    "print(\"Summed RMS: \", rms_sum)\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(y_test_original,photoz, marker='+',color = 'black')\n",
    "#plt.title('Photo-z determination')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('photo z')\n",
    "plt.plot([.18,1.6*2.4],[0,1.2*2.4], color='black')\n",
    "plt.plot([0, 1.6 * 2.4], [.15, 2 * 2.4],color = 'black')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_test_original,photoz, marker='+',color = 'black')\n",
    "#plt.title('Photo-z determination')\n",
    "plt.xlabel('spectroscopic redshift')\n",
    "plt.ylabel('photo z')\n",
    "plt.plot([.18,1.6*2.4],[0,1.2*2.4], color='black')\n",
    "plt.plot([0, 1.6 * 2.4], [.15, 2 * 2.4],color = 'black')\n",
    "plt.ylim(0,4)\n",
    "plt.show()\n",
    "\n",
    "y = np.ravel(photoz)\n",
    "\n",
    "print(\"coverage_list_mean: \", coverage_list_mean)\n",
    "print(\"coverage_list_std: \", coverage_list_std)\n",
    "\n",
    "mean_per_gal_after_500_determinations = np.mean(prediction_mean_list,axis=0)\n",
    "mean_stdv_per_gal_after_500_determinations = np.mean(prediction_stdv_list, axis = 0)\n",
    "stdv_stdv_per_gal_after_500_determinations = np.std(prediction_stdv_list, axis = 0)\n",
    "stdv_per_gal_after_500_determinations = np.std(prediction_mean_list, axis = 0)\n",
    "stdv_of_delta_z_after_500_determinations = np.std(delta_z_list2, axis = 0)\n",
    "mean_of_delta_z_after_500_determinations = np.mean(delta_z_list2, axis = 0)\n",
    "\n",
    "#print(\"mean per galaxy after 500 determinations: \", mean_per_gal_after_500_determinations)\n",
    "print(\"mean of mean stdv of zPDF after 500 determinations: \", np.mean(mean_stdv_per_gal_after_500_determinations))\n",
    "print(\"mean of stdv of zPDF stdv after 500 determinations: \", np.mean(stdv_stdv_per_gal_after_500_determinations))\n",
    "print(\"mean of std of zPDF means after 500 determinations: \", np.mean(stdv_per_gal_after_500_determinations))\n",
    "print(\"mean of delta_z (specz-photz) mean after 500 determinations: \", np.mean(mean_of_delta_z_after_500_determinations))\n",
    "print(\"mean of delta_z (specz-photz) stdv after 500 determinations: \", np.mean(stdv_of_delta_z_after_500_determinations))\n",
    "\n",
    "# print(\"mean outlier % after 500 determinations: \", np.mean(outlier_list))\n",
    "# #print(\"mean bayesian outlier % after 500 determinations: \", np.mean(bayesian_outlier_list))\n",
    "\n",
    "# print(\"average outlier rate after 500 determinations:\", np.mean(outlier_list2))\n",
    "# #print(\"average bayesian outlier rate after 500 determination: \", np.mean(outlier_list3))\n",
    "\n",
    "print(\"average outlier rate after 500 determinations:\", np.mean(outlier_list2))\n",
    "print(\"average bayesian outlier rate after 500 determination: \", np.mean(outlier_list3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
